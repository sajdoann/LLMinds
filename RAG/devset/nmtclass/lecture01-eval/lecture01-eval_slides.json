[
  {
    "folder": "screen01-slide01",
    "content": "Good morning, welcome to the class on statistical machine translation. The first lecture is on evaluating machine translation quality."
  },
  {
    "folder": "screen02-slide02",
    "content": "First, I'd like to give you a very rough overview of what is going to happen in this semester. So this is the course outline. As you see, we start at the end. We will discuss the evaluation first and you will learn very quickly why evaluation is so important for all the research that you are doing. And then, in the next lecture, we will discuss the overview of various approaches to machine translation. This class is called statistical machine translation but nowadays all machine translation systems are statistical in some sense, so we will just say machine translation. It's simply automatic that all the systems are statistical. And that obviously includes neural machine translation. Neural machine translation has changed the field a few years ago, so we're going to discuss it very briefly already on the third lecture and then we'll get to that later on as well. So in the first three lectures we will get like a glimpse and an overview of everything and then from the fourth lecture onwards we'll be a little bit going back in the history and discussing the important algorithms and approaches that finally build up to what we have now, the highly multilingual neural machine translation systems. So these would be the advanced lectures. The field of machine translation is a field which at least to me at the beginning seemed to guarantee that I would get hold of the meaning of the sentence, because it was impossible for me to realize or to think that we would be translating from one language to another language and not operating with the sense, with the meaning of the sentence. Luckily or unfortunately, it depends on the way you look at it, you can deliver perfect translations by some magical components and these components do not understand the meaning of the sentence at all. So it turns out that performing very well in machine translation can be kind of mocked, simulated, faked by just clever copy-pasting and adjusting pieces of texts that humans previously translated. So one of the last lectures towards the end of the semester is also going to be devoted to the question of meaning, how much the systems actually internally kind of understand, whatever it means, and whether the neural MT is making this understanding more accessible or actually less accessible. So we'll discuss that later on. And I'm again highlighting the last point. At the very end of the semester your projects will be presented and that's the cornerstone of this seminar. So it's not only the lectures but it will be also your research work."
  },
  {
    "folder": "screen03-slide03",
    "content": "So now for today I'm going to talk about machine translation evaluation and for that we first need to very briefly summarize what is the task of machine translation for us and then we'll go over well maybe two high number of evaluation methods and they are grossly divided into manual evaluation methods where you need humans annotators or assessors and then the automatic evaluation where you implement algorithms and these algorithms somehow estimate the quality of the MT output. And if I have the time at the end I'll also briefly mention empirical confidence bounds on these scores and also the question whether you should evaluate some individual components in the system or whether you should evaluate the system end-to-end."
  },
  {
    "folder": "screen04-slide04",
    "content": "Okay, so why is measuring of output quality in machine translation important or actually absolutely critical? Why do we put it in the very first lecture? Well, without some metric, without some form of measuring the current quality, you would not be able to track your progress. So there would be no research possible. So empirically you would not know where to go, which system is better. Here is an example from the history. Back then, in 1970s people used or relied primarily on manual judgments. So there was one particular system developed at Euratom. It was a system based system for Russian to English translation, and they gave the same outputs to two groups of people. The first groups of annotators were teachers of language, and they scored the system, and they gave it 1 out of 5 possible points, so like a D minus, that was the worst score ever. They said it's totally useless garbage, it doesn't make any sense, this Russian to English translation. And then the same outputs were scored by people in the field, nuclear physicists, and these nuclear physicists gave it an A plus for 4.5 out of 5 points for the same outputs. And the reason is that suddenly the articles, this was articles on nuclear physics were no longer in Cyrillic, so the nuclear physicists could see the keywords. Maybe the sentence structure was totally wrong, but the keywords were there. So they understood what the formulas were also there, so they could understand the formulas and suddenly the whole article made a lot of sense. For someone who doesn't read Cyrillic, this was simply a lifesaver. So with the same output you can get very different assessment depending on who are you asking. So that's the importance of choosing the right metric and then if you choose the metric that in turn influences where the research goes. So one metric that we will discuss today is called BLEU score and that scores short sequences of words like four words at once and because of the popularity of that metric there was like a positive loopback that the research focused on methods which delivered translation which as piece-wise good. So the phrase-based machine translation, which we'll discuss in the next lecture in a very brief overview and then in some more detail later, that delivers translations perfect in four-word windows, but that can easily forget the verb in the sentence. So the sentence doesn't make any meaning altogether, but piece-wise it is perfect. If another metric was chosen then maybe the focus would be on other methods and like before neural MT came we would have totally different MT systems. So the metrics drive the research. So if you choose your metric right, you arrive at the point where you are happy, where you have the system which does what you want. If you choose a wrong metric, well then you are optimizing towards some"
  },
  {
    "folder": "screen05-slide05",
    "content": "So what is our goal? We restrict the task to the following conditions. We have no writer's ambition, so we are not translatologists. We want kind of literal translation as literal as possible and we don't aim to handle any cultural differences or other things that human translators care about. That said, the methods actually do it by themselves. They are trained on data so if the data reveal these cultural differences, then suddenly the system does that as well. Yeah, you have a question? What do you mean by cultural differences? So that's, yeah, an example of these cultural differences, I remember of a story I heard somewhere. The psychologists ran a test on bilingual kids and these bilingual kids were English-Romanian. And in the test, the kids were given, like small kids, they were given a soup. It was known they didn't like. And when they were given the soup by someone who is talking in English to them, they said no thank you. And when they were given the soup in the Romanian context of their bilingual brain, they said no. So that's like a cultural difference. The way of agreeing or disagreeing varies and in the English language or I would say English culture, British culture, you are trying to be polite and that is part of the phrases that the kids learn. So that's something that we would expect for our systems to, like if they were ideal, to obey the culture rules of the setting where they are applied. And the systems are not designed with this in mind, but it comes for free if it's in the data. So that's the cultural differences. And then the expected output quality. So this is the very rough scale, the roughest possible scale, and then we'll refine that in the manual evaluation and automatic evaluation. So the output can be worth reading. If you do not know the language, the source language, the machine translation would be of some use to you. It could be worth editing. So if you are a professional translator, the output will save you a lot of typing. So that's perfect. Notice that here the goal is slightly different. That already like asks for different optimization. Either preserving the core meaning or minimizing the edits necessary to bring it to a perfect output. And then the third stage would be worth publishing so that you could immediately publish that result and have no fear that people will laugh at you. So in general we are aiming at level one or two depending on the language pair and available data. And the level two remains and it will remain risky because the systems cannot step out of the training data, they have read more text than a human can read in a lifetime. So there is a little chance that they could avoid like stupid pitfalls, but still it's uncertain because they don't understand what is the message and who is the intended audience of that message and so on. So it remains risky."
  },
  {
    "folder": "screen06-slide06",
    "content": "Yeah, so then basic directions of manual evaluation. When you are designing a manual evaluation method, you need to decide what to show to the annotators, what context they should consider, and what are you asking. So what to show? You can show the source text, or you can show the reference translation, which already distorts the scores a bit, because the single reference translation could have highlighted something which was not so important in the source. And so it is really like text interpretation that happens when the translator provides the reference translation. So the source is kind of more reliable but that slows down the process and you need bilingual people to assess that. So the best is possible to show them both but this definitely like changes the overall setting. Then there is the question of context to consider and for many years people in machine translation field focus only on the sentence level. So machine translation systems these days operate on the level of sentences. If you shuffle the sentences you get the same outputs. Well, there is some randomness in the system, so not all the time, but in deterministic systems, yes, you get the same output regardless of the order and there is no cross sentence information or relations preserved. Document level score would be something which asks the human assessor to judge the whole document and say how good it is, whether the cross sentence phenomena are also well preserved. And there you obtain or there you ask for one single score for the whole document. And document aware is something of a mix. You are going to show people smaller pieces, so individual sentences, but you give them the whole context. And you are asking whether these sentences, each of them, is good given the whole source document and also given the whole target output. So then what to ask? You can show people two different outputs or more different outputs and ask them to relatively sort them in some way, so some relative comparison, or you can show people just one output and say how good it is on some scale. And this also has the various consequences on the results of the evaluation. And you can also ask some more complicated questions."
  },
  {
    "folder": "screen07-slide07",
    "content": "So here are the scoring techniques. So what are you asking. And I kind of divide those into techniques where you only consider the MT output as such so that's the blackbox method. So that's where you don't care who created the translation and how. Than some kind of grey box evaluation in which you already look at the content of the sentences from a linguistic point of view or other points of view. And you are trying to figure out what error types are there, which may be important for some particular applications. So some applications may be sensitive to grammar errors, some may be sensitive to negation drop and other things. So that's a gray box. And then if you are an author of the system, you are most interested in knowing which component fails, doesn't do its job properly, and that would be kind of glass box evaluation. So you're checking how far this information from the source got actually in the system and where it was lost in the processing. As you see, most of effort is being devoted to the black box methods and that's because you can apply these methods to all the systems. So there is regular competitions in machine translation, the WMT task, and that has, aside from competing in machine translation, also had a track on competing in or internally, like, competes, like how do we evaluate how do we find the winner. And that's where a lot of insight comes from. So the two classical measures of evaluating translation is checking the adequacy and fluency. These two scales are also correlated. You cannot speak about the adequacy of the output if the fluency is so bad that you don't understand it at all. These two scales also correlated in human judgments. So today they are kind of revisited in something which is direct assessment which we will see on a slide in a second. And this direct assessment uses just one scale normally. Then there is relative ranking and that could be the relative ranking could be applied to either the whole sentences or some smaller chunks of of those. That's the ranking of constituents. Then you can do another thing. Well I'll discuss them in a second. So some comprehension test, whether the sentence preserves the meaning in some way and then you can check whether the translated text serves the purpose. So that's the task-based evaluation which is very costly but this is actually what you are after. So the question here could be do I dress appropriately if I read the Chinese weather forecast translated into English or was there some information lost and I didn't"
  },
  {
    "folder": "screen08-slide08",
    "content": "So let's look at the direct assessment. That's something which was introduced to the competition of WMT in 2013 and the idea is to make the evaluation as simple as possible so that we could give it to random volunteers who were working on some crowdsourcing platforms such as Mechanical Turk. So it is reference based so that you require only monolinguals in the target language and it has just one simple slider. It's not too visible on the slides, but the slider is a score, essentially on a continuous scale from 0 to 100, and you as the assessor will read the reference translation, and you are checking whether the candidate translation adequately expresses the meaning of the reference. And you just pick some score whether you like the output or not. So this is like a like button but it's like slider. And obviously when you're looking at your first evaluation then you would not know like is this a good output or bad output. But after about 15 judgments each annotator stabilizes. People also heavily differ in their strategies. Some are like heavily optimistic, some are heavily pessimistic. Styles are different but if you average it across a large number of people and across a large number of judgments then you are relying on statistics and like statistical interpretation of the scores and it is interpretable. So you will learn on average which sentences by which system are the best translated. The drawback is that if we run it on a mechanical Turk or another crowdsourcing platform, then there are also people who cheat just want to get the money for the scoring and put the slider at a random position. There are easy ways to find out these people. You will give them sentences which you know are worse and if they score them higher, then you know that they haven't read that because they have not spotted that there are some words which do not fit into the sentence at all. So we can easily filter them out, but we pay them anyway and there is like a half of the money lost because half of the assessors are not reliable. And another big problem, actually bigger problem because you can always ask for money and get it from somewhere. But the bigger problem is that the crowdsourcing platforms have mainly English-speaking participants. And if you want to score translations into other languages, you are running out of people. So the other languages are normally in this competition evaluated by the researchers themselves. So we as the Czech team take part, and we also promise to score some number of sentences. So this may be part of your homework when the campaign comes, but this year it is a little later, so it may not overlap with the semester at all. And it's definitely interesting to see what are the best systems, and multiple languages are explored there, so yeah, depending on what you speak, you could help us. So that is the assessment"
  },
  {
    "folder": "screen09-slide09",
    "content": "of adequacy. And there is also a version of direct assessment used for fluency, but it is used less frequently and it was only used to break ties. So if two systems score around the same value in adequacy, then the fluency would help one of these systems. And in fluency you have the same single slider and you are showing just one sentence, the output of the system and it does not matter what the source was or what the reference was, you are asking whether this is a fluent sentence in the target language. So this is something which if we would rely only on this, then we would totally lose the meaning and systems which would produce fluent sentences would win. But if we use it as a complement, then it works"
  },
  {
    "folder": "screen10-slide10",
    "content": "reasonably well. So here is a result from 2018 using this style of evaluation, the direct assessment, and it was switched to source based, not reference based. So because we had the reference like sitting aside, we were able to mix it among the systems. And people, the assessors, were evaluating not only machine translation systems compared to the source, but also the human professional translation. So like standard, normally picked translation agency, and we didn't pay any extra double checks. We paid the normal price that someone from the street would order. It happened that our system, system trained by Martin Popel, was significantly better than the professional translation in this style of evaluation. So that already is suspicious. The significance is indicated by these bars. So the professionals were on par, indistinguishable, in quality from the Edinburgh machine translation system. So that is suspicious."
  },
  {
    "folder": "screen11-slide11",
    "content": "An underlying reason for this particular result, but we are actually getting towards the time when machine translation will be very much similar in quality to human translation. Here in 2018 the reason was probably this. Humans, when they were producing these reference translations, were translating whole documents and the machines were translating individual segments. No one cared about document level context in 2018. Like we knew that it is important, but we had other problems. And the evaluation was also done at the level of individual segments. So here is the benefit that the MT got. The sentences when judged without the context can seem more natural if they were created without the context. But if you evaluate the sentence in the proper context of the document, then suddenly I'm quite confident that the human scores would be higher. Okay, so that's a very important thing to realize, that you can easily give some benefit to your systems just by the way you design your evaluation. And it's very difficult to be fair in this. So there people have then later on followed up on this and there are many studies that like discuss how much we are on par with humans, but it's the critical question is what you"
  },
  {
    "folder": "screen12-slide12",
    "content": "are actually evaluating. So one attempt to fix it, but that's a very simple attempt, hacked quickly together for last year's evaluation for 2019, was to, well, ask people to score the whole document. So this is the direct assessment, but instead of one sentence, you see here the whole news article and then you give one number. So obviously this is mentally impossible to score it reliably. And another problem is that you are getting too few judgments. So this will make your statistics run over two small set of numbers, and these statistics are generally unreliable. So that's the problem of a very simple way of handling document level scores."
  },
  {
    "folder": "screen13-slide13",
    "content": "Another setup which was run at the same time is this source-based pseudo-document or direct assessment and there you are scoring the sentences one by one as you did before but you are showing the sentences in the original order of the document. So people, the assessors, will actually read the whole article so they will have the context in their heads and they can score the coming sentences worse and worse when they see that the sentences depart from the meaning of the whole document. So it's mentally manageable. But the way it was done, it was again like a very quick hack. There was no way to go back to the previous sentence. So you score the first sentence high because you seemed it makes sense. But then as you were reading the article, you realized there was some very bad word choice in the first sentence and there was no way to indicate it anymore. And another problem is that all the sentences in the whole document must come from the same system. Otherwise you would not be judging the coherence in the document. So that requests for more judgments so that you cover all the systems with sufficient number of judgments. So that's the problem. And another problem that I see here is that these probes, these questions to the humans, are no longer independent of each other. And that's one of the assumptions of the underlying statistical evaluation afterwards. So I'm not so good in statistical tests but I'm afraid some of these statistics can also be skewed by this change."
  },
  {
    "folder": "screen14-slide14",
    "content": "So here is the results from this pseudo-document-aware evaluation. So from the last evaluation campaign in English to Czech, the system from 2018 was now significantly worse than humans and it was on par with some system which tried to handle the document context as well. And what else? Well, having said this it is also important to realize that every year it is new texts. So it can be the case that the translation agency was maybe a little bit more careful or the texts were for some reason, like domain match or domain mismatch, harder or easier than the previous year. So like having lost the human parity is something which sounds strange. Like in 2018 we would be saying, oh, we have beaten humans in translation and in 2019 we'll be saying, well, machines are no longer better than humans. That doesn't sound reasonable. So in the first place we didn't say that we have beaten humans in 2018. It's important to realize that we know what are the limitations of the evaluation method. So this is English to check, but"
  },
  {
    "folder": "screen15-slide15",
    "content": "There are other languages and in English to German there is this result that the Facebook system appears significantly better than the professional translation. And there is one more thing to realize. Well, there is more speakers and more researchers of German area than for Czech. So the number of participating systems in English to German is very high. It's a very competitive task. And the number of hours that people can spend on assessing them is limited. So there is a huge chunk of incomparable systems. So we have this one that is significantly better than the others, but the others is actually about 20 systems, all that cannot be distinguished. Statistically, they are on par. There is differences between them, but they are in the same chunk. So you have to do many more judgments to see whether Microsoft was better than humans or not. Now with confidence we can say that all these systems are just on par, there were only two systems which were like worse."
  },
  {
    "folder": "screen16-slide16",
    "content": "Okay, so the best setting that we could do in this area is mixing all together. So it's something that Martin Popel is trying and I'm not showing the results because it's not yet published anywhere, but here we are benefiting from showing more systems, so we are showing two or more systems so that people have like and idea what is the average performance there. One of these systems can be also humans. We are showing the source and we are showing the whole document. And the whole document is often too long, so what we do is we just highlight 10 sentences in a row in this document and ask people to score these 10 sentences in a row. So that way the scoring doesn't take too much time. There is the chance to see the whole document. We are getting 10 scores per one row. And we have all the benefits. And actually, we are asking three scores. We are asking adequacy, fluency for each of the sentences and an overall quality. And then maybe one more score for the whole chunk of sentences. So we are getting quite a large number of numbers. So the statistical significance can be achieved reasonably well. But it is very time-consuming. People really have to focus on that. So this is something we cannot do with the Turkers."
  },
  {
    "folder": "screen17-slide17",
    "content": "So what was in the past? And let's quickly discuss this because there was a period of 80 years of WMT evaluation where some other simplification was used and it's called the relative ranking. So in the relative ranking, you run by sentences and you are showing the source, the reference, and you are showing five outputs of MT systems. Possibly there could be human output as well, but you would have to pay twice for the test set translation because you would have to... Well, you could run this also without showing the reference, but if you want to show the reference you need to create a separate human translation independently. So this is what you see and this is kind of people who are used to looking at this. It took a while to realize where to put scores, but essentially you put bullet points to indicate that these first two sentences are the best, they are on the same rank and then this one is the worst and these are like on par again somewhere in the middle. So this is the way you would score it. The benefit is that you are saving time, you are reading the source sentence only once and you are assessing five systems at a time. So that is some saving, but the limitation is that the context of the document is not considered and also the interpretation of"
  },
  {
    "folder": "screen18-slide18",
    "content": "numbers can be tricky. This is just a suggestion for a project, if you still have not decided what your project could be. I've also run a little eye tracking study on this. I had, I think, just eight colleagues, so a small study. They were doing these assessments under an eye tracker. We could also redo that. We have an eye tracker here as well. I was curious what people do mostly. Indeed, they mostly start with reading the source, then they read the reference and then they read one by one the candidates and then they score it. And maybe they could be doing some pairwise comparisons. So this is some analysis which was no longer done with the data. I like the number of pairwise comparisons compared to like rereading and it would be interesting to see whether people like focused on errors and whether the score correlates with the errors appearing there and so on. So this is an invitation for anyone to analyze the data."
  },
  {
    "folder": "screen19-slide19",
    "content": "And there is the obvious problem that the sentences can be bad at different places. So if the first sentence is bad at the beginning and the second sentence is bad at the end, how do you rank them? Are they both equally bad or what do you do? So that's a sentence level problem already. And now imagine that we are going to evaluate the whole document. So it's absolutely inevitable that these two documents will be incomparable. I would say. So when people are still doing the sentence level evaluation, they deemed a sentence is maybe too long unit to compare, so they also tried doing relative ranking of just a part of a sentence. So here you are given the whole sentence content, but you are assessing only the highlighted phrase. So that gives you some simplification for the annotators, but again you are not evaluating even the whole sentence, so maybe the verb is lost, this method will not notice."
  },
  {
    "folder": "screen20-slide20",
    "content": "So here is a little intermezzo. If you have these scores, how do you get the winner of the MT system? And the main message from this is whatever field you are working in, whatever evaluation method is established there, you need to very carefully see what is behind the scenes and what is actually being done. So when interpreting these middle ranks, what you are getting from people is these screens of annotations or we can maybe call them blocks."
  },
  {
    "folder": "screen21-slide21",
    "content": "This block of annotation is the dots where you put them. I've simplified it. I'm using four systems here and not five in the picture. But system A was the best here, B and C were on par, and D was the worst, and the annotator even said it's like a little more worse than just one bit worse. So this is one screen. That will show to one person. Then maybe to another person,"
  },
  {
    "folder": "screen22-slide22",
    "content": "a different subset of systems was shown and on a different sentence, by the way, as well. So here the system A was the worst one, actually. B and C were again on par and E was the best one. So now if you have these assessments, you have asked many people, collected many of such screens"
  },
  {
    "folder": "screen23-slide23",
    "content": "The question is who wins the competition. I would like to highlight that the point of the competition is not really to win. It is only now when the media are following that. The competition was always to see what works best. And the next year you would follow the article, the system description from the previous year of your competitors, of your colleagues. And in a friendly way you would learn what worked for them and you would implement it in your system which is often very different in internals and by this mutual cross-fertilization, that's a favorite word, you would get the best systems altogether. So this is a fast progress and it's not too important who wins. It's definitely like a wrong idea to shake your medals. We are not giving out any medals at the competition and that's for a good reason. So it's because there are many caveats to this evaluation. But anyway, if we want to sort the system in some way, how do we interpret these scores?"
  },
  {
    "folder": "screen24-slide24",
    "content": "A description of that in the paper and the description says. Systems are ranked based on how frequently they were judged to be better than or equal to any other system. So when having read this, I was looking at the..., I downloaded the judgments and I was trying to come up with the same ranking as was in the paper."
  },
  {
    "folder": "screen25-slide25",
    "content": "So, better than any other system judged. So, A took part in two screens and out of these two screens it was better than any other system in one case. So, A got one out of two possible points, B got zero of two points and so on, and E is the only system which gets one out of one. Obviously this is too small evaluation, so this is an unreliable number but yeah of the one screen where E was, it was the best system. So based on this score"
  },
  {
    "folder": "screen26-slide27",
    "content": "E seems to be the winner of this competition. These numbers, this ranking was different from what was in the papers. So I asked the organizers, how did you get your scores from these judgments? I followed the rule and they said,"
  },
  {
    "folder": "screen27-slide28",
    "content": "Well, we do this simulated pairwise thing actually. So we interpret all these judgments as pairwise comparisons and here in pairwise comparisons A is compared with B and it is better. A is compared with C and it is better. A is compared with D and it is better and so on. So one screen here creates six pairwise judgments because it's four systems and if you have five systems it creates ten pairwise comparisons."
  },
  {
    "folder": "screen28-slide29",
    "content": "You do the same for all the screens, for each of these screens."
  },
  {
    "folder": "screen29-slide30",
    "content": "Then you have a large number of pairwise comparisons. So this is a way to get larger statistics, larger numbers, so that this evaluation is more stable because it is based on more numbers."
  },
  {
    "folder": "screen30-slide31",
    "content": "And then you evaluate in these pairwise comparisons how often the system was better than its competitors."
  },
  {
    "folder": "screen31-slide32",
    "content": "A took part in six pairwise comparisons, and in three of them it was the better one. And"
  },
  {
    "folder": "screen32-slide33",
    "content": "B took part in six competitions, and it was better in four of them."
  },
  {
    "folder": "screen33-slide34",
    "content": "Sometimes the score is the same. So E took part in one screen and won. It took part in three pairwise comparisons and won of all of those. So here the score would be 100% for all cases."
  },
  {
    "folder": "screen34-slide35",
    "content": "But in some cases the results will differ. So in the simulated pairwise, B seems better because it was better than in more pairs, whereas A won more screens, so to say. So I'm showing this again to highlight that the actual calculation of the score can affect what you will get as the end result. And the eye tracking was also performed with the idea in mind, maybe when people are shown five systems at once, we should not interpret these five scores as pairwise comparisons. If they don't do pairwise comparisons, these pairwise comparisons can be unreliable. So we have something which is statistically better because it is based on more numbers, but the human process behind that is less stable. So that is why people keep questioning the way things are evaluated. And this is something that you should always keep questioning."
  },
  {
    "folder": "screen35-slide36",
    "content": "Okay, so that was the ranking. Now another style of evaluation. Ideally we would want to check whether the translation is saying the same thing as the source. So how do we operationalize this? How do we ask that question? Like does this sentence say what it should say? So the way to do is to run something which can be labeled blind editing or comprehension test. In this blind editing, you first show only the output and you ask humans to correct it. So here is a sentence. Tell me what it says. Because the sentence will contain many errors. It's from MT output. So first humans somehow guess what was the meaning of the sentence. Maybe it is fluent. If it's fluent, perfect. There is no change needed. We can just ship it. It says what it says. If there is an error, we make a guess, fix the sentence. And then a second person comes"
  },
  {
    "folder": "screen36-slide37",
    "content": "And the second person checks whether these corrected sentences, blindly corrected sentences, preserve the meaning. So the second person validates whether the first person has understood. And this is a nice way of evaluation. Again, it is costly in some way, but it's nice because it easily, it directly tells you the proportion of sentences that were correctly understood by the first person who didn't have access to anything else, only to the system output. So that's the, that's the computation."
  },
  {
    "folder": "screen37-slide38",
    "content": "Here is another thing. I've talked about the task-based evaluation, so giving people instructions translated by machine and checking whether they do what they are supposed to do. We made a simplification of that. We asked them to answer a quiz-like questions. So we prepared English texts. We found them so that the text will be hopefully written by native speakers, and then we machine translated them into Czech and we equipped each of these texts with three questions and these questions were already in check because we were checking whether the Czech speakers would be able to correctly answer the question given the machine translated or machine produced Czech. And these texts came from different areas it was like when do you meet and where do we go, then directions, how do you get to a monument, some basic quizzes from various areas and also the domain of the machine translation systems which is the news articles. So election chances and similar stuff. So the annotation is Given machine translated snippet answer the questions."
  },
  {
    "folder": "screen38-slide39",
    "content": "Here is an example. It's from 2010 and like one system is from 2010, one is from 2007. Well, you can read the Czech and there is obvious errors in the outputs, but still you are somewhat able to answer these questions whether a particular institution, DCU, which is the Dublin City University, is on one particular street or not whether there are roundabouts, where there are traffic lights on roundabouts in that city, which seems kind of unnatural for Czech people. And if the description says that you should do something on a roundabout and a traffic light at the roundabout, then you will be able to answer yes and on which side of the street the institution is. Now the problem here is that the questions revealed too much. So in general it was too easy to answer these questions because you were just able to spot even if there was some error in the translation, you were able to spot whether something is mentioned or not. And this mentioning was sufficient. So this kind of explains why gisting machine translation with low quality for a language that you don't speak works so well, if you have the context what's the question. If you know what you are after, you will find it even in some very badly translated output. So that's an approximation of the task"
  },
  {
    "folder": "screen39-slide40",
    "content": "evaluation. Here is another way of scoring. This is the GCSE style of evaluation, the maturita in Czech. So we were checking this year how the best systems translate audit reports. It was auditors who assess them, and they use scores similar to what students are tested on when they write their essays for GCSE. So whether the language is used well in spelling and morphology, whether the vocabulary is chosen adequately for the content and whether the text is clear or not clear because of the vocabulary choice, whether the syntax and word order are good and overall whether the text is understandable. So the auditors read machine translated audit reports and gave a score. We again mixed in human translation and the scores indicate that the best systems were always like statistically impossible to distinguish from the reference translation except for one category the spelling and morphology the language use where the system was better than humans. So this indicates that not only in... so this is 2019 so this is the same evaluation where the direct assessment said that significantly humans are better and here in one of the scales it said that significantly the machine was better, so again different questions give you different results. So here's one more test that"
  },
  {
    "folder": "screen40-slide41",
    "content": "we've used these superhuman machine translation systems to translate agreements. So this is a sub-lease agreement, something that you could easily be signing if you are living in Prague, because it's non-professionally translated. There is a landlord and he will show you like double-sided agreement. One side will be in Czech, you don't understand it at all. The other side will be in English, some vague, some maybe bad English, but it will indicate what is the agreement about. And then if you are Czech living somewhere else, you may get the agreement in English and you want to machine translate it into Czech so that you understand better. So, well, let's not read this as a whole, but it's some sublease agreement that some person, Marta Boreshova and Karolina Cherna, they are entering this and... Yeah, so there's a lot of details."
  },
  {
    "folder": "screen41-slide42",
    "content": "Put it to these best systems, these superhuman systems, this is what you get and I'm hardly"
  },
  {
    "folder": "screen42-slide43",
    "content": "I think the problems. So it says things like that the that the tenant as one side of the party and the tenant as the other side of the party, it uses the same Czech word na n√°jemce, are entering an agreement in which the tenant and the tenant are entering the agreement and the tenant is giving the apartment to the tenant. So the choice of the words made these parties totally mixed. You don't know who is promising to do what to whom because it is a sublease agreement. So there is three parties in practice. It's trained on texts which mention either these two parties or these two parties, but when you have three the system is free to choose any of the translations and there is no document level context so these superhuman systems produce totally unusable outputs for agreement. So this is again to highlight that depending on the setting, different results will"
  },
  {
    "folder": "screen43-slide44",
    "content": "arise. Yeah, now we are entering the part where we are trying to understand what was wrong in the MT output. So the previous evaluation was only score the system, tell me which system is better. Now we would like to learn more what is what is the output like. So one evaluation was called HMENT and it was checking whether the basic even structure was understandable. And I will only highlight it."
  },
  {
    "folder": "screen44-slide45",
    "content": "I'll illustrate how it works. So you have the candidate A and then the reference translation. So the reference translation says the referee Wolfgang Stark then garnered some attention and the two candidates are finally he stood in the center of the referee Wolfgang Stark. That doesn't make much sense. At the end of the day was at the center of a referee Wolfgang Stark. That doesn't make much sense either. So if you were to rank A and B in the ranking evaluation, you would probably know where to put them. If you were to put these sentences on a continuous scale, that would not help you either. So here, H is meant"
  },
  {
    "folder": "screen45-slide47",
    "content": "to at least suggests what to do."
  },
  {
    "folder": "screen46-slide48",
    "content": "You should identify the action, then you should"
  },
  {
    "folder": "screen47-slide49",
    "content": "Identify the participants, so someone"
  },
  {
    "folder": "screen48-slide50",
    "content": "stood and somewhere and when you did the same for the reference"
  },
  {
    "folder": "screen49-slide51",
    "content": "This is easier to do because this is fluent sentence, so there is an"
  },
  {
    "folder": "screen50-slide52",
    "content": "action and there is again who was taking part in that action and some other circumstances."
  },
  {
    "folder": "screen51-slide54",
    "content": "and then you align these two. And this will kind of indicate which bits of information, whether the event or some of the participants got lost, because you cannot align them, or they got somehow distorted in the meaning. So, obviously,"
  },
  {
    "folder": "screen52-slide55",
    "content": "the meaning is distorted, but you know where it was"
  },
  {
    "folder": "screen53-slide56",
    "content": "distorted. So it's easier to process this. You are more confident when doing this evaluation. But again, it is too complicated to show it to turkers."
  },
  {
    "folder": "screen54-slide57",
    "content": "Here is another variant which relies on the semantic trees for the source language. So the benefit of this Hume style evaluation is that the semantic trees are something which can be like automatically created by some parser. It is source based so you are relying on good grammatical sentences and not non-grammatical output. The other benefit is that you can also include the reference translation in the evaluation because you do not need it. It's not reference based. So that's it, but still it was"
  },
  {
    "folder": "screen55-slide58",
    "content": "It is always complicated to run these evaluations, so it was not run on large scale and never tested in"
  },
  {
    "folder": "screen56-slide59",
    "content": "in a big run. And I'm showing these because we're in Prague and while in Prague we must not forget tectogrammatics, the deep representation of the sentence, deep syntactic. And it is very close to what H. Mann and Hume do. So if your other area of studies is in any way related to syntax or deep syntax, if you have worked with the Prague dependency treebank and so on this would be a nice project for you. Use the available tools to get to the deep syntax or there's this universal dependencies that are also going deep in the recent years. So you can you could build an evaluation measure which relies on this deep syntax of the source. So that could be a research question."
  },
  {
    "folder": "screen57-slide60",
    "content": "project. A simpler way which doesn't require people to have syntactic knowledge is to just label for errors. So here is a simplified classification of errors that we tried at this seminar and we have asked our colleague students to label words which are wrong. And the words can be wrong for various reasons. So there can be bad word form, the word itself is correct but the form doesn't fit the sentence, or it can be bad sense, and there we were distinguishing two categories, whether it is like clear that it was misunderstood by the system, or whether it was like you cannot really pinpoint where the mistake is, but the meaning of the chosen word doesn't fit the sentence very well. So that's bad disambiguation. And obviously words can be missing, and if they are missing, it can be a missing content word or a missing auxiliary word a and there can be also bad punctuation errors, this is easy, and there can be errors in word order, so maybe some local error or the word can be too far away from its best position, there will be some long-range word-level error. So this is a simpler hierarchy."
  },
  {
    "folder": "screen58-slide61",
    "content": "that we used. This is something which got later standardized and the translation industry also uses that, so I need to mention this multi-dimensional quality metrics. They have similar issues. For accuracy they are checking for omission or mistranslation and addition. There can be also words that should not appear in the output and they are also checking for the fluency, which is the spelling, typography and the style or register of the document or whether the whole sentence was totally unintelligible and they are also checking the verity, which is like the practical usability of that sentence in the setting. So the example to illustrate verity is if you are translating driving instructions from German into English and you are doing it for driving in Australia or in the UK and you are saying drive on the left-hand side and you preserve the side of the road then it will cause crashes in those countries. So that's like the local applicability. If the translation will fit the local context of where it is going to be used. So this is the standardized classification of errors and"
  },
  {
    "folder": "screen59-slide62",
    "content": "This is the full version of that. So actually the categories can be much more refined. And then imagine that humans are expected to label words with this fine hierarchy. Everybody will get lost. So I'm showing this for one reason. If you have a very difficult classification task for humans, then what they did, that's a clever idea, they've actually created a"
  },
  {
    "folder": "screen60-slide63",
    "content": "the flowcharts. So there is an order of preference for labeling the errors. So instead of you are labeling the errors you are"
  },
  {
    "folder": "screen61-slide62",
    "content": "Yes, you are labeling errors in this hierarchy, but you are following a strict procedure and that ensures that two people will have much better agreement because they are walking in this hierarchy in the same direction."
  },
  {
    "folder": "screen62-slide64",
    "content": "So this is the decision tree, but still the full version is kind of excessive. Yeah."
  },
  {
    "folder": "screen63-slide65",
    "content": "So here is the result that we have done in our simple evaluation. So again the example is in Czech so I'm not going to highlight it too much. The sentence was perhaps there are better times ahead and the problem is with this English there which is just a syntactic word it should not be used in the Czech language at all and in some of the translations it appears so that's a superfluous an extra word. And there is another problem with the form. Some of the systems fail to produce the correct conjugation or declination forms of the words. And this times is mistranslated. It is mistranslated as multiplication in one of the cases. And this head is also mistranslated in one translation. It is translated not like on the timescale head, but on the local scale. So in Czech we have a slight distinction whether ahead is in terms of time or in terms of space and that was like wrongly chosen. So that's another of this label. So when we"
  },
  {
    "folder": "screen64-slide66",
    "content": "We did this evaluation and that's finally some utilities coming from that. We actually, we used the same systems which took part in this 2019 competition. And the 2019 competition was interesting because there I won in the BLUE score, in the automatic scoring, which we'll discuss in a second. So I said yes, great, I'm winning the competition. And then the manual ranking came, that was the relative ranking back then. And PC Translator a traditional kind of rule-based system was the best and I was the third. And I said, how come that my system is so good in the automatic evaluation and so bad in the manual assessment? This flagging of errors explains that. So it turned out that my system did the fewest mistakes in word choice. So I had the lowest number of words with a bad sense but I had the highest number of missed content words. So my system was afraid of making errors, so it rather dropped the words, it truncated the sentences. It was not penalized too much by the automatic scoring, but humans didn't like the fact that words got lost. So that's an obvious problem. And PC Translator, it actually made almost the highest number of bad lexical choices, but it had the lowest number of missed content words. So that explains why humans preferred PC translator. So this is how the gray box style evaluation can give you a better explanation. And obviously, for me, it was a confirmation, well, you have overfitted to this automatic metric. The metric does not score all it has to score. You must not drop content words, otherwise people will not like your outputs."
  },
  {
    "folder": "screen65-slide67",
    "content": "So this was one particular contradiction. There is also contradictions in manual evaluation itself. So we have discussed the ranking and in the ranking in that year Google won when we were considering the ties. When we were not considering the ties, when we asked the comparison to be strict so that one system is strictly better than the other, then PC translator won. So even just the little change whether you take the tie into account or not, can change the order of the systems. That is strange. The number of edits deems acceptable, that's the comprehensibility check. So there was the two people, one reading the sentence and the other, one correcting the sentence and the other checking whether it was corrected in line with the original meaning. There Google won. Google also won in the automatic evaluations, but in the quiz based evaluation it was actually our deep syntactic system which preserve the meaning of the sentences for the purposes of this testing best. So different evaluation methods, and most of them are manual evaluation methods, they will give you different results. So that's why I'm always highlighting the friendly competition and not the competition competition, not who is going to win, but who has the lowest number of errors of a particular type aspect. This year in 2019 we have seen that the best systems match humans in the GCSE style scoring but they score worse in the pseudo document or direct assessments and they are absolutely terrible when translating agreements. So again we have different outcomes based on the manual evaluation."
  },
  {
    "folder": "screen66-slide68",
    "content": "So we are summarizing, finally we are summarizing manual evaluation. It is expensive in terms of money. It is subjective, so different judges can guess more. Different judges can be more picky and more fussy about grammar and so on. It is obviously the results will differ across judges, but the results will also differ for a single person. If you ask the same person twice, they will give you a different answer because they will notice another part of the sentence and give you a different score. It is not reproducible. That's the hardest problem of manual evaluation because as soon as an annotator has read a sentence they will remember it even if you give it to them in a week they will still be biased by that. So you cannot do this repeatedly at all and we have already seen that the experiment design is absolutely critical. You will learn different results. The black box evaluation is important for users or sponsors who just want to know who is the best and full stop but the more fine-grained gray or glass box evaluation is important for us developers. We know which aspect of the system is wrong and what should we focus on. And then the source-based evaluation allows to compare with humans, so there we can check whether in the evaluated aspect we are on par with humans or not. And the sentence level is no longer relevant for any language pair where there is enough large enough training data. So the sentence level is going to happened on for languages where we have sufficient resources and good enough systems because, well, that was the 2018 result, beating humans at the level of sentences, but not"
  },
  {
    "folder": "screen67-slide69",
    "content": "not in the full context. So the big area that I would like to cover now is automatic evaluation. Automatic evaluation will compare machine translation outputs to some reference translation and there is also another type of automatic evaluation which doesn't require the reference, it only relies on the source and that's called quality estimation but we're not going to discuss that. So we're going to rely on the on the reference translation. The main benefit is that it is fast and cheap. So you translate the test sets once by humans, and then you can repeatedly do it. It's fully replicable. It's deterministic. So because it is fast and deterministic, you can even do it within some modal optimization. So this merge thing or this tuning is from the old pre-neural approaches. These days, we would do some reinforcement learning on the final scores. Usually automatic evaluation of MT quality is good for checking progress because you have the same setup and you are simply improving it gradually. And it is usually very bad for comparing systems of different types. I've already mentioned the problems of BLEU and we're going to discuss them in more"
  },
  {
    "folder": "screen68-slide70",
    "content": "detail now. So there is a lot of different automatic evaluation metrics. As I said, there were years and still running, years of competition. How do people like, which metric matches better the human assessments. So many metrics were created. The one that still remains the standard in the field is called BLEU and it is simply checking the number of n-grams that are confirmed by the reference. So here we have the source, ... we have the reference translation .... And we have four MT systems. And if the word produced by the system is not confirmed by the reference no score is given .... If the word alone is scored well then like unigram wise the system is getting a point and up to four-grams. And if a particular sequence of four words is confirmed by the reference, then the system is getting one four-gram. So the first system, which is phrase-based translation, Moses open source machine translation system, in uni-grams it produced nine good ones. So one, two, three, four, five, six, seven, eight and the full stop is the ninth out of ten, ten words in total. So it gets score of 9 out of 10 uni-grams, it gets the score of 7 out of 9 bigrams, 5 out of 8 trigrams, and 4 out of 7 four-grams. And so this is like checking whether the long sentences and short sequences are confirmed by the reference. And then to aggregate these n-gram scores, you just take the geometric mean of those, and then you apply on ... which we will discuss in a second. So there could be some weights in the geometric mean, but the weights are always said uniformly, because for some situations the longer sentences could be potentially more important than the shorter ones, but people don't care much. Yeah, so this is it. And here you see that Moses, which is phrase-based, has produced many long sequences which were in line with the reference whereas PC Translation which was the rule based system well, it said the same thing, but it used different words and it didn't get any four-gram and any trigram. It only scored a few individual words and just two bigrams. You already see the beginning of the problem the BLEU has promoted the use of phrase based systems because they got, they were getting higher BLEU scores, they were getting the four grams correct. In that year I think PC Translator was already losing but still it was not losing by that much as the BLEU score suggests."
  },
  {
    "folder": "screen69-slide71",
    "content": "Yeah, so one very critical thing is when you're designing any automatic evaluation metric is to avoid cheating or gaming your metric. You want to make sure that there is no simple way to fool your metric into thinking that you're good. So one way in which the BLEU score could be cheated but it is not because it has a mechanism to avoid it is that you could be producing only reliable words so if you are translation into English, it's absolutely certain that there will be the definite article in the sentence. So what if we just produce the definite articles? So if we just produce the the the the as the output. So here we would be like the metric would be very stupid if it would be giving credits to all the copies of the definite article. It counts only those, only as many outputs of the word, as there appear in the reference. So if one of the references uses two definite articles, then up to two will be scored. And the same strategy, this clipped count, is used obviously also for the higher n-grams. Then another strategy in which people could, or the systems could fool the BLEU score, would be producing only the reliable words. So we know that the sentence is, like we know that the definite article is a good thing to put into an english sentence so let's just do the the the full stop. This is three out of three uni-grams confirmed. So this seems like the best possible score if the best score would not look at longer sentences. And obviously it is not a good output. So put in another way, the metrics can be either precision based or recall based. So either you are checking whether all your output is confirmed by the reference, so that's precision based, or you're checking whether you have produced all what the output expects and that's recall based. So, brevity penalty is precision based because it is checking whether all your words or your n-grams were confirmed by the reference. And it doesn't contain, or without the brevity penalty, it would not contain any recall check at all. And it would be easily fooled by making like very high precision and low recall outputs. So the brevity penalty is exactly to introduce the recall aspect. The brevity penalty simply strikes a score, reduces the score if the output is too short compared to the reference. And that's the trick. So the idea is the output must be of the expected length, and then when it's of the expected length, then the precision aspect will take care if it's reasonable or worse in that output. But if it's too short, then obviously something must have been omitted. So that's the brevity penalty. So the brevity penalty is this simple formula. If our output is longer than the reference, then okay, we don't get any penalty. If our output is shorter than the reference, then we get an exponentially growing penalty so that we are penalized for trying to game the metric. The question is what is the length of the reference if you have more references? So the BLEU score was originally designed and tested with four different references used at a time. But then what is the length? That's the question. If the number of references is lower, then the BLEU scores are less reliable. So that's the BLEU score and this is something which you really should know how it works, and you should know its problems. And we're now going to"
  },
  {
    "folder": "screen70-slide72",
    "content": "to discuss the properties and the problems. It's within the range of 0 and 1. It's often written as 0 to 100. So if you see a black score of 0.25, you don't know whether the system is totally crappy because it's in the scale of 0 to 100 or if it's actually reasonable like 25 could be a good score. The human translation, humans against humans, is usually around 60 percent and this is many years ago phrase based system 30 and 50 for different languages these days and also it depends on the domain for some domains we are getting BLEU scores around 70 because the text is so repetitive that even like new sentences in the test set still contain large portions that were seen in the training data so it's easy to get high scores. Yeah BLEU score for individual sentences is not reliable. It works only reasonably well if the document is larger and more so if there is only one reference. So I've said that the original paper expects people to use four different translations as the reference, but the standard in all these competitions is to have just one. So the results are not not as reliable. So here's an illustration why it is a bad idea to use just one reference. Why is it suggested to use four sentences? Well that's just the balance between the price you are willing to pay for the creation of the test set and a reasonable performance in terms of correlation with humans. It was tested. In the original paper it was like tested and like they didn't go any further. The further you would go the better but it was shown that four is kind of enough. But that means that one is not really enough. And everybody uses one."
  },
  {
    "folder": "screen71-slide73",
    "content": "Because it already feels bad translating the text, which you already have translated, translating it a few more times. We have played around with BLEU scores at the seminary in 2007. And there we were trying... So at that year we were first starting with our English to Czech systems. And we didn't have test sets, which would be in Czech. So we created a test set ourselves. We ourselves created four reference translations. So these are BLEU scores on the same set of sentences, either used in the direction from Czech into English and their professional translators provided the reference translations, or from English to Czech and there was students of maths and physics. So I'm showing this to just give you the idea that the BLEU scores differ a lot. They differ because of the language difference. So here we are comparing human translation against three other human translations. So depending on which you choose the scores will vary and if you then average it the scores will also vary. And for translation into English we were getting scores like 35 or up to 50 and for the other direction English into Czech we were getting scores almost an order of magnitude lower. This is because Czech is harder, there is more words to choose from. And also this is non-professional translation. So if the references are bad, they differ from each other more. So that's why you get fewer n-grams confirmed. But I would like to ask why the number of references is so critical for the actual value of the BLEU score. If you have one reference, it's 35. If you use three references, the score is 52. Can anyone say why the more references you have the higher the score will arise. Because there's a finite number of ways to translate some. Yeah, so that's you are answering exactly correctly. So it boils down to the question to what extent do we match the human translation? And if we have more possible, more allowed human translations there is a bigger chance that our n-grams will be confirmed by any of those. So the more references you have, the higher the chance, the higher the number of n-grams you will score. So the number of references is an absolutely critical information when you score. But the main message that you should learn today is that you should not trust any numbers unless you have evaluated them yourself. So unless you saw the outputs of the system and you ran the evaluation yourself. So there is heavy dependence on the number of references. More differences allow to match more n-grams and there is also heavy dependence on the translation direction and translation quality."
  },
  {
    "folder": "screen72-slide74",
    "content": "So here's an example how the BLEU score correlates badly with humans. It is 2008 results and here you would like to see like a diagonal, the better the higher the BLEU score the better the human rank should be. And that applies to the phrase based systems, a simple version and an improved version, but it doesn't apply to our system which is based on this deep syntax. And also to the commercial system. The commercial system did not care about n-grams, it cared about the users, so it focused on not forgetting the words, if you remember, and it scored well in the human rank and very low in the BLEU score. So that's the correlation."
  },
  {
    "folder": "screen73-slide75",
    "content": "I'm going to skip these dirty tricks. If you have... well, you can somehow fiddle with the way you use the reference translations, and you will arrive at a higher scores because of this fiddling trick. But it doesn't do anything. It doesn't change any consequence, and you don't get any new implication by this trick on the .... On the previous slide"
  },
  {
    "folder": "screen74-slide74",
    "content": "PC translator was in domain. Yes, so this is mainly to illustrate the observation on two different test sets. How could it be worse in domain and better out of domain? No, this is just... Yes, so the label whether the test set is in domain or out of domain applies only to the train systems. So it applies only to the MOSES system and our tech-to-md system and PC translator was simply ran on those, so we don't know if it was in domain for PC translator or not. It is only for our systems."
  },
  {
    "folder": "screen75-slide76",
    "content": "Yeah, so here is something like my first experiment with BLEU cores many years ago. I was working on translation from Czech to English and I was doing some clever stuff with like corpus expansion using dependency trees and all that. And that gave me just plus 0.3 BLEU core. And then I fixed clear BLEU problems, so like, I don't know if I..."
  },
  {
    "folder": "screen76-slide77",
    "content": "Yes, I did these tricks. I reversed the order of full stop and quote mark, and then gave him"
  },
  {
    "folder": "screen77-slide76",
    "content": "me a better improvement and then also I applied a similar tokenization to the reference and my MT output and that gave me plus 10 points. So obviously this just highlights that I was very young and stupid and I didn't know that the tokenization absolutely has to match, but because I didn't know it I just learned the hard way in that experiment. So the impression was that complicated methods bring more. I trust these. But the most important message was that huge jumps of the absolute BLEU scores are due to just superficial properties, the tokenization and all that. And for this reason, it is absolutely critical that when you are comparing your system with other systems, that you get the outputs, and you look at the numbers, and you run the same processing pipeline on these outputs. You cannot copy numbers from any paper because you don't know what type of tokenization they use and all that."
  },
  {
    "folder": "screen78-slide78",
    "content": "So here are technical problems. BLEU scores are not comparable across languages. They are not comparable across different test sets. They are not comparable across different number of reference translations. They are not comparable with different implementations of the evaluation tool. There are two reasons why it's that. And they are very sensitive to tokenization. So the good thing that you can do is to rely on one fixed reference implementation and one has been done recently by a Matt Post, it's called SACREBLEU. So that's something which removes some of these problems."
  },
  {
    "folder": "screen79-slide79",
    "content": "But there are also fundamental problems of BLEU and these fundamental problems are that BLEU is overly sensitive to word forms and sequences of tokens. So we have seen that somewhat in the past and again this flagging of errors provides an explanation for that. So let's look at the uni-grams only. So that is the number of words in the MT output. In total there is 35,000 of uni-grams 35,000 of words and the question is whether these words are confirmed by the reference, so what the BLEU will give a credit for them or not and whether they received a flag by humans, whether they are errors, whether they contain some error or whether they are good. So that's the two distinctions. Whether the particle word is confirmed by the reference and whether it contains errors and luckily, the two cases that we do want are the majority together. So words that are confirmed and do not get any error flag were about 37% of the test set. And words that are not confirmed by the reference, and they do contain an error are about 22% the test set. So the majority of words are like properly validated by the check with the reference. What sometimes happens is that word is confirmed by the reference and still the annotator labels it as wrong word. So that's a false positive error. That's about 6% of the volume. But the real trouble comes from this line and that's words which do not contain any error according to the people who read the sentences and still they are not confirmed by the reference. So there is a single reference. The translator decided to translate it somehow differently and for that reason the word the correct word from the MT output is not confirmed by the reference and this amounts to more than a third of the output so this is too large space where these systems can differ and some of the systems can be bad in these unconfirmed words and some can be good in these unconfirmed words so too large volume of the output is not scored, so that is where the system can differ, BLEU will not notice, and we will get lack of correlation with human judgments. So in a paper where I've discussed this, we have actually found out that the higher the BLEU scores themselves are, the more they correlate with humans. So if you have a higher number of matches, the reference fitted well with the MT outputs and therefore the mismatches are indicators of error. In normal case, the mismatches in one-third of cases do not indicate anything. So that's a fundamental problem."
  },
  {
    "folder": "screen80-slide80",
    "content": "So to fix this fundamental problem we can evaluate coarser units, so not exact word forms, but lemmas or deep-lemmas. We can focus on characters instead of words, so that's chrF3 or chrF3. That's something which is equally simple to the BLEU score calculation and it should be popular because it is simpler and correlates better with humans. We can also use shorter and gapy sequences. So these other metrics are a bit more complicated and there is a number of, a large number of other metrics. And another option is that you could use better references. If you use more references alone that helps, but it is costly. If you use references which are created from the MT outputs, then you will indeed have a mismatch only when there was an error that the post editor had to fix. So if you have references that are based on the MB outputs, then they will serve better."
  },
  {
    "folder": "screen81-slide81",
    "content": "in the evaluation. This is just highlighted in numbers. So you get a higher correlation with a fewer number of sentences. So just 100 sentences with six or seven post-edit references is as good in correlation as 3,000 of independently created references. So if you have a chance to correct empty"
  },
  {
    "folder": "screen82-slide83",
    "content": "Yeah, so there is another fundamental problem and there's also a fundamental problem of evaluating metrics as such, and we're finally coming to the end of the lecture. So I've talked about a large number of methods and like the manual methods and the automatic methods, and I've told you that the automatic methods are designed to correlate well with humans. So I'm gonna now totally blow it up with saying that it is difficult, that this correlation with humans is not something very stable. So the correlation of an automatic metric with the human judgment depends on the underlying set of systems. So this is English to German translation where there are 20 systems. If you consider all of them, it will seem that the standard BLEU score correlates almost perfectly with that 0.99 or something. That's the SACREBLEU is this line the violet line the other lines are other metrics that take part in the competition. So one of the metrics is very bad, like it's always under zero. This one is also like pretty bad. Most of the metrics seem to correlate very well, but if you reduce the set of systems, and you take only the top eight systems, then suddenly you are around zero, so there's no correlation. And if you take just four systems, the top performing systems in English to German translation the BLEU correlates negatively with the human rank. So the underlying set of systems is like"
  },
  {
    "folder": "screen83-slide84",
    "content": "So this is what happens here. So we have the direct assessment and we have the Suckerblatt score and we have the individual systems. How well they score in each of these metrics. As you are moving to this part where the good systems are, then it becomes more of a cloud and less of a straight line and actually the seemingly high correlation in this particle language pair was to a big extent caused by this outlier and this second outlier and there it is still like there is some reasonable correlation. But the correlation vanishes when you are limiting yourself to top four systems and these top four systems are actually totally in the reverse order. So there BLEU score correlates negatively. So this is something that we still have to work on. Like what is a fair way of comparing metrics given the underlying set of systems. In each of the languages, language pairs, the set of systems is different, so there is outliers in different areas and we want to avoid outliers, so we want to have some stable and general impression, but then there will be many counter examples. So this picture"
  },
  {
    "folder": "screen84-slide83",
    "content": "This applies to most metrics and most language pairs. They are good across the whole set of participating systems, but as you move towards the best systems, the metrics will not allow you to distinguish which of them is the best."
  }
]