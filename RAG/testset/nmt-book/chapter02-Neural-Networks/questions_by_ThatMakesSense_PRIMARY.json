[
    "What are the limitations of Linear Models?",
    "Why do Linear Models fail with XOR?",
    "What is a hidden layer in a neural network?",
    "Why are activation functions used in neural networks?",
    "What is the main advantage of using hidden nodes in neural networks?",
    "How does the XOR operation get implemented in the neural network?",
    "What is the main advantage of using neural networks over linear models for modeling Boolean operations?",
    "Why is moving alongside the gradient a good idea in the context of weight optimization during back-propagation training?",
    "Why is moving alongside the gradient a good idea?",
    "What is the derivative of the output value with respect to the weight?",
    "What drives weight updates?",
    "How do we track how the error caused by a hidden node contributed to the error in the next layer?",
    "What drives weight updates?",
    "Why is setting the learning rate too high a problem during gradient descent training?",
    "What happens when the activation function is sigmoid, which only has a short interval of significant change?",
    "What is the consequence of local optima when using gradient descent training?",
    "What happens when a neural network misses its global optimum due to local optima?",
    "When should training stop in a neural network?",
    "What happens to the error on the validation set when training a neural network?",
    "Why do initial weights for a hidden layer have to be chosen from a specific range?",
    "What is the purpose of the momentum term in training a neural network?",
    "Why does the learning rate need to be adapted during training?",
    "What happens to the momentum term at each time step?",
    "How does Adagrad adjust the learning rate for a parameter?",
    "What is used to correct for bias in the hyperparameters?",
    "How does Adam adjust the learning rate?",
    "What is drop-out?",
    "How does layer normalization work?",
    "Why is layer normalization needed?",
    "What is the purpose of normalization in the context of neural networks?",
    "What is the purpose of normalizing the values in a weighted sum vector?",
    "What is stochastic gradient descent?",
    "What is the advantage of using mini batches in neural network training?",
    "What is Hogwild?",
    "What is the main advantage of using specialized hardware like GPUs for fast graphics processing?",
    "What problem do methods like drop-out and gradient clipping solve in neural network training?",
    "What is the purpose of the 'tensor' concept in neural networks?",
    "What is the main challenge in using matrix operations for neural network computations?"
]