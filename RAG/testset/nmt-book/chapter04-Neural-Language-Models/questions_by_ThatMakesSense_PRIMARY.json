[
    "What is one hot vector?",
    "Why use one hot vectors to represent words?",
    "What is word embedding?",
    "How does the embedding matrix work?",
    "What is the purpose of an activation function in a neural network?",
    "Why do language models use softmax activation function to ensure that all values add up to one?",
    "What role do word embeddings play in neural machine translation?",
    "How can word embeddings be used to enable semantic inference?",
    "What can be computed offline for neural language models in decoding?",
    "How to train a self-normalizing model to have normalized node values?",
    "What is the main idea of noise contrastive estimation?",
    "What are the objectives of noise contrastive estimation?",
    "What is the primary difference between using a recurrent neural network with start-of-sentence neurons and traditional statistical back-off models?",
    "What is the main advantage of using recurrent neural networks over traditional statistical back-off models in terms of contextual information?",
    "How does the training process for recurrent neural networks with start-of-sentence neurons differ from traditional statistical back-off models?",
    "How do we train recurrent neural networks with arbitrarily long contexts?",
    "What are some challenges with recurrent neural networks?",
    "What problem does the use of long sequences in recurrent neural networks pose?",
    "What is the main problem addressed by the long short-term memory (LSTM) neural network architecture?",
    "What is the purpose of the gates in an LSTM cell?",
    "What happens when over long distance gradient values become too large in an LSTM network?",
    "What is the effect of propagating through many steps in an LSTM network?",
    "What role do gate parameters play in an LSTM layer?",
    "How are gate parameters set in an LSTM layer?",
    "What is the difference between LSTM cells and GRU cells?",
    "What are the two gates in GRU cells?",
    "What is the purpose of the update gate in GRU cells?",
    "How do GRU cells balance between the current input and the previous state in the combination process?",
    "What is the motivation behind using deep learning for sequence prediction tasks in language?",
    "What is the purpose of using a sequence of hidden layers for shallow neural networks?",
    "What is the main difference between shallow and deep stacked neural networks in sequence prediction tasks?",
    "What are some ways to reduce computational complexity in deep neural networks for sequence prediction tasks?",
    "What is the main purpose of using noise contrastive estimation?",
    "What is the advantage of using a traditional n-gram language model in ARPA (SRILM) format?",
    "What is the benefit of having multiple hidden layers in neural language models?",
    "What is the main challenge in integrating a language model with an end-to-end neural machine translation system?"
]