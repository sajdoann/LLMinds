[
    "j an state? formula the is attention the new calculating to in word given What decoder the input for",
    "dimensions 1 What document? are Table the of in provided the",
    "the What 33, is 80, significance in 81, 45, 71, the document? of 86, 84, 44, and numbers 80 40, 63, the 37,",
    "the attention neural is mechanism the translation models? purpose What main of machine in",
    "word? the and association the How mechanism attention the state between decoder each compute input does",
    "attention value mechanism representation? input contribution does to attention word normalized of use the the weigh How the the",
    "is of of during size training What using neural purpose translation the machine a models? batch",
    "from does models training translation the neural for objective differ other How training objectives? machine",
    "the is the document? What focus of main",
    "are of in RNN? authors the that to the contributed attention Who the development mechanism",
    "mechanism role in attention RNN? the What with sequence-to-sequence model the of is the",
    "et What the model is proposed Luong and difference between attention by local model attention al.? global",
    "et (2016a)? the purpose introduced gate What al. the of context Tu is by",
    "the al. et by the added step to Tu What model? the (2017) purpose of attention is reconstruction",
    "are words text? How many this in",
    "text? many in words How this are",
    "many this are How in words text?",
    "are How many words text? this in",
    "is this text? What of main topic the",
    "examples text? some of described the in are images What",
    "the is What purpose of of electronic diagram? circuit the the image",
    "be might error 0x80070426'? 'Error the code What",
    "software What be interface table? the with the might or list purpose a of",
    "is What the theme descriptions? among common these",
    "Can these specific from technical items of descriptions? any you identify types",
    "the the the is of last purpose in description? What text",
    "descriptions many How include images people? of",
    "you description? from its of context the image about infer the What can first",
    "among image descriptions all the the What model? by the theme common is generated",
    "patterns Can identify types any you the of described? images in",
    "context? or How with images model poor quality the of does lack handle",
    "image is model descriptions? What the this in generating of purpose",
    "accuracy and Can model's the generated the ways usefulness suggest to improve descriptions? of you",
    "of What the provided? topic the text is",
    "the you this context? and encoder role of in Can explain the decoder",
    "mechanism of What the is this the context? purpose attention in",
    "an difference model the and between sequence-to-sequence architecture? a What is encoder-decoder",
    "the in What this is function purpose of the context? softmax",
    "in the this What function cross-entropy is purpose context? the loss of",
    "purpose fine-tuning a pre-trained What is the of model?",
    "the sequence What context? in batch this size is and between length difference",
    "search beam context? in What is this the of purpose",
    "focus document? 5 is this What of main in Chapter the",
    "neural is when machine What the translation challenge training models?",
    "is objective models? training machine neural translation What for the",
    "training machine parallelism used translation to increase What neural is models? practical of in",
    "are pair? How in represented input a sentence specific words",
    "translation and network-based between of difference recurrent graph neural machine translation is statistical diversity? search What traditional terms in main machine the",
    "network-based in the is the mechanism neural of the model? What translation machine attention recurrent purpose",
    "generated source added the model attention link that output recurrent words authors were the neural an model? to to words seminal Who translation of work network-based in the machine",
    "of search translation? role the machine neural What normalization is network-based in recurrent during",
    "decoding? of the short-term were memory) the (long before network and a LSTM reversed authors source the the sentence used Who work order of that",
    "5? the of Chapter is What title",
    "do et Tu What al. source target to the trade-off model introduce and between context?",
    "the the proposed of name in attention Beamsearch What by model 2015b? is",
    "the new as conditioning for What text? the context formula in state, decoder in mentioned the the is",
    "table the What text? presented dimensions in the are the of",
    "numbers 84, 81, in 84, significance the 80, 71, 63, 86, the text? 37, and 40 the 45, 80, is 33, What of",
    "refer does in What of the context the text? to term this attention' 'accumulated",
    "decoder What context the of accumulated the a purpose adding in as state? conditioning is attention the",
    "attention model? purpose in using an neural of is What mechanism translation main machine the a",
    "the role of function mechanism? in softmax What the the attention is",
    "pairs a model? neural for sentence machine translation training How during processed are",
    "training of 100? batch How are sentence during represented a size with pairs",
    "is of the focus main What the document?",
    "NMT? have in the of the the development to Model Who Attention authors that significantly are contributed",
    "the Attention Model for role attention the mechanism of in is What NMT? the",
    "is for difference Attention between NMT? global the What attention and in Model attention the local",
    "of in the purpose on Model What the the work Tu gate Attention context et introduced al. for NMT? by their is",
    "for by NMT? the reconstruction Attention et is added al. What Tu of Model the step to the significance",
    "source.pdf? words many the How are title of in",
    "is the this of What text? main topic",
    "this What the in context of is of the text? model the function encoder-decoder",
    "images purpose What the is this of included in the text?",
    "How the on responses given inputs? does based generate encoder-decoder model",
    "is the significance context model? the in What the 'attention the of of term encoder-decoder mechanism'",
    "in is What encoder-decoder of the training model? technique the role the 'teacher forcing'",
    "is between model? and model an sequence-to-sequence a What encoder-decoder the difference",
    "model? encoder-decoder training What the purpose in is fine-tuning the of",
    "model? 'beam the What the generating the using of responses in role encoder-decoder algorithm search' is",
    "the training of using model? encoder-decoder purpose in the pre-trained models What is",
    "are What of images referring to? descriptions type these",
    "common the Can images? identify among elements any you described",
    "related industries What these technical images be to? might",
    "types of more technical are images any this set? prevalent in that specific there Are",
    "images infer can the based descriptions? the about these we What purpose context of or on",
    "the of are What common provided? characteristics the images",
    "these about man infer images? What we the can in",
    "or devices identify any objects specific you Can the images? in",
    "better we improve these quality can How images the of for analysis?",
    "are for these uses potential some images? What",
    "the of main topic text provided? is What the",
    "of mechanism the the attention the purpose What in text? is the context of",
    "explain the the Can softmax in attention is you how function mechanism? used",
    "'alignment' context the the mechanism? refer to term of in What attention the does",
    "machine in text? the of the role the is encoder in and translation decoder the What described model",
    "the purpose the What text? forcing the mentioned of in is technique teacher",
    "greedy between and decoding? the beam difference is What search",
    "the What purpose training function of translation in is model? the the cross-entropy loss machine",
    "the attention mechanism? What weights purpose the in attention (W_a) is matrix of the",
    "'scaling What attention factor' refer the in term does context to of mechanism? the the",
    "What provided the text excerpts? is main of the focus",
    "the attention layer purpose What a using feedforward mechanism? the is in of",
    "in values are the normalized How used computation? context attention vector the",
    "for training models neural machine translation objective on? the based is What",
    "models? practical neural translation of training GPUs are Why machine in important",
    "a represented sentence pairs sentences batch processing of at multiple How when once? is",
    "models? translation machine conflicting goals the What in neural are",
    "is the machine models? translation of neural purpose Search Beam What in",
    "How long translation machine models? neural it to take typically does train",
    "of the machine the corpus neural for is term used models? context What training the in a subset of entire translation",
    "purpose length in sorting What translation is machine pairs models? the neural of by sentence",
    "sentences dealing machine models the of lengths? with neural What role different of translation masks when in is",
    "a What up in the neural is maxi-batch into mini-batches models? purpose of translation breaking machine",
    "breaking corpus the it the up is of models? training neural translation maxi-batches in into shuffling purpose What before machine",
    "of between translation and the models? is a context machine difference in What neural the mini-batch batch a",
    "purpose is in 5.7 the provided the What document? Figure of",
    "main between What the machine recurrent translation? machine statistical neural (RNN) difference is and based traditional network translation",
    "machine translation? mechanism attention What is RNN-based the of in role the",
    "translation? model RNN-based for machine the introduced sequence-to-sequence Who",
    "two labeled of column '>'? table one is What the as and with rows ' and ' significance the",
    "context in al. target and introduce the model source Tu decoder? What et to do between trade-off a",
    "2015b? the in of What proposed model Beamsearch the is attention by name"
]