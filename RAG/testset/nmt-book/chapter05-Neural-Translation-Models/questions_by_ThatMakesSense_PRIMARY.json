[
    "What is the main difference between the encoder and decoder phases in this network?",
    "What are some limitations of the proposed neural translation model?",
    "What does the attention mechanism in the deep learning world refer to?",
    "What does the encoder in the machine translation model consist of?",
    "What is the primary function of the decoder in the machine translation model?",
    "What is the cost function that drives training in the machine translation model?",
    "What is the cost function during training?",
    "Why do we normalize attention values?",
    "What is dynamic computation graph creation called?",
    "How do output word states from a batch of sentences get lined up?",
    "What is batching in the context of training neural machine translation models?",
    "What is the purpose of sorting sentence pairs by length before breaking them up into mini-batches?",
    "What is overfitting in neural machine translation models?",
    "How many epochs does it take to train a neural machine translation model?",
    "What is the common stopping criteria for training neural machine translation models?",
    "What is the process of selecting the highest scoring word pairs for the next beam?",
    "Why do we normalize the score by the output length of a translation?",
    "What is the difference between the search graph in attention models and traditional statistical machine translation?",
    "What is the purpose of using an alignment model (attention mechanism) in the seminal work by Bahdanau et al.?",
    "Why can we no longer combine hypotheses if they share the same conditioning context?",
    "Why is the search graph less diverse in attention models compared to traditional statistical machine translation?",
    "What type of neural network is used in Cho et al.'s approach?",
    "Why do Sutskever et al. reverse the order of the source sentence before decoding?",
    "What is the purpose of Bahdanau et al.'s alignment model?",
    "What is the purpose of Luong et al.'s global attention model?",
    "What is the purpose of Tu et al.'s interpolation weight?",
    "What is added to the attention model by Tu et al. in their 2017 work?"
]