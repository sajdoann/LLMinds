[
    "What are stacked recurrent neural networks and deep transition recurrent neural networks?",
    "What is an alternating recurrent neural network?",
    "What is the purpose of residual connections in deep models for neural machine translation?",
    "What is the attention mechanism in neural machine translation models used for?",
    "What are some potential uses for word alignments beyond their intrinsic value in improving translation quality?",
    "How many words are in the title of source.pdf?",
    "What are the refinements discussed in this document related to?",
    "How do neural methods handle large vocabularies?",
    "What is the more common approach today for handling rare words in NMT?",
    "What is a popular method to create an inventory of subword units and legitimate words?",
    "What is the term used for the combination of left-to-right and right-to-left systems in the context of this document?",
    "What is the issue with integrating deep integration for the combination of left-to-right and right-to-left systems?",
    "What are the steps involved in the ranking process for the combination of left-to-right and right-to-left systems?",
    "What is forced decoding in the context of this document?",
    "What language-related problem does the term 'retweeting' represent?",
    "What is the role of Zipf's law in NMT?",
    "What are the two learning objectives in round trip training?",
    "How can we optimize both models in round trip training?",
    "What is the goal of optimizing both models in round trip training?",
    "In the combined model with a large neural language model, how can we balance the translation model and the language model?",
    "What is the objective of the back-translation idea from a strict machine learning perspective?",
    "How can we achieve a balance between the translation model and the language model using gated units?",
    "What is the purpose of back-propagating through time (BPTT) in the context of recurrent neural networks?",
    "What is the main topic of this text?",
    "What are the two main challenges faced when using RNNs for machine translation?",
    "What is the solution proposed for the vanishing gradient problem in RNNs?",
    "What is the solution proposed for the exploding gradient problem in RNNs?",
    "What are some other techniques used to improve the performance of RNNs in machine translation?",
    "What is the main idea presented in this text?",
    "What are stacked RNNs?",
    "What is the function f(h, h) in the equation for stacked RNNs?",
    "What is the function g in the equation for deep transition RNNs?",
    "What is the concept of alternating RNNs in the context of a deep version of the encoder?",
    "What is the main topic of the text?",
    "Can you summarize the key points about the improvements made to the machine translation system?",
    "What is the significance of the example given about the Spanish-English translation of 'el perro corre por el parque'?",
    "How does IBM Watson's machine translation system compare to other systems in terms of speed?",
    "What is the purpose of the work on machine translation by IBM Watson?",
    "Can you explain how the use of neural networks contributes to the accuracy of translations in IBM Watson's system?",
    "What is the role of parallelization in improving the speed of machine translation by IBM Watson?",
    "How does the use of context improve the quality of translations in IBM Watson's system?",
    "What is the potential impact of improved machine translation systems on global communication and collaboration?",
    "How does IBM Watson's machine translation system handle languages with complex grammar or syntax?",
    "Can you explain the concept of word alignment in machine translation?",
    "What is the purpose of enforcing word alignment in machine translation?",
    "How can we add given word alignment to the training process of neural machine translation models?",
    "What is the formula for calculating the attention given to an input word j in this context?",
    "What is the significance of the numbers in this document?",
    "What does 'i-1 j' represent in the context of this document?",
    "What is the purpose of adding the accumulated attention given to a word as a conditioning context?",
    "What does the table on page 2 represent in this document?",
    "What is the method described in the first document for domain adaptation?",
    "What is another less commonly used method for domain adaptation mentioned in the second document?",
    "In the second document, what are the four language models used for detecting in-domain and out-of-domain data?",
    "In the second document, how is the relevance of a sentence pair from the out-of-domain data calculated?",
    "In the second document, what are some possible modifications to the method for improving its performance?",
    "How can the subsampled data be used according to the second document?",
    "What is the debate in this document about?",
    "How can linguistic annotation be added to the input words in a neural machine translation system?",
    "What are some examples of linguistic annotation that can be added?",
    "Why is it argued that providing additional linguistic knowledge to the neural machine translation system may make the job easier for the machine learning algorithm?",
    "What is the purpose of using BEGIN, CONTINUATION, and OTHER annotations in linguistic annotation schemes?",
    "What is the main idea of the text?",
    "What are some ways in which components can be shared among language-pair-specific models?",
    "What is the purpose of sharing components in neural machine translation models?",
    "How can monolingual data be used to train neural machine translation models?",
    "What is the main finding of Johnson et al. (2016)?",
    "What do Johnson et al. (2016) study?",
    "How does Johnson et al. (2016) train their model on parallel corpora for various language pairs?",
    "What are the small benefits found by Johnson et al. (2016) in their study?",
    "What are the mixed results shown by Johnson et al. (2016) in their study?",
    "What does Firat et al. (2016) support in terms of language input and output?",
    "How do Firat et al. (2016) achieve multi-language support in their model?",
    "What are stacked recurrent neural networks and deep transition recurrent neural networks?",
    "What is an alternating recurrent neural network?",
    "What are residual connections in deep models and why are they beneficial?",
    "What is the purpose of guided alignment training in neural machine translation models?",
    "How many words are in the document title?",
    "How do neural translation models typically handle large vocabularies?",
    "What is the common approach today for handling rare words in NMT?",
    "What is a popular method for creating an inventory of subword units and legitimate words?",
    "What is the purpose of using byte pair encoding in NMT?",
    "What is the term used for the idea of using multiple systems with different random initialization in the context of NMT?",
    "How does the deep integration described in the document not work for the combination of left-to-right and right-to-left systems?",
    "What is the process for combining scores from different models in the context of NMT?",
    "What is forced decoding in the context of NMT?",
    "What language(s) have a normal writing order that is right-to-left?",
    "What are the two learning objectives in round trip training?",
    "How can we train two machine translation models in round trip training?",
    "What is the purpose of round trip training according to the text?",
    "How can we achieve the balance between the translation model and the language model in the combined model with gated units?",
    "What are the two methods mentioned for improving machine translation models?",
    "What is the concern when updating both the parameters of the large neural language model and the combination layer in the combined model during training?",
    "What is the objective of back translation from a strict machine learning perspective?",
    "What is the main idea of this text?",
    "Can you explain how LSTMs work?",
    "What is the purpose of the input gate in an LSTM?",
    "What is the purpose of the forget gate in an LSTM?",
    "What is the purpose of the output gate in an LSTM?",
    "What are the benefits of using LSTMs over traditional RNNs?",
    "Can you provide an example of how LSTMs can be used in practice?",
    "What are some potential challenges when working with LSTMs?",
    "How do you choose the number of timesteps (or layers) in an LSTM?",
    "What is the difference between an LSTM and a GRU (Gated Recurrent Unit)?",
    "What is the main idea of the text?",
    "In Figure 6.6, what are the two ideas combined?",
    "How does the function f(h, h) work in the text?",
    "In the text, what is added to deep RNNs for encoders?",
    "What does Figure 6.7 show?",
    "What is the main topic of the text?",
    "What are some industries that IBM Watson's language translation service can be applied to?",
    "What is the purpose of using machine learning in language translation?",
    "How does IBM Watson's language translation service work?",
    "What is the advantage of using IBM Watson's language translation service over traditional methods?",
    "What is the role of context in IBM Watson's language translation service?",
    "How can IBM Watson's language translation service be customized to meet specific business needs?",
    "What is the significance of the example given about a patient with a rare disease in a remote location?",
    "What is the potential impact of IBM Watson's language translation service on global commerce?",
    "What are some challenges faced by IBM Watson's language translation service?",
    "What is the relationship between Obama and Netanjahu, and how long have they been in this relationship?",
    "What is the purpose of adding given word alignment to the training process for neural machine translation models?",
    "How does the attention mechanism work in neural machine translation models?",
    "What is the role of the alignment points in neural machine translation models?",
    "What is the goal of training neural machine translation models?",
    "What is the formula for the new conditioning context in the decoder's state, as described in the text?",
    "What is the purpose of adding the accumulated attention given to a word as conditioning context in the decoder's state?",
    "What are the dimensions of the table shown in the text?",
    "What is the significance of the numbers '37', '33', and '13' in the first row of the table?",
    "What does 'i-1' mean in the context of this text?",
    "What is the method described in the first document for domain adaptation in machine translation?",
    "What is another less commonly used method for domain adaptation mentioned in the second document?",
    "What are the detectors used in the method described in the second document for extracting in-domain data from large collections of mainly out-of-domain data?",
    "What is the formula used to calculate the relevance of a given sentence pair from the out-of-domain data in the method described in the second document?",
    "What is the purpose of using part-of-speech tags or word clusters instead of open class words (nouns, verbs, adjectives, adverbs) in some work mentioned in the second document?",
    "What is the debate about in this document?",
    "How can linguistic annotation be added to a neural machine translation system?",
    "What is the purpose of providing richer input representations in a neural machine translation system?",
    "Why might it be beneficial to provide linguistic annotation to a neural machine translation system even if the model can automatically learn some of the features?",
    "Why might it be argued that forcing a machine learning algorithm to discover features that can be readily provided be harder than needed?",
    "What is the main idea of multiple language pairs in neural machine translation?",
    "What are some ways to mark the output language in the context of systems for a single language pair?",
    "What is the concept of sharing components in neural machine translation models with different language pairs?",
    "What is the purpose of sharing the encoder in multiple language pair-specific models?",
    "What is the purpose of sharing the decoder in multiple language pair-specific models?",
    "What is the purpose of sharing the attention mechanism in all models for all language pairs?",
    "What is the purpose of sharing components in neural machine translation models and exploiting monolingual data?",
    "What is the potential issue with training the decoder in isolation with monolingual language model data?",
    "What does Johnson et al. (2016) explore in their study?",
    "How does the model of Johnson et al. (2016) train on parallel corpora for various language pairs?",
    "What are the benefits shown by the model of Johnson et al. (2016) for input languages with the same output languages?",
    "What is the result when translating into multiple output languages according to Johnson et al. (2016)?",
    "How do the encoders and decoders of Firat et al. (2016) work together?"
]