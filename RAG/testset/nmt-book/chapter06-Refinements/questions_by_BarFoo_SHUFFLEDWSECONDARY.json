[
    "neural networks? networks and recurrent deep What transition neural stacked are recurrent",
    "recurrent is neural network? What an alternating",
    "purpose models residual translation? What in the neural is for deep of machine connections",
    "What machine mechanism neural used translation in attention models for? is the",
    "quality? for improving some potential value are translation word uses alignments What their beyond intrinsic in",
    "many are How words in of source.pdf? title the",
    "the document this are to? related discussed refinements in What",
    "neural handle do How large methods vocabularies?",
    "for rare words NMT? more approach in the today What common handling is",
    "popular units words? to a What of subword create legitimate an and method inventory is",
    "the systems for the document? the What context used left-to-right and right-to-left in of term combination is of this",
    "integration deep combination What the integrating and is issue the systems? of for right-to-left with left-to-right",
    "the and left-to-right right-to-left the ranking process the involved combination for What of systems? are in steps",
    "is forced context in What the this document? decoding of",
    "represent? What language-related term the problem 'retweeting' does",
    "Zipf's NMT? role is in law of What the",
    "objectives in training? two round What the trip are learning",
    "in we trip both optimize training? round How models can",
    "the optimizing both training? trip What of models is in round goal",
    "model model, combined a model? how balance model the the the and In large language neural translation we can with language",
    "is the a learning back-translation machine of strict What the from idea objective perspective?",
    "model gated How between the model balance the and achieve units? we language translation using a can",
    "through What recurrent time networks? of of neural context is (BPTT) back-propagating in purpose the the",
    "this of topic is text? What main the",
    "for when machine main are using RNNs the two faced translation? challenges What",
    "proposed vanishing for solution problem in RNNs? is What the gradient the",
    "problem RNNs? the solution What is for exploding proposed the gradient in",
    "performance What in techniques improve machine are used RNNs other translation? to of the some",
    "in idea What the is presented this main text?",
    "stacked What RNNs? are",
    "f(h, the is stacked What RNNs? function h) equation in the for",
    "in transition deep is RNNs? g the equation the What for function",
    "RNNs What the in the the concept of is version of alternating a deep encoder? context of",
    "of topic text? main the What the is",
    "the you system? the to points the improvements summarize about machine key made translation Can",
    "the parque'? example corre perro el about What por the is of Spanish-English given significance translation of the 'el",
    "translation How machine Watson's speed? to does system of in compare systems other IBM terms",
    "the on machine purpose IBM work by What is of the translation Watson?",
    "translations the use explain of system? in the you contributes Watson's how neural to of IBM Can accuracy networks",
    "translation machine of the role is speed parallelization of What improving in IBM Watson? by the",
    "Watson's How of the IBM does of translations use improve context the quality in system?",
    "machine is and of on improved potential What the impact translation communication systems global collaboration?",
    "or IBM translation does system complex Watson's with grammar How syntax? languages machine handle",
    "alignment word the of translation? in Can concept explain you machine",
    "purpose of the word machine enforcing What is alignment in translation?",
    "translation can the add machine training process word we neural alignment given of How models? to",
    "in context? this to the is the an calculating formula given attention j input for word What",
    "in the numbers What the of this significance document? is",
    "j' this in 'i-1 does represent document? of the context What",
    "word given of to context? accumulated the a the purpose What a adding attention is as conditioning",
    "on page does in the this table What 2 represent document?",
    "the in first the What is method for described document domain adaptation?",
    "second another method document? used domain What commonly less is adaptation mentioned for in the",
    "what and the data? models used In for four second language the in-domain out-of-domain document, detecting are",
    "a second the calculated? pair of how relevance the In document, the data out-of-domain sentence from is",
    "some second document, its performance? improving possible to are for method the the In modifications what",
    "the How be to used the document? second subsampled according data can",
    "in debate What is document the this about?",
    "annotation system? linguistic machine How be the translation input words neural in to can a added",
    "annotation can linguistic are be What added? that some examples of",
    "algorithm? may system additional is machine machine for the it that make the easier learning Why linguistic providing to argued translation neural the knowledge job",
    "using BEGIN, the is in purpose OTHER What of CONTINUATION, annotations linguistic annotation schemes? and",
    "the What the main is text? of idea",
    "ways models? shared which among language-pair-specific some What can be are components in",
    "the models? machine translation in is of purpose What sharing components neural",
    "How to be data train can neural monolingual translation models? machine used",
    "et (2016)? of finding al. main is the What Johnson",
    "al. Johnson do What study? (2016) et",
    "on corpora pairs? language their (2016) various al. et parallel How model Johnson train does for",
    "small (2016) found in the by their are What et Johnson study? al. benefits",
    "What the their in Johnson et (2016) by shown results are mixed study? al.",
    "terms output? support (2016) language Firat al. input in does What and et of",
    "et How Firat in their multi-language al. support achieve model? do (2016)",
    "and recurrent recurrent networks stacked neural are What networks? deep transition neural",
    "recurrent What an network? neural alternating is",
    "in are What are residual beneficial? they deep connections why models and",
    "guided purpose neural translation models? training of is machine alignment the What in",
    "words title? in are many document the How",
    "typically models neural do vocabularies? handle translation large How",
    "common handling approach the for rare in is today words What NMT?",
    "units What an subword for method creating a inventory popular is of legitimate and words?",
    "What encoding byte purpose the is in of pair NMT? using",
    "the idea used for the systems in initialization context NMT? multiple What term different of with the is random of using",
    "the described not of systems? integration combination the and does document How for deep the work in left-to-right right-to-left",
    "What of the combining for models is process the in NMT? different from context scores",
    "NMT? of the forced decoding What in context is",
    "What is normal order have right-to-left? that language(s) a writing",
    "in training? learning the What objectives are trip two round",
    "translation can machine in round two models train training? trip How we",
    "according training the What text? is purpose of to the trip round",
    "between we model the the translation combined can balance model model How language gated with achieve the in units? and the",
    "machine methods What are the translation two models? improving for mentioned",
    "layer both combination concern of language the large the neural What during the model training? in updating model when parameters is and the combined the",
    "objective translation strict a back the machine learning from of is What perspective?",
    "text? main What idea the of is this",
    "Can LSTMs work? explain how you",
    "of an LSTM? input gate the What is the in purpose",
    "LSTM? is purpose the What an forget in of gate the",
    "gate LSTM? an What is in output the of purpose the",
    "LSTMs over traditional using RNNs? are benefits the of What",
    "practice? you LSTMs example can be an Can in how of provide used",
    "What with LSTMs? when challenges working some potential are",
    "LSTM? you the layers) How do choose timesteps in an of number (or",
    "LSTM GRU difference Unit)? a between is Recurrent an and (Gated the What",
    "is of the main What idea text? the",
    "combined? the are two what Figure In 6.6, ideas",
    "does in the text? the function How work f(h, h)",
    "the text, deep to added is what RNNs for encoders? In",
    "Figure does What 6.7 show?",
    "is the of main text? What topic the",
    "industries to? What service be applied some can language are that translation IBM Watson's",
    "learning machine of translation? purpose What the is language using in",
    "does service work? language Watson's How IBM translation",
    "of What the over methods? translation traditional IBM using service advantage is language Watson's",
    "role IBM context is What language service? of Watson's translation in the",
    "translation needs? IBM Watson's customized can language specific be How to meet business service",
    "with location? the about What remote is a the rare a disease example patient significance of in given a",
    "service impact global potential What IBM Watson's is on language translation the of commerce?",
    "are service? faced IBM What challenges by language translation Watson's some",
    "they and long between Netanjahu, have in What been relationship? and Obama is relationship the how this",
    "training the translation for purpose process is alignment word given neural What machine of to models? adding the",
    "machine translation neural attention models? the does in How mechanism work",
    "role the machine is points in alignment models? the translation neural of What",
    "training goal machine translation of is the What neural models?",
    "What for formula as text? described the conditioning decoder's new the state, is in in the context the",
    "context the purpose attention in What adding is a state? the word as conditioning accumulated of given the decoder's to",
    "the shown the of the What dimensions in text? are table",
    "in significance table? '33', the of of and the first What numbers row the '13' the is '37',",
    "the mean of 'i-1' context does What in text? this",
    "in domain described for document adaptation is in method translation? machine the the first What",
    "commonly for less another What mentioned the is method document? used domain in second adaptation",
    "collections described mainly data are for detectors data? in-domain large the from of the extracting used out-of-domain second What in method the in document",
    "out-of-domain described to a formula the given in data calculate of What from used relevance the method second document? the pair is the the sentence in",
    "verbs, instead the the clusters document? second purpose of some or open using of (nouns, work mentioned adverbs) in in What part-of-speech word adjectives, tags is words class",
    "What this about debate document? in is the",
    "linguistic a neural to can system? How machine added translation be annotation",
    "a system? richer neural is machine What input providing translation the of in purpose representations",
    "translation of some linguistic Why if the provide annotation can learn features? a automatically it might be neural to even machine to the system beneficial model",
    "discover harder a needed? readily that Why provided it to algorithm machine be argued be can that than learning forcing features might be",
    "main machine What multiple idea of is neural language pairs the translation? in",
    "some output to language of for in What the language a ways the single systems mark are context pair?",
    "neural components of machine pairs? language with the different What in concept sharing models translation is",
    "is language sharing the in encoder models? pair-specific multiple purpose of the What",
    "in sharing language models? is the pair-specific decoder multiple the What purpose of",
    "models of sharing purpose What pairs? the is the all attention mechanism language in for all",
    "data? the neural purpose in of components What is and exploiting machine models monolingual sharing translation",
    "model monolingual the decoder in training with language potential with isolation the is What issue data?",
    "study? What (2016) in explore al. their et does Johnson",
    "model corpora train Johnson How language the pairs? various al. et for (2016) parallel on of does",
    "by et What are the benefits the the for model with of input same languages languages? al. output (2016) Johnson shown",
    "et according is result the output Johnson What translating al. languages into when to multiple (2016)?",
    "decoders work and the (2016) together? et How encoders al. of Firat do"
]