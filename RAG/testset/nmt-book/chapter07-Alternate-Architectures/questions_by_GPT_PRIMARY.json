[
    "What are some limitations of using recurrent neural networks for processing input sequences in machine translation tasks?",
    "How do convolutional neural networks (CNNs) differ from recurrent neural networks in their approach to machine translation?",
    "What challenges arise from compressing an entire sentence into a single vector representation in neural machine translation models?",
    "In what way does the architecture combining convolutional neural networks with attention mechanisms improve upon previous models?",
    "How does self-attention enhance the encoding of input sequences in neural models for translation?",
    "How do residual connections in self-attention layers facilitate the training of deep neural models?",
    "What distinguishes the self-attention mechanism in the decoder from that in the encoder?",
    "Why is layer normalization an important step after self-attention computations?",
    "How does the use of multi-head attention improve the capacity of the model to encode information?",
    "What are the benefits of replacing recurrent neural networks with self-attention layers in sequence modeling?"
]