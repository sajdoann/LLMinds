[
    "What is the disadvantage of using recurrent neural networks on the input side?",
    "What is the hierarchical process of building up a sentence representation bottom-up, similar to?",
    "What is the problem that the decoder has to decide on in the reverse process?",
    "How do convolutional layers encode a word with its left and right context?",
    "What type of neural network is used in the encoder?",
    "How are words processed in parallel in the convolutional version of the decoder?",
    "What is self-attention?",
    "How does self-attention work?",
    "What is the purpose of residual connections in the self-attention layer?",
    "What is added to the output of the attention computation?",
    "What is added to the output of the attention computation?",
    "What layering technique is used in the model to skip over deep layers and speed up training?",
    "Who proposed a refinement of the model that incorporates wider context with each layer?"
]