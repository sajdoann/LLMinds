 Okay, so now we have not discussed and we will come to that in a second, we have not discussed the translation model. The translation model is the probability of the source given the target or possibly vice versa. But before we do that, let's still do one more generalization of the overall approach. Let's move from the noisy channel to something which is more general. And this observation the one who set up the successful machine translation at Google when it still was phrase based. And he is the student of one of the students of Hermann Ney from Aachen. And in his thesis in 2002, he made just some experiments. And he observed that sometimes if the language model is trained on much larger collection of data, it can be so good that it is better to trust that language model more than the base law would recommend. So empirically, it made sense to just square the probability of the language model. And that is something which you cannot do with the base law. You just cannot enter a square function somewhere in your formula. And another observation was that, well, base law forced you to swap the order. So the conditional probability was of the source sentence given the target. But if you accidentally swap the directions in your training and you use the wrong table, the system worked equally well. So there was no loss from swapping the directions, despite it violates the base law. And the reason is that in a large collection of text, it doesn't really matter much in which way you are looking at the table. So you can use this swapped. So you can use this wrong base law and that works equally well. So the problem, technical problem here is how do we write the formulas so that people don't laugh at us when submitting the papers. We cannot just mess around the formulas and say, well, it worked better. So we used to square instead of the proper numbers. And we added up instead of multiplying and it empirically helped.