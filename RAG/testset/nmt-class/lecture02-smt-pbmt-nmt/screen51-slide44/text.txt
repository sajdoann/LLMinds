 So what is the solution? The solution is to use word embeddings. The idea is you map these long word vectors into much shorter word vectors. Instead of 1 million zeros and 1,1 in that you will use 300 to 2,000 dimensions and these dimensions will be the new features describing the words. So the embedding is the mapping and that's the first layer of the neural network to convert this of words into the deep one and the embeddings are trained for each particle task as a side effect of training for the task. So you can download some pre-trained word embeddings and these come from various versions of language modeling tasks. So they are not related to translation in any way. The embeddings that emerge in training neural MT systems are found out automatically by the system so that they distinguish the words in ways most useful for the translation. So if some synonyms in the source language correspond to synonyms in the target language, they will arrive at the very same embedding. If some words are ambiguous, they will again, this will be reflected in the embeddings based on the target language. So the famous word to WAC, that's a language model style pre-trained approach. but there is two different tasks. So each task of processing words with neural systems will give you some embeddings.