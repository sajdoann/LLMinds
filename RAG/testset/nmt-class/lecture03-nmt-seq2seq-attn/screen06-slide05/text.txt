 Properties of the two languages. Yeah, so the basic building block of neural networks is one fully connected layer. The layer is something which takes a vector of inputs and then these vectors of real numbers is converted to an output vector which is denoted H here and this transformation is overviewed or this transformation is driven by matrix I'll just say that the basic building block is one fully connected layer that takes a vector of inputs. So these inputs are real numbers and then they are converted to some output vector again a sequence of real numbers and this transformation is governed by a matrix the weight matrix that tells you how to combine the input how to combine the input features. So these are the input features and then this is like a preliminary calculation after application of the matrix and so this like weights all the inputs in some way and then you add a bias which is another trained vector. So this vector is what you input what you give as the input to your computation after it has been trained. The weight matrix and the bias are trained terms and this vector is an intermediate state of calculation. So you first reweight the input features with the weight matrix and then you add the bias term and you can think of this bias term as like the basic activation if there is no input. So this is like this this sets the level of the C how high the C should should be and then after the application of the bias the basic level whether we should fire or not the non-linearity is applied and that's that's the the function fH. So the one layer computation can be summarized with this simple formula multiplication of a matrix of a matrix and a vector plus addition of a vector and element wise application of a non-linearity function. So this is the basic. do you think the basic. you think the basic. you think the basic.