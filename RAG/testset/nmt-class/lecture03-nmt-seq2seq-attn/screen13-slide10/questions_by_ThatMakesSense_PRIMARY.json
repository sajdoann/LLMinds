[
    "What does the transformation A do in a vanilla recurrent neural network?",
    "How is the input vector for the fully connected layer defined?",
    "Why does the state of the network radically change at every step?",
    "What does it mean for the learned representation to be 'close to impossible to interpret'?",
    "What is a potential issue with using a simple vanilla layout in recurrent neural networks?"
]