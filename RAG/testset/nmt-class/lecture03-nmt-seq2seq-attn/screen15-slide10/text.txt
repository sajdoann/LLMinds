 This problem is also realized in something which is called the vanishing gradient problem. So if you know, if you knew more about the backpropagation algorithm, there is the change of the parameters that has to propagate backwards through the network to update the weights. and if all these changes of the parameters go through the non-linearity at every step, then in a deep network and remember the network is now deep as the network is now as deep as the input is long, then the values will get smaller and smaller. So if we go back to.