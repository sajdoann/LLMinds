[
    "What is the idea behind Word (Actually Token) Embeddings?",
    "How many dimensions does a token have after embedding?",
    "What is the term for the mapping of tokens to dense vectors?",
    "Which layer of NNs for NLP technically maps 1-hot input to the first layer?",
    "How are embeddings trained for each particular task?",
    "What are examples of tasks where embeddings are used?",
    "What is a famous method for training word embeddings?",
    "What does CBOW predict in the word2vec method?",
    "What does Skip-gram predict in the word2vec method?"
]