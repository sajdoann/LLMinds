 The linguistic point of view, but it is nice from the mathematical point of view because it reduces with all the zeros. It cancels all the parts of the computation. So the loss is then the cross-entropy between the distribution that your soft-mach function produce, the hat p hat and this true distribution. And this true distribution is the one with all zero and one for the word that you were expecting.