[
    "What is the primary purpose of attention mechanisms in neural network architectures for language processing?",
    "How does the use of a fixed-size input representation in sequence-to-sequence models affect the translation or generation quality for long sentences?"
]