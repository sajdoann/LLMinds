 If you consider this setup, here is a horrible bottleneck in the network. The whole input sentence is converted to a fixed size vector and then this fixed size vector is used typically only once. It varies across implementation but it is used typically only once at the beginning to initialize the decoder state. And through the predictions of the output the state is also being updated so as the network is producing the outputs it is more forgetting what the original input was. The representation of the input is becoming less and less visible. It is like overwritten in some way with the words that were produced so far. So at the end of the out... at the end of the... towards the end of the sentence of the output the decoder is fabulating. It is no longer linked to to the input. So what we could do is that we could inject this encoding of the input sentence to every step of the network. And that alone helps a little. So at every stage the network could... could like reconsider which... which words it... it should look at or it would have still access to the first sentence representation. But even with this approach there is a fixed size representation. of the whole sentence and this fixed size representation is more... like intense or represents better the last words... the last tokens in the input than the early ones. Because again the information consumed at the beginning is kind of all written as the encoder was digesting the input sentence. So the fixed size representation even if we injected it here at every step would not be an adequate representation of of all the words. What helped in early... early implementations was to input the... was to insert the input sentence in the reverse order. So in the reverse order the... the beginning of the sentence was consumed last. So it was most... most... salient, most visible in the sentence representation. And while it was so active, so well represented, the beginning of the... of the decoder ran smoothly and produced nicely what... what it... what we were expecting. So then the... the problem was the fabolation was... was still there. But at least the beginning of the sentence was... was good. So it like... the decoder started in a reasonable... sub space of the... of its... of the space of its states. And then... there it was more likely to produce... even the... the tails... nice. But that was obviously like reading the sentence from... ...backwards is not... not a systematic... a nice way. And it doesn't allow us to... like... consider all the words with... equal importance...