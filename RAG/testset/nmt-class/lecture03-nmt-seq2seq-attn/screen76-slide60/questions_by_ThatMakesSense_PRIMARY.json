[
    "What is the purpose of using the attention mechanism in this context?",
    "How does the decoder represent target sentence dependencies?",
    "What information does the attention mechanism provide at every time step?",
    "Why are long distance dependencies not represented within one single vector?",
    "What will be used as a query for the source word sentence?"
]