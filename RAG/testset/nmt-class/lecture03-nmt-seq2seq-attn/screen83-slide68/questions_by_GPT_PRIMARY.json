[
    "How does the attention mechanism modify the traditional encoder-decoder architecture in neural networks?",
    "In what ways does the decoder utilize the encoder's outputs during the generation of each output word?"
]