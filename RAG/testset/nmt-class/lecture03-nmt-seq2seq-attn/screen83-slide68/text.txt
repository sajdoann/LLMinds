 So the way the attention mechanism is inserted into the encoder-decoder approach is very simple. Well, encoder is the same. It's a bidirectional encoder. The output of the encoder is used differently. The output is now used in all its positions and you can come back to these positions whenever you like. And the decoder is different but it's not much different. Here the decoder at every time step