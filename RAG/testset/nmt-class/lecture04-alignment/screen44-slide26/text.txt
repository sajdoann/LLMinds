 So here is an example the inputs that we have is the lexical probabilities so the dictionary of all the target words given the source word so the source sentence is does house is client and for each of these words in our lexical translation table we know that does can be translated as the that which who and so on and we know the the lexical so there is the probability regardless any context similarly for house we have seen this before similarly for east and similarly for client and then if you consider the direct alignment where the words the words corresponded like monotonically to each other then the probability of the whole sentence in model one in IBM model one is this normalization times the product of the lexical probabilities of the words so the was aligned to does so we take this point seven multiplied with this point eight which corresponds to house that's so you see that this the particular sentence that we had on the previous slide was the most likely lexical translation of the source sentence because for each of the words we use the top scoring top scoring element in the list so this is the situation when we know the source sentence know the target sentence and know the lexical probabilities for each of the words the IBM model one defines how do we calculate the probability of the whole sentence given given the alignments and given the words and given the source sentence