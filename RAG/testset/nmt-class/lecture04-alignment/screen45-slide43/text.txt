 But the question is how do we obtain these lexical probabilities? So this... So we would like to somehow estimate these lexical probabilities from a parallel corpus. This is where we started. At the beginning I told you if you look up the word house in a big corpus you will find that the English version of that is house most frequently. But that already relied on some alignment. So this is the chicken neck problem. If we had the alignments then we could easily estimate the parameters in the generative model. And if we had the parameters in the generative model, so if we had this lexical dictionary of word probabilities then we could estimate the alignments. So the question is how to tear this loop apart. And...