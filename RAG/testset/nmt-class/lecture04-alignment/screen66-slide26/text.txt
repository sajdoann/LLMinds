 Quick summary what is happening here and now let's put it together. Remember we need to know how likely are the alignment points what how much do we trust them for a given source and target sentence. If we know the probability of the alignment of the matrix of zeros and ones then we can use this to collect an updated fractional counts and that will refine our lexical translation probabilities. So to get the probability of the alignment given the source and target sentences we had the chain rule applied and now we plug in what we have simplified. So here we plug in simply the definition of the IBM model 1 alignment and here we plug in now after the trick the swapped product and sum. So this is all the possible alignments of the sentences of the given pair of sentences and considering the lexical alignment between the words in those sentences. So here the normalizations cancel out and the product can be done after the division and not vice versa. So this is what makes the computation tractable. It's this is simply the normalization that you have seen in the.