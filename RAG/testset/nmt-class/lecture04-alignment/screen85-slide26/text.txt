 So we are closely coming to the end. So the main use of word alignment these days is for extracting dictionaries. The main use before NeuralMT came was to use the word alignments in phrase-based machine translation. And we have already discussed why the word alignments are problematic from the linguistic point of view. But they were also problematic from the statistical point of view because these word alignments were in no real with the phrases that were then used. So the probabilities were kind of ill-defined. So there was a technique how to move from word alignment to true phrase alignment based on the phrase-based model. But it didn't bring enough improvement in translation quality. So that's why people don't use it.