 The choice which of these output options is the best one is driven by the log linear model. So as you know from the second lecture we are no longer using the simple Bayes law. When we are defining the conditional probability of the target sentence given the source we use this equation which summarizes which sums over defined an arbitrary number of and each of these feature functions the individual HMs considers the current translation candidate from some point of view. And so we will talk about these features in a second. You score each of the candidates with all the features. Each of these features is associated with the weight and that gives you the weighted sum of that. And because we are running we are running this probability estimate within the argmax. We are actually interested in the sentence itself and not its probability. We only want to know which sentence will win in this competition. We don't need to normalize. So we don't need to see how many score points have we allocated given out to all possible output sentences. So we can only check who has obtained the highest score of the features. So that is the log linear model.