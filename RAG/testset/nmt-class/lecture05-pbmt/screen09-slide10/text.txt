 That's the slide that you have seen at the beginning. You have your parallel corpus. You have your word alignments between the words. And you extract all phrases up to a given maximum length, which was normally set to ten words, that are consistent with the word alignment. And this huge list of phrase pairs, that is exactly your set of observations. This is where you do the counting. So you count how often the English phrase this time around was paired with nini and how often across the many sentences it was paired with tou, to dobo and so on. So you have many many possible translations. For the same phrase you will see which of these translations is most likely later on. Now you only collect the counts and do the division to normalize that to make it conditional probabilities. One thing is important to know pretty well. That's the consistency with word alignment. so the consistency makes sure that no word is forgotten. So to illustrate the consistent phrase, like phrase consistent with the word alignment here at the beginning of the sentence is nini corresponding to this time around. An example of an inconsistent phrase would be saying that nini can be translated as this time only. So this smaller rectangle would not be a valid phrase pair. Because there is one word which is also kind of involved in the meaning of nini. So the word alignment, the points here indicate which words correspond to the meaning of this source word. And we must not forget any of those. So the consistency with the word alignment makes sure that you extract rectangles so that always, that you touch a column all the dots in that column all the dots in that column are included in the rectangle and whenever you touch a row all the dots in that rectangle are there. So similarly, even cannot be translated based on this sentence as dokonce only. To translate even correctly based on the evidence from this sentence you have to use the two words dokonce jeszcze. So that is the consistency of phrases with the word alignment. and you extract all phrases. Short and long and they are overlapping in many ways. So you extract this time around and you are also extracting this time around they're moving. So if your sentence is shorter than the 10 word limit, then you also extract the whole sentence as one translation unit. And the phrase based system then has the capacity to simply copy the full output as it has and the same. So that is the general quality in the average case because the system is very good at copy pasting and what humans wrote is the best thing that you can do. And there is also a very strong like robustness of this phrase table because the same phrase is extracted in many small variations with a few words around and it's also extracted in the smaller pieces. So you can reconstruct the same output using the smaller pieces or the longer pieces. So the phrase table is like a robust data structure. It contains the observations in many, many possible ways.