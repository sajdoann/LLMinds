 So what we need to do now is to fight the large search space. And if you remember we were gradually expanding hypothesis and I was saying that sometimes the phrases are short and translate individual words and sometimes the phrases are large cover more words at once. It is obvious that the same input string can be now translated with shorter phrases and longer phrases. the candidate hypothesis the partial hypothesis in this example could be it is as two separate like expansion steps or it is covering the two words at once. So this is the different segmentations when I was talking about the the segmentation as the hidden parameter in the in the maximization. This is exactly where we should be summing the probabilities of these two paths to to know the probability of the it is being the translation of the source phrase whatever whatever that was. But I've told you the summation is not done in practice. What people do in practice is only choose the high scoring one. So these two candidates translations or these partial hypothesis are scored against each other and the one which will win in the end is the one which is higher scoring in the in the calculation. Well if we are searching for the higher scoring one why do we want to expand the whole trees that further go down from these two hypotheses. So there is no point if one of these hypotheses for example this single step one is higher scoring one there is no point in expanding all the continuation of this and also all the continuation of this. The continuations are identical. so we can reduce the continuation of the of the search by factor of two to half by recombining these two hypotheses. So the system whenever it constructs a new partial hypothesis it will check whether it already has a hypothesis which covers the same set of input words and produce the same output. And if this is the case then it's very that's it's very natural to combine these two hypotheses so we do not create the the second variant and we join them together and then we expand all the future only once. This type of pruning it is kind of pruning of the search space it's reduction of the search space but I should not use the word pruning. This is this is a safe modification of the search space. We are not losing anything. We are only ignoring something which will be ignored anyway in the end. So there is no difference in the scores of the future. The future of these two candidate hypotheses is totally identical. Unfortunately this safe recombination