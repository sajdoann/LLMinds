 It's not sufficient to limit the search space. But before we get to that, let's realize one more thing. We can recombine also hypotheses that do not have the exact same output, but that are sufficiently similar to put it in simple words for now. So for example, the input sentence was again something in German and there is these two candidate hypotheses, these two partial translations. The one was the first word was translated first as others. There's an error in the picture. So first the second word was translated as it and then the surrounding words were translated as does not. So we have one hypothesis which says it does not and we have another hypothesis which translated the second word differently. It translated the second word as he. So he does not. So we have two now different if you consider the full history. One of them says he does not, the other one says it does not. You have to consider what scores will be used to judge which of these hypotheses is better in the future. So I'll come back to this again in a while. But every of these hypotheses whenever it's expanded is scored with the phrase probabilities, the scores associated with the phrase and also with the language model. So if we assume that we use a trigram language model, then whatever we attach here, we only look at the last two words of this because the trigram will then have to consider the next word after that. So the short summary is that whenever two hypotheses have the same last two words and we use only a trigram language model, we can combine them. and it is still safe. The beginning, the difference in the beginning will not affect the scores of the future. So when I was talking about the subtrees of expansion, the future paths from this does not after he and from this does not after it, the scores in these two subtrees will be identical because none of the scoring functions will look into some deeper history. so the last two words. So the recombination can put together two hypotheses whenever they translated the same source words, so the coverage vector is identical and the history considered by all the other features is identical. The history which is beyond that, which is out of scope of the scoring functions, whether the beginning was he or it, that is no longer important. So that doesn't make any difference. the difference in the scores beyond that. So this is the safe pruning or safe reduction.