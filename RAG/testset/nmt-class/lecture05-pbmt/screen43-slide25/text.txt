 Okay, so now we need to explain the last part and the last part is the weight optimization and that is the actual training in the machine learning sense. So you have the phrase tables, you have the language model probabilities and you are searching the space with the with the beam search with the stack based beam search and you start with some initial weights and it's important to realize that because of the pruning in the stack based beam search the weights influence which hypothesis will survive. So if your weights are set on for in an unfortunate way you are searching in the wrong part of the of the search space. So we need to somehow find the best balance of the weights. but we need to always retranslate because with every given weight setting we would be exploring a different part of the search space. So this is the that's the idea. That's one part of the idea of the minimum error rate training. The other part is already included in the title. That's the minimum error. So the scores that we have been talking so far are internal modal scores. based on the training data. We know which phrases co-occur with which phrases. We know which engrams in the target language are more probable and we can estimate which candidate translation would be the higher scoring one. We can then give this candidate translation to the user and the user can tell us whether he likes it or not. And this would be the external score. So the internal score is based on the probability tables and the weights that are now set for the model. And the external score relies on the reference translation for example. So it is something which assesses the translation quality after the whole process has been finished. And with access to some oracle, some golden truth, how this sentence should be translated. is a kind of a future test set. It should mimic the properties. It should closely match the properties of the future test set. And we use this held out set to run the system many times. Each time with a different setting of the weights. includes these hypotheses. The first one was translated kind of word by word like mluvíme or we are speaking and up was translated as upwards. Mluvíme nahoru. So word by word this hypothesis gets a very good score because the word translations are very nice but to we are speaking upwards as a sentence in the language model of Czech mluvíme nahoru. That's a very bad thing to do. So the score based on the language model is very low and also the phrases are kind of not common. So the phrase translation probability is also pretty low. And there are the weights. So the total score for this hypothesis based on the internal score is 2 because it's 2 times 1 plus 0 times 1 and 0 times 1. So that's the internal score. We have another candidate. This is Nahlas which is just like speak up in one word. The language model is pretty happy about this because it's shorter. So that's one thing. And the word translation is not very happy about it because it's like word by word the correspondence is bad.. So with these weights the total score sums to 3. Then we can get the mluvíme nahlas and that is that has been seen in the training data as a phrase. the translations are not very frequent word for word. And here we arrive at the final score of four. So then we have the fourth hypothesis and so on. So right now based on the current weights of 111 the highest scoring candidate would be this Mluf Nahlas. And it turns out that based on the reference translations people would rather the translation to be more polite to say could you please speak up. So the external score says that well this we are speaking upwards is a very bad thing. This short one just like speak up exclamation mark that's kind of okay but not the best one. This Mluf Nahlas for some reason is not scored well and the polite one could you please speak up is the the one that externally based on the external measure should be should be chosen. So we now need to somehow modify the model so that it would ideally arrive at the solution that the user wants. And changing all the internal parameters like the the probabilities within the word translation and language model scores that would be too much work. There will be too many numbers to juggle with. So this is what is not done in phrase based MT but it is actually what is done in neural MT. So there you really go into all the internal numbers of the model components. Here we leave the model components intact and what we change are only the weights. So there is the algorithm by Franz Josef Och from 2003 which finds the new weights in such a way so that the external score will now better match so that the internal score comes out different and the internal score will better match the external score. So for example instead of one if we would set the weights to two three one so we increase the importance of the language model. Then the very very fluent hypothesis the way to the weights. So by tuning of the weights we can change the order of the hypothesis and we can make sure that the hypothesis that the external score likes better will be also scored better by the internal score. so some new hypothesis will probably arise. We will score them and we will again check whether we need to update the weights or not. So we will find the new weights based on the new set of hypothesis. And then if we do this twice and get the same best weight setting then we will say okay we're finished. We have found the best solution. four. Many réussi tests will we access the results up for thesu My