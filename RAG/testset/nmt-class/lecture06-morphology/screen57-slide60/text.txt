 So now the reverse self-training and that is something which is more successful. So the idea is that you have this translation model and you have this language model. And the language model now when you use the clever techniques and you train on text and things like that, that will allow you to select the good combination of the word forms. But if the translation model doesn't propose these forms, then you cannot select the right form anyway. So in the first place, your translation model has to propose the word forms to consider. And the translation model is based on your parallel data. So how do you get these needed word forms in the parallel data? And that's where the reverse self-training comes. So this is something similar to the back translation technique which is very successful in neural machine translation. But in phrase-based MT you have to help it a little. the idea is that you have a small parallel corpus that illustrates how the word cat can be translated by two sentences. A cat chased, kočka honila and I saw a cat. Viděl jsem kočku. Because of different roles of the cat in the sentence, the Czech word forms differ. Kočka a kočku. And in our monolingual data, we can see that we can also have other word form of this word. a Juliet after works. We have one more thing that we would never be able to produce with our existing system. So what do we do?