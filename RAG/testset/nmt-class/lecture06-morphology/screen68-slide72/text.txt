 are broken into smaller parts. So given a dictionary of token types and frequencies and their frequencies, you are trying to come up with a fixed size dictionary that will contain the frequent words as holes and that will break longer words or unknown words into more units and these units of themselves by themselves should be also like the most The way of constructing this dictionary is that you repeatedly identify which character pairs are often seen together. So you replace the most frequent character pair with a new unit.