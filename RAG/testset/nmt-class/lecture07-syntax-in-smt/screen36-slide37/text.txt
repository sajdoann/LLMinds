 So you see that the words grass and cut are very far away in the adjacency sense, but in a dependency trees, they are neighbors. So regardless how many words you pump in between, there will be one edge connecting the cutting and the grass. So the context in the dependency tree is better at predicting the lexical choices and the choice of the grammatical categories than n-grams. You would need very long n-grams to at the same time, it is true that the adjacency is very much reflected or the dependencies are very much reflected in adjacency. So if you take a Czech tree bank with manual trees, then 50% of the edges will actually link neighbors and 80% of edges will fit in a foreground. So that means that phrase-based translation is a very good approximation of the dependencies. So in the dependency approach, dependent words are very often close or next to each other. But the way you break it is that you pump words in between and then the phrase-based system can know.