[
    "What are the key components of the transformer architecture, and how does self-attention function within it?",
    "How do different factors such as language, annotation quality, and training data influence the usefulness of explicit syntax in neural machine translation?"
]