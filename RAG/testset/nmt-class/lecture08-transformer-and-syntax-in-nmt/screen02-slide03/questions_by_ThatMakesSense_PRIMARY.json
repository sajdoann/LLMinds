[
    "What was equipped with attention in sequence-to-sequence models?",
    "What is one of the most important components in the transformer architecture?",
    "What is being added to each individual token in neural machine translation?",
    "What affects whether the information from explicit linguistic annotation will be useful or not?",
    "What do the results at the end of this talk suggest about transformer networks?"
]