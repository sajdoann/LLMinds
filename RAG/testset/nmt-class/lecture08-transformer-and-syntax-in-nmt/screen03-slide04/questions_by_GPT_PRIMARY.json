[
    "How does the transformer architecture improve upon traditional sequence-to-sequence models in neural machine translation?",
    "What is the significance of self-attention in the transformer architecture and how does it function?",
    "In what ways can explicit syntactic information be integrated into neural machine translation models, and what are the potential benefits?",
    "Why do current studies suggest that transformer networks can learn linguistic structures without explicit syntax?"
]