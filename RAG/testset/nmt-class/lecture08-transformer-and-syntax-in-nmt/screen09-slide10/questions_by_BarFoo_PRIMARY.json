[
    "What mechanism does the decoder use in a Transformer model to find similar words in the encoder?",
    "What is the name of the self-attention mechanism used within the encoder in a Transformer model?",
    "How does the decoder prevent looking into the future when performing self-attention in a Transformer model?"
]