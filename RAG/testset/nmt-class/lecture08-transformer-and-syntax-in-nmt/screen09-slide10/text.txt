 So what is transformer? At the high level, at the highest level, it is very similar to the... it is just an encoder decoder approach again. But it's already designed from the beginning to be deep. So it's multiple layers, multiple encoders in a row. And the original paper said six is the best number. So it's six layers of encoders and six layers of decoders. You get the input of the sentence. The first decoder processes it to some intermediate sequence of representation. And then these new words, whatever they encode, are processed by the next decoder and so on. So the network has the capacity to do very diverse transformations and like information extractions in this process. And then you arrive at the final representation of the sentence, which is of arbitrary length obviously. and you use this information the output of the last encoder as the input to all the decoders and the decoders start again like there is again a sequence of these decoders and each of these decoders gradually refines the predictions of the previous decoder so the network can produce the final sentence in sequence of gradual refinements whatever it finds appropriate and each of these decoders has access to the to the information from the encoder so that is important that the network always can reconsider what the original sentence was I'm not highlighting that any further with a network this deep it is often very important that you keep some linear flow of information the network some residual connections so all of these and decoders and decoders can be also skipped so there is there is always a combination of like processing at that layer or skipping the information just just taking the information from the previous layer so the network has the capacity to like copy everything without without any calculations but it's obviously