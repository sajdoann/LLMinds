[
    "What kind of sublayers does a transformer have?",
    "How many layers are in the feedforward network of the transformer?",
    "What is the main difference between the encoder and decoder sublayers of a transformer?",
    "What can be achieved by only using six layers of feedforward network in a transformer?",
    "How does self-attention contribute to the processing capacity of a transformer?"
]