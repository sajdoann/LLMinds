[
    "What is the standard thing done after subword processing with an input sequence?",
    "How does self-attention allow information to flow between words?",
    "What is the purpose of residual connections in this network architecture?",
    "Can the words in an input sequence be shuffled for later processing?",
    "What effect do intermediate representations have on the output of this network architecture?"
]