 is performed token by token. So the feed for word networks, I've already said that they are very simple and they are even simpler in that they are not talking like all the words are not talking to each other anymore. It's token based. So you do the same transformation at every of these tokens. And that is how you obtain the final result of the encoder layer one. You have the same number of positions as there were from the input words. Maybe the information has flown elsewhere but it normally stays at that word. And then you pass these tokens, these representations to the next layer of the encoder. Yeah.