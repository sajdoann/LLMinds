[
    "What is the purpose of positional encoding in the transformer model?",
    "How do positional encoding vectors change with every single token?",
    "What is used to construct the positional encoding vectors in the paper?",
    "Why is it sufficient for positional encoding to distinguish positions but not specify their order?",
    "What happens to the positional embeddings when two occurrences of the same word appear in different positions in a sentence?"
]