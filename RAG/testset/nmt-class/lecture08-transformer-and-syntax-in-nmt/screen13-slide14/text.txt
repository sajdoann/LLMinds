 So I've said that information can flow from any token to any token. I've also said that the feedforward layer is applied token wise. That means that with this description the network would have no information about the positions of words. So if the words were totally shuffled, if you shuffle the sentences before feeding them to transformer, it would have the exact same information. And obviously positions of words are important for the meaning of the sentence. And also imagine if the same word appeared at two positions, then the network would not have any means to distinguish them. So the way of encoding positions in the transform model is kind of surprising, at the first sight at least. The information about where the token was is stored within each of the tokens. So what they do, they introduce so called positional encoding. And this positional encoding are vectors which change with every single token and with the index of the token. So that these vectors are what the network can use then to identify whether the word is at the beginning or at the end of the sentence. And these vectors are added to the word level embeddings. So what these vectors are, it turns out that you could use any random vectors actually, and it would work similarly well. So just distinguishing positions but not specifying their order in any way would be sufficient. But what is actually used in the paper is something which is kind of similar to the Fourier transform style of information processing. So the way you can look at it is that the first element of the vector flips at the fastest rate, and the second bit flips a little bit with like lower frequency, and the third bit with a little bit lower frequency. So depending on which of the elements you look, there you will see how many sine waves are we from the beginning. So that's the way the positional encoding vectors are constructed. And then they are simply added without any normalization whatever to the word embeddings. So the same word if you have the word catch twice in the sentence, the same word will have the same word level embedding, but it will differ in the positional embeddings. Then there is, where the model can text a little bit lower to a little bit lower to an arrow from there. So if notew word, you know, that I will make the pronunci- conte 준비 training as one.