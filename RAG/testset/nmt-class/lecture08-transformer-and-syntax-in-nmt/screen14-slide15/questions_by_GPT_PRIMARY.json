[
    "What is the fundamental purpose of self-attention in the transformer model?",
    "How does self-attention differ from traditional recurrent neural networks in handling sequence data?"
]