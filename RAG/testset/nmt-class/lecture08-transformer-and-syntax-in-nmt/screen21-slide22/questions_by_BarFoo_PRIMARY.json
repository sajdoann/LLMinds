[
    "What method does the Transformer model use to encode positions of tokens?",
    "How are the positional encoding vectors added to the word level embeddings?",
    "If a word appears twice in a sentence, how will its positional encodings differ?"
]