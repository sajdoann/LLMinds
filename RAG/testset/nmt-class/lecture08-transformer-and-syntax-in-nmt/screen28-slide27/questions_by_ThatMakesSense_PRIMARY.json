[
    "What type of attention mechanism does the encoder-decoder use?",
    "How do the decoder's self-attention positions attend to all previous positions in the encoder?",
    "What is the purpose of masking in decoder self-attention?",
    "Why is it necessary to prevent looking into the future in decoder self-attention?",
    "How does the decoder self-attention mechanism differ from the encoder self-attention mechanism?"
]