[
    "Why is masking used in decoder self-attention in a Transformer?",
    "How does decoder self-attention prevent looking into the future during the decoding process?",
    "In which type of attention mechanism does the decoder use masking to prevent looking at future words?"
]