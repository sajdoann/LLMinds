[
    "How do the individual attention heads in a Transformer model contribute to understanding complex language structures?",
    "In what ways do attention weights reveal the model's focus when interpreting ambiguous or complex sentences?",
    "What does the ability of the network to learn to identify antecedents suggest about the nature of machine language understanding?"
]