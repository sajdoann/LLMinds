[
    "What is the purpose of using attention weights in a similar way as in the sequence-to-sequence algorithm?",
    "How does the network determine where to look for information when producing a representation?",
    "How did the network figure out that 'it' was the antecedent of the pronoun?",
    "What did the trained weights manage to identify in the antecedent of the pronoun?",
    "What did the network learn how to do when producing the next layer representation of the word it?"
]