 I'm not showing any numbers here essentially because I have not done these experiments myself and I trust only numbers that I create myself. So here is another way of incorporating explicit knowledge into the network structure and here the graph convolution networks combine the idea of convolution with the idea of network structure along the dependency parse. So first you allow for like looking at surrounding words so you use some convolutional network with the receptive field covering part of the sentence not the sentence as a whole but this is parameterizable so if you if you make the constants large you could cover quite long sentences and that will give you some more compact representation of whatever is and then you use the dependency tree to flow the information in multiple layers of the subsequent processing. So this is the structure of computation is directly designed to mimic the dependency tree so each position has access to the dependence and heads to dependence. So in the dependency parse each node depends on some other node on one other node the governor and then some node is the root of the sentence which is often the auxiliary node. So each node has one governor and an arbitrary number of children and the calculation respects that. So there is one weight matrix which tells the node how to make use the information from the governor and there is another matrix which specifies how to make use of information from the arbitrary number of children.