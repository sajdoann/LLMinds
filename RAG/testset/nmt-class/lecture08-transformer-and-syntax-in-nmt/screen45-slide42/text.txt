 So what I did with my students some years after that, we retried this approach with CCG tags and we applied it not only to sequence to sequence model but also to the modern transformer model and we were predicting the target syntax using a secondary decoder so the sequences of length could differ or with the interleaving which makes the output sentences double the length. That's somewhat risky but it that every position will have the correct number or well it does not ensure but the network is clever enough to make the tags and words properly interleave and have the same number of these additional factors for each output position. So as tags they used the correct CCG tags from a parser, random tags and also a single dummy tag. So we would just say like beep the first word and beep the second word. So we were only like with a single dummy tag the vocabulary size of that was just one. And the paper is published in the Prague Building of Mathematical Linguistics. It's called Replacing Linguists with Dummies. That's the dummy tag. And you'll...