 It could be export even further. With the transformer setup, the learning curves have a slightly different shape than with the record networks. But the main observation is the same. If you use the boring identical tags, the network fails to learn. So if you give the network a too easy task, it's not going to learn in a multitask setting. It is not going to learn the interesting task at all. It will optimize for the easy part. and not learn anything. And then the baseline is somewhat worse than CCGs or random tags. And here the CCGs come negligibly better than the random tags.