[
    "How do self-attention heads in transformer models relate to potential dependency representations of sentences?",
    "What are the advantages and challenges of embedding dependency structures directly into the training of transformer models for translation tasks?",
    "Why might the best performance in dependency parsing within transformer layers occur at higher layers, such as layer four or five?",
    "What implications do the findings about early-layer dependency parsing have for designing neural machine translation systems?",
    "What does the comparison between the simplistic baseline and complex linguistic information integration reveal about the role of extra linguistic data in neural networks?"
]