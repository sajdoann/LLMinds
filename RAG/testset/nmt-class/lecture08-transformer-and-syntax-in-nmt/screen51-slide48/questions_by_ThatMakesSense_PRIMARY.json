[
    "What type of matrices can the self-attention heads in a transformer model be seen as?",
    "What happens if the attention matrix is very peaked and resembles a one-hot vector?",
    "How does adding a penalty to the training objective for not producing a parse with the attention head work?",
    "What is the purpose of constraining the training of the encoder to also produce dependency parts of the source sentence?",
    "What is the value of the parameter theta (number 20) in the document?"
]