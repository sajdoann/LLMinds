 of the source. So here are the results. It was translation from Czech into English and we measure the quality of the translation, the BLEU score and we also measure whether this parse as produced by the transformer is matching the parse that the parser set. And it turns out that depending on... so we actually we do get an improvement over the baseline where the transformer has just objective and the best improvement is requiring the parse on the layer one. So if the network is trained so that right after the words, head number one considers the dependence of each words so it somehow the network will learn to find what is my governor. And it it will learn to find it early in in layer one already. Then this information because the head the output of the head is used in the subsequent layers as a normal transformer. Then this information really helps. It leads to better translation in the end. The parses that the network produces at the layer one are not the best ones. The best ones are actually from layer four or five. So right after the last... right before the last layer of the encoder. So this is also like not very surprising. If the network has more time and more parameters to train before it emits the final parse, it will do the parse better. So if you want to... if you have to implement a dependency parser and you don't want to implement your own but you have your training data ready, you can use just the transformer model, train it to translate and train it to predict the parse on the way and it will it will quite succeed in that. And it will best predict the parse on the fifth... on the second to last layer. It will produce better translations if it's forced to know the parse earlier, but the parse is not that great. Okay, and then we tried another thing and this is again the same idea as the replacing linguists with dummies. I think that every... every experiment that tries to document that linguistic information or any additional information to the neural network is beneficial also has to try some simple baselines with a similar type of information.