 So let's talk about word representations. We've already talked about them as a technical means to circumvent or to resolve the problem of too many words to rich vocabulary and also too dissimilar words. So word embeddings are continuous representation of words that have much less. These dimensions, these continuous dimensions do not have any clear interpretation and the embeddings are trained for various tasks. So the best known word embeddings are Word2Vex, but there's also GloVe embeddings and many others. by the neural empty system are also of different nature. So it's continuous representations of words but they are they have properties which help to translate the sentence. So yeah so so this word to act representations are from two language modeling tasks either you are predicting the word in a corpus from its four neighbors or you are predicting the likely neighbors of a given word and this leads to different different exact specifications of of these embeddings.