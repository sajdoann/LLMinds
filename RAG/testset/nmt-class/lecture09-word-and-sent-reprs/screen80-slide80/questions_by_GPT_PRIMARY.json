[
    "What is the purpose of using multiple heads in the static application of inner attention within transformer models?",
    "How does the fixed number of views in the transformer architecture influence the encoding process?"
]