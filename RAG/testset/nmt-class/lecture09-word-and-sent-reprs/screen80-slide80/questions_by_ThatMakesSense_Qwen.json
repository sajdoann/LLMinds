[
    "How is inner attention used in the transformer architecture, and what is the significance of using a fixed number of attention views before the decoder starts processing?",
    "What is the role of multiple attention heads in a transformer model, and how are they utilized in a static manner before the decoder starts processing the input?",
    "What is the purpose of using multiple attention heads in the transformer architecture?"
]