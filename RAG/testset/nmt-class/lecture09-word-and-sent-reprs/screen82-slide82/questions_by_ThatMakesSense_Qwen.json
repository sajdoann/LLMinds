[
    "What is the key assumption about the output sentence when using fixed-length heads for attention in a transformer-style model?",
    "What assumption does the text make about the output sentence in the context of using a transformer-style encoder and decoder?",
    "What assumption do transformer-style models make about the output sentence length, and what do they refer to as \"heads\" in this context?"
]