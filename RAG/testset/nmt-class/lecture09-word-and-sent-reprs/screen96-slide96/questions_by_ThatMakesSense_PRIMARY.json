[
    "Where does the most attention get put when translating?",
    "What happens to the decoder after it uses a subject head to spot and translate the subject of the sentence?",
    "What is unique about the structure of sentences in this context?",
    "How does the recurrent neural model use attention in this context?",
    "What happens when the input is a complete sentence in this context?"
]