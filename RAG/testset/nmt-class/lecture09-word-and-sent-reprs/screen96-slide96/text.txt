 So the question is where is the most attention put when translating? So the question is where on average across all the sentences do the heads look? And one would expect and we had these experiments mentioned in the previous lectures that one of the heads and another head would search for the subject of the sentence and then the decoder would use this subject head to spot the subject of the sentence, translate it, so produce the translation of the subject, then it would move on to the predicate, the verb head and search for the verb head and translate that one. and so proceed according along the semantics of the sentence. What we see here is different. There is not a head for verbs, there is not a head for punctuation symbols, there is not a head for numbers, there is the head for the first eighth of the sentence, there is the head for the second eighth of the sentence. So if you have eight heads, this inner attention with the recurrent neural model will actually learn to divide the sentence equidistant. we need to enter the sentence, no further, the end of the sentence. project the structure is outside now they הה Probe they