 What we did was to organize the sentences into batches. And these batches, or like big chunks, were limited in sentence length. So first we had this batch of very short sentences up to five words and the model learned somewhat. And then we opened, we exposed the model also to the longer sentences, so we allowed sentences of length up to ten. And then we allowed sentences of length up to fifteen. And then in the end we allowed sentences of all lengths. So at this end of the training, which by the way indeed improved the baseline a little bit, so there is something to this curriculum learning. In the end of the training, the model was seeing all the sentences of all lengths and it was slightly skewed to the longer the important fact was that the batches were still fully diverse. And then Tom Cosme had this French idea to just flip the order of this sorted batched corpus. So when we took this very same corpus organized in these like big chunks of sentences of varying length.