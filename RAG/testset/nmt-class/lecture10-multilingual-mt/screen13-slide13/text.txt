 We got this learning curve. So at the beginning, the system was learning on all sentence lengths and the performance seemed better than the baseline. This could be an artifact of the oversampling of longer sentences. So in one training step you see actually more words. This reflects the number of sentences processed. But in the same number of sentences with this we have longer sentences so more words. So the better performance at the beginning is probably due to this effect. But then the interesting observation is that as we were closing the buckets, as we were prohibiting sentences beyond certain lengths, the modal actually unlearned to produce long sentences. So this is catastrophic forgetting. At the end of the training, the system was able to produce only very short sentences. It produced sentences as long as the last batch was, not as long as the source sentence is. So the the modal, and you can check in the paper whether this is still the recurrent sequence to sequence model or whether we tested it also for transformer. This modal memorizes the length of the sentences, kind of, and and it does not memorize the relation between the source sentence length and the target sentence length. And it unlearns to produce long sentences. So when you want to fit many language pairs, you have to be careful.