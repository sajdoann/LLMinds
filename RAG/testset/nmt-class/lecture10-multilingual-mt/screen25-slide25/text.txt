 The previous experiments, the experiments so far, always had English on the same side. So if the English was on the source side, then the encoder learned better from... or like the encoder was sensibly pre-trained for English and the same English encoder was useful for the child. If it was the target side, then the decoder knew the English well and it was again cont gypsy. But we also tried to swap the position of English. Now the parent has always some language with English. But the English is on the other side in the parent than it is on the child. So for example this experiment, the parent model is English into Finnish and the child model is Estonian into English. the knowledge of encoder or decoder. The thing that helps is the shared vocabulary, obviously. So there is some gain from the English being one of the common languages. And even with this English position swap, we do get the improvements. So the column delta-blah says that yes, it was better to use the transfer setup. Note that it's always the case when the parent was bigger, so we didn't attempt the smaller parent anymore because we country knows that it doesn't help, and there is some improvement. The improvements were bigger when the parent had the English on the correct side, but the difference is not too big, it is big, but we still get an improvement over the baseline even when the parent model has English on the wrong side.