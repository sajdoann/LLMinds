 So these papers also included an interesting experiment that went from the single source but multi-language setup into multi-source multi-language. So you train on pairs of sentences, you don't have multiparl corpus, and then you suddenly want to use these encoders and decoders to process a sentence which is already available in more languages. And here what you can do is you can simply use these two encoders at once and just combine the context vectors, so the result after applying attention, you can just combine them together and have the decoder process this joint knowledge. And either you can do this averaging of context vectors and then based on this improved context vector generate the output words or you can well this context vector then also is processed by the decoder and helps the decoder to modify its state or you can do this prediction of the best word later. You can run the decoders separately for the two languages and each of the each of the source languages will offer some word and you can then choose so only the softmax layer is is like... doing the choice you can choose the word by voting between the two independent processing setups. So in any case it is possible to train independent encoders and decoders for each of the language pairs and then put these network pieces together to process input which has both source information mutters and Mister how