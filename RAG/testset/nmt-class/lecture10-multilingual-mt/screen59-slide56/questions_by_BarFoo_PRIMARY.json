[
    "How many sub-layers does each encoder layer have?",
    "What is the main structure of the decoder, similar to the encoder?",
    "What does the multihead attention in the transformer setup do?",
    "How many heads are there in each of these layers in the transformer setup?",
    "What happens after a few layers of self-attention with multiple heads in the transformer setup?",
    "In which language pair setups did the model perform well according to the text?",
    "What were the main platforms faced with size issues in previous experiments?",
    "Are the encoder and decoder structures exactly the same in the transformer setup?"
]