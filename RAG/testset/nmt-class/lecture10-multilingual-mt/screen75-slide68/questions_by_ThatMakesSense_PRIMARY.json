[
    "What is the typical structure of the transformer setup, consisting of an encoder and a decoder?",
    "What are the sub-layers within each encoder layer, and how do they contribute to the overall processing of the input?",
    "How does the feedforward network contribute to the processing of intermediate representations in the transformer setup?",
    "What are the key differences between using recurrent neural networks versus the transformer setup in multi-language setups?",
    "What is the primary challenge in adapting the transformer setup to multi-language setups, according to the previous experiments?"
]