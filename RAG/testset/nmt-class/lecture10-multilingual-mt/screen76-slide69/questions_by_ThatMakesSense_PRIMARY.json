[
    "What is the default size of the transformer setup?",
    "How many parameters are in the default feedforward network?",
    "What would be the effect of doubling the number of layers and increasing the size of the dimension of the feedforward network?",
    "How many GPUs are required to train the wide transformer setup?",
    "What is the effect of using a deep network compared to a wide network, especially on low resource languages?"
]