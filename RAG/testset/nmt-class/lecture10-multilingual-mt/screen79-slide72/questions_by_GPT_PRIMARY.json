[
    "What is the purpose of using sparsely gated mixtures of experts in large transformer models?",
    "How does the gating network contribute to the functioning of a mixture of experts in these models?",
    "In what way can training only certain experts at a time benefit the overall training process of large models?",
    "Why is increasing the model size particularly important when working with many target languages in machine translation?",
    "What is the significance of adding small, language-specific adapter layers on top of a pre-trained network?"
]