 What are the tricks? These 50 billion parameters of transformer needed these sparsely gated mixtures of experts. So the idea is that this is an idea for the LSTM or some convolutional setup. But the idea is that some parts of the network will be flexible and they will not be used all the time. But this mixture of experts will consist of a large pool of experts and only a subset of them will be used for each particular input sample. And which expert to use would be something which the network will learn by itself again. So there will be this gating network. The gating network will observe what is the current input and it will decide which of the experts should be used which are most relevant for this and then these experts would operate on this input to get the output and then we proceed. So this way at each of the training rounds you have only a subset of the parameters loaded in or you're operating only on this lower number of parameters. But the model as such includes all the other experts and with a particular training and with random initialization you are able to train parameters by training them only sometimes. It's up to the gating network to decide when which of the experts should be trained. So maybe some of these experts will be like trained more often and some of them will be trained less frequently but still they will the experts have the chance to divide the work among themselves. So maybe these experts are specialized for the particular languages in the model that's something which maybe has not been analyzed yet. But the model as such can be much larger reaching 50 billion parameters and yet fit on the on the GPUs. So with this trick with these 50 billion parameters we got improvements or at least some reduction of loss and then even improvements even on high resource language. but it's important to realize that for 100 languages in the mix pair with English we had to use the model which is 150 times larger. So the bottleneck is still there. If you try to put too many target languages especially into one model and you do not increase the model capacity then it will lose performance. on the high resource languages which could be trained better when trained alone. The low resource languages would not use that capacity of that model anyway because they don't have enough data for that. So for the low resource languages putting more data in is useful. And that's the main... that's the main...