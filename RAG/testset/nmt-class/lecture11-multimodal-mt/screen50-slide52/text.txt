 So here is a proof of concept that was the first serious attempt for end-to-end spoken language translation by Berard from 2016. And this exactly used the synthetic corpus. So it was small, it was the BTEC corpus, the basic travel expressions corpus. And the corpus is normally French to English text only. And Berard and colleagues synthesized French speech with seven and these voices are corpus based and concatenative approach. So they are very repetitive. The same word is uttered in exactly the same way in the whole corpus, which also makes the sounds easier to grasp for the architecture afterwards. And the end-to-end neural network that was trained on this halfway synthetic corpus was a deep LSTM encoder, then the standard attention and the deep LSTM decoder that... So at the beginning did not have the words or subord units, but instead it had the MFCCs, which is the standard representation of frequency frames of sound. So the results on this small corpus were not too far from the cascading approach, but again, I have to remind you that the input was synthetic. So the LSTMs could have kind of learned to classify the word samples because the words were too simple.