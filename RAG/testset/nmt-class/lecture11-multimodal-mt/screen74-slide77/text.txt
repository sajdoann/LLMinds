 but also for input images in the image captioning task. So here you have an image encoder. Here the images are processed with convolution networks to reduce down the dimensionality. And the image has like a 2D representation, the annotation vectors for the regions in the image. And you run the attention mechanism not over these one-dimensional sequences of input tokens, but you run it over the 2D parts of the sentence. And you use the same decoder afterwards, so this image captioning task has no source language, it has only the image and it's producing the expected output of the description of that image. And it attends to the various parts of the image as it's producing the output. And if you put these two...