So let's look at the direct assessment. That's something which was introduced to the competition of WMT in 2013 and the idea is to make the evaluation as simple as possible so that we could give it to random volunteers who were working on some crowdsourcing platforms such as Mechanical Turk. So it is reference based so that you require only monolinguals in the target language and it has just one simple slider. It's not too visible on the slides, but the slider is a score, essentially on a continuous scale from 0 to 100, and you as the assessor will read the reference translation, and you are checking whether the candidate translation adequately expresses the meaning of the reference. And you just pick some score whether you like the output or not. So this is like a like button but it's like slider. And obviously when you're looking at your first evaluation then you would not know like is this a good output or bad output. But after about 15 judgments each annotator stabilizes. People also heavily differ in their strategies. Some are like heavily optimistic, some are heavily pessimistic. Styles are different but if you average it across a large number of people and across a large number of judgments then you are relying on statistics and like statistical interpretation of the scores and it is interpretable. So you will learn on average which sentences by which system are the best translated. The drawback is that if we run it on a mechanical Turk or another crowdsourcing platform, then there are also people who cheat just want to get the money for the scoring and put the slider at a random position. There are easy ways to find out these people. You will give them sentences which you know are worse and if they score them higher, then you know that they haven't read that because they have not spotted that there are some words which do not fit into the sentence at all. So we can easily filter them out, but we pay them anyway and there is like a half of the money lost because half of the assessors are not reliable. And another big problem, actually bigger problem because you can always ask for money and get it from somewhere. But the bigger problem is that the crowdsourcing platforms have mainly English-speaking participants. And if you want to score translations into other languages, you are running out of people. So the other languages are normally in this competition evaluated by the researchers themselves. So we as the Czech team take part, and we also promise to score some number of sentences. So this may be part of your homework when the campaign comes, but this year it is a little later, so it may not overlap with the semester at all. And it's definitely interesting to see what are the best systems, and multiple languages are explored there, so yeah, depending on what you speak, you could help us. So that is the assessment