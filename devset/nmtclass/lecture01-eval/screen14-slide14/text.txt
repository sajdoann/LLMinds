So here is the results from this pseudo-document-aware evaluation. So from the last evaluation campaign in English to Czech, the system from 2018 was now significantly worse than humans and it was on par with some system which tried to handle the document context as well. And what else? Well, having said this it is also important to realize that every year it is new texts. So it can be the case that the translation agency was maybe a little bit more careful or the texts were for some reason, like domain match or domain mismatch, harder or easier than the previous year. So like having lost the human parity is something which sounds strange. Like in 2018 we would be saying, oh, we have beaten humans in translation and in 2019 we'll be saying, well, machines are no longer better than humans. That doesn't sound reasonable. So in the first place we didn't say that we have beaten humans in 2018. It's important to realize that we know what are the limitations of the evaluation method. So this is English to check, but