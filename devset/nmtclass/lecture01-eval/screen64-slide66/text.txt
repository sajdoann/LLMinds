We did this evaluation and that's finally some utilities coming from that. We actually, we used the same systems which took part in this 2019 competition. And the 2019 competition was interesting because there I won in the BLUE score, in the automatic scoring, which we'll discuss in a second. So I said yes, great, I'm winning the competition. And then the manual ranking came, that was the relative ranking back then. And PC Translator a traditional kind of rule-based system was the best and I was the third. And I said, how come that my system is so good in the automatic evaluation and so bad in the manual assessment? This flagging of errors explains that. So it turned out that my system did the fewest mistakes in word choice. So I had the lowest number of words with a bad sense but I had the highest number of missed content words. So my system was afraid of making errors, so it rather dropped the words, it truncated the sentences. It was not penalized too much by the automatic scoring, but humans didn't like the fact that words got lost. So that's an obvious problem. And PC Translator, it actually made almost the highest number of bad lexical choices, but it had the lowest number of missed content words. So that explains why humans preferred PC translator. So this is how the gray box style evaluation can give you a better explanation. And obviously, for me, it was a confirmation, well, you have overfitted to this automatic metric. The metric does not score all it has to score. You must not drop content words, otherwise people will not like your outputs.