So this was one particular contradiction. There is also contradictions in manual evaluation itself. So we have discussed the ranking and in the ranking in that year Google won when we were considering the ties. When we were not considering the ties, when we asked the comparison to be strict so that one system is strictly better than the other, then PC translator won. So even just the little change whether you take the tie into account or not, can change the order of the systems. That is strange. The number of edits deems acceptable, that's the comprehensibility check. So there was the two people, one reading the sentence and the other, one correcting the sentence and the other checking whether it was corrected in line with the original meaning. There Google won. Google also won in the automatic evaluations, but in the quiz based evaluation it was actually our deep syntactic system which preserve the meaning of the sentences for the purposes of this testing best. So different evaluation methods, and most of them are manual evaluation methods, they will give you different results. So that's why I'm always highlighting the friendly competition and not the competition competition, not who is going to win, but who has the lowest number of errors of a particular type aspect. This year in 2019 we have seen that the best systems match humans in the GCSE style scoring but they score worse in the pseudo document or direct assessments and they are absolutely terrible when translating agreements. So again we have different outcomes based on the manual evaluation.