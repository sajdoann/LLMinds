So we are summarizing, finally we are summarizing manual evaluation. It is expensive in terms of money. It is subjective, so different judges can guess more. Different judges can be more picky and more fussy about grammar and so on. It is obviously the results will differ across judges, but the results will also differ for a single person. If you ask the same person twice, they will give you a different answer because they will notice another part of the sentence and give you a different score. It is not reproducible. That's the hardest problem of manual evaluation because as soon as an annotator has read a sentence they will remember it even if you give it to them in a week they will still be biased by that. So you cannot do this repeatedly at all and we have already seen that the experiment design is absolutely critical. You will learn different results. The black box evaluation is important for users or sponsors who just want to know who is the best and full stop but the more fine-grained gray or glass box evaluation is important for us developers. We know which aspect of the system is wrong and what should we focus on. And then the source-based evaluation allows to compare with humans, so there we can check whether in the evaluated aspect we are on par with humans or not. And the sentence level is no longer relevant for any language pair where there is enough large enough training data. So the sentence level is going to happened on for languages where we have sufficient resources and good enough systems because, well, that was the 2018 result, beating humans at the level of sentences, but not