not in the full context. So the big area that I would like to cover now is automatic evaluation. Automatic evaluation will compare machine translation outputs to some reference translation and there is also another type of automatic evaluation which doesn't require the reference, it only relies on the source and that's called quality estimation but we're not going to discuss that. So we're going to rely on the on the reference translation. The main benefit is that it is fast and cheap. So you translate the test sets once by humans, and then you can repeatedly do it. It's fully replicable. It's deterministic. So because it is fast and deterministic, you can even do it within some modal optimization. So this merge thing or this tuning is from the old pre-neural approaches. These days, we would do some reinforcement learning on the final scores. Usually automatic evaluation of MT quality is good for checking progress because you have the same setup and you are simply improving it gradually. And it is usually very bad for comparing systems of different types. I've already mentioned the problems of BLEU and we're going to discuss them in more