to discuss the properties and the problems. It's within the range of 0 and 1. It's often written as 0 to 100. So if you see a black score of 0.25, you don't know whether the system is totally crappy because it's in the scale of 0 to 100 or if it's actually reasonable like 25 could be a good score. The human translation, humans against humans, is usually around 60 percent and this is many years ago phrase based system 30 and 50 for different languages these days and also it depends on the domain for some domains we are getting BLEU scores around 70 because the text is so repetitive that even like new sentences in the test set still contain large portions that were seen in the training data so it's easy to get high scores. Yeah BLEU score for individual sentences is not reliable. It works only reasonably well if the document is larger and more so if there is only one reference. So I've said that the original paper expects people to use four different translations as the reference, but the standard in all these competitions is to have just one. So the results are not not as reliable. So here's an illustration why it is a bad idea to use just one reference. Why is it suggested to use four sentences? Well that's just the balance between the price you are willing to pay for the creation of the test set and a reasonable performance in terms of correlation with humans. It was tested. In the original paper it was like tested and like they didn't go any further. The further you would go the better but it was shown that four is kind of enough. But that means that one is not really enough. And everybody uses one.