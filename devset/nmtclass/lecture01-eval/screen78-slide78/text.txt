So here are technical problems. BLEU scores are not comparable across languages. They are not comparable across different test sets. They are not comparable across different number of reference translations. They are not comparable with different implementations of the evaluation tool. There are two reasons why it's that. And they are very sensitive to tokenization. So the good thing that you can do is to rely on one fixed reference implementation and one has been done recently by a Matt Post, it's called SACREBLEU. So that's something which removes some of these problems.