But there are also fundamental problems of BLEU and these fundamental problems are that BLEU is overly sensitive to word forms and sequences of tokens. So we have seen that somewhat in the past and again this flagging of errors provides an explanation for that. So let's look at the uni-grams only. So that is the number of words in the MT output. In total there is 35,000 of uni-grams 35,000 of words and the question is whether these words are confirmed by the reference, so what the BLEU will give a credit for them or not and whether they received a flag by humans, whether they are errors, whether they contain some error or whether they are good. So that's the two distinctions. Whether the particle word is confirmed by the reference and whether it contains errors and luckily, the two cases that we do want are the majority together. So words that are confirmed and do not get any error flag were about 37% of the test set. And words that are not confirmed by the reference, and they do contain an error are about 22% the test set. So the majority of words are like properly validated by the check with the reference. What sometimes happens is that word is confirmed by the reference and still the annotator labels it as wrong word. So that's a false positive error. That's about 6% of the volume. But the real trouble comes from this line and that's words which do not contain any error according to the people who read the sentences and still they are not confirmed by the reference. So there is a single reference. The translator decided to translate it somehow differently and for that reason the word the correct word from the MT output is not confirmed by the reference and this amounts to more than a third of the output so this is too large space where these systems can differ and some of the systems can be bad in these unconfirmed words and some can be good in these unconfirmed words so too large volume of the output is not scored, so that is where the system can differ, BLEU will not notice, and we will get lack of correlation with human judgments. So in a paper where I've discussed this, we have actually found out that the higher the BLEU scores themselves are, the more they correlate with humans. So if you have a higher number of matches, the reference fitted well with the MT outputs and therefore the mismatches are indicators of error. In normal case, the mismatches in one-third of cases do not indicate anything. So that's a fundamental problem.