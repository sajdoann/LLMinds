Yeah, so there is another fundamental problem and there's also a fundamental problem of evaluating metrics as such, and we're finally coming to the end of the lecture. So I've talked about a large number of methods and like the manual methods and the automatic methods, and I've told you that the automatic methods are designed to correlate well with humans. So I'm gonna now totally blow it up with saying that it is difficult, that this correlation with humans is not something very stable. So the correlation of an automatic metric with the human judgment depends on the underlying set of systems. So this is English to German translation where there are 20 systems. If you consider all of them, it will seem that the standard BLEU score correlates almost perfectly with that 0.99 or something. That's the SACREBLEU is this line the violet line the other lines are other metrics that take part in the competition. So one of the metrics is very bad, like it's always under zero. This one is also like pretty bad. Most of the metrics seem to correlate very well, but if you reduce the set of systems, and you take only the top eight systems, then suddenly you are around zero, so there's no correlation. And if you take just four systems, the top performing systems in English to German translation the BLEU correlates negatively with the human rank. So the underlying set of systems is like