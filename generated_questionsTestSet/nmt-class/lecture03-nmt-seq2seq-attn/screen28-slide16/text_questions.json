{
  "all_questions": [
    {
      "question": "What is the expected output of the network for a vocabulary size of 30,000 units?",
      "context": "So we indeed and it asks the network to give us the probability of each of these possible output words, output units.",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.4,
        "diversity_score": 0.75
      }
    },
    {
      "question": "How does the network calculate the normalized probabilities for each word in the vocabulary?",
      "context": "So the terminology talks about logits or any energies for word at a time t.",
      "difficulty": "medium",
      "category": "analytical",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.1111111111111111,
        "diversity_score": 0.8
      }
    },
    {
      "question": "What is the purpose of using softmax normalization in the network?",
      "context": "Yeah and there were some tricks in the past how to handle this big matrix such as the matrix was not trained in full but it was trained in parts.",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.0,
        "diversity_score": 0.7947916666666667
      }
    },
    {
      "question": "How does the network decide on the output word given a hidden state?",
      "context": "So the last state of the network is projected scaled up to the size of the vocabulary and then it is normalized to make it a probability distribution so that it will sum to one.",
      "difficulty": "medium",
      "category": "inferential",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.1111111111111111,
        "diversity_score": 0.7904166666666667
      }
    },
    {
      "question": "What are some techniques used to handle the large weight matrix in the network?",
      "context": "So this is now not frequently used because with subord units and the 30k or 80k output vocabularies we can.",
      "difficulty": "hard",
      "category": "analytical",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.1111111111111111,
        "diversity_score": 0.8134615384615385
      }
    }
  ],
  "selected_questions": [
    {
      "question": "What is the purpose of using softmax normalization in the network?",
      "context": "Yeah and there were some tricks in the past how to handle this big matrix such as the matrix was not trained in full but it was trained in parts.",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.0,
        "diversity_score": 0.7947916666666667
      }
    },
    {
      "question": "How does the network calculate the normalized probabilities for each word in the vocabulary?",
      "context": "So the terminology talks about logits or any energies for word at a time t.",
      "difficulty": "medium",
      "category": "analytical",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.1111111111111111,
        "diversity_score": 0.8
      }
    },
    {
      "question": "How does the network decide on the output word given a hidden state?",
      "context": "So the last state of the network is projected scaled up to the size of the vocabulary and then it is normalized to make it a probability distribution so that it will sum to one.",
      "difficulty": "medium",
      "category": "inferential",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.1111111111111111,
        "diversity_score": 0.7904166666666667
      }
    }
  ]
}