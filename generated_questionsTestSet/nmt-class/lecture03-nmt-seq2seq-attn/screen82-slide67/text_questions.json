{
  "all_questions": [
    {
      "question": "What inputs do the attention mechanism in the bidirectional recurrent neural network take as input?",
      "context": "The inputs to the attention mechanism is the decoder state and all the encoder states.",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.25,
        "diversity_score": 0.7574404761904763
      }
    },
    {
      "question": "How are the attention energies calculated?",
      "context": "These weights specify which of the input states is now how important. So that's one layer of neural network.",
      "difficulty": "medium",
      "category": "analytical",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.25,
        "diversity_score": 0.7083333333333334
      }
    },
    {
      "question": "What process do the attention energies undergo to be normalized?",
      "context": "These attention energies need to be normalized so that they sum to one",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.375,
        "diversity_score": 0.6729166666666667
      }
    },
    {
      "question": "How is the context vector calculated from the attention energies?",
      "context": "So this is the softmax for the normalization",
      "difficulty": "medium",
      "category": "analytical",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.0,
        "diversity_score": 0.7321428571428572
      }
    },
    {
      "question": "What role does the previous decoder state play in the attention mechanism?",
      "context": "The attention energies need to be normalized so that they sum to one and then when they sum to one you can see and simply use them as weights to combine all the encoder states.",
      "difficulty": "hard",
      "category": "inferential",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.2222222222222222,
        "diversity_score": 0.7638888888888888
      }
    }
  ],
  "selected_questions": [
    {
      "question": "What inputs do the attention mechanism in the bidirectional recurrent neural network take as input?",
      "context": "The inputs to the attention mechanism is the decoder state and all the encoder states.",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.25,
        "diversity_score": 0.7574404761904763
      }
    },
    {
      "question": "How is the context vector calculated from the attention energies?",
      "context": "So this is the softmax for the normalization",
      "difficulty": "medium",
      "category": "analytical",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.0,
        "diversity_score": 0.7321428571428572
      }
    },
    {
      "question": "How are the attention energies calculated?",
      "context": "These weights specify which of the input states is now how important. So that's one layer of neural network.",
      "difficulty": "medium",
      "category": "analytical",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.25,
        "diversity_score": 0.7083333333333334
      }
    },
    {
      "question": "What role does the previous decoder state play in the attention mechanism?",
      "context": "The attention energies need to be normalized so that they sum to one and then when they sum to one you can see and simply use them as weights to combine all the encoder states.",
      "difficulty": "hard",
      "category": "inferential",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.2222222222222222,
        "diversity_score": 0.7638888888888888
      }
    }
  ]
}