{
  "all_questions": [
    {
      "question": "What is the state-of-the-art tool for word alignment?",
      "context": "And now we can move to word alignment. So in word alignment you are already given a sentence in two languages and you need to align tokens, I'll keep saying words but I actually mean tokens in these two languages.",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.2,
        "diversity_score": 0.7372159090909091
      }
    },
    {
      "question": "Why is Giza++ considered complicated to compile?",
      "context": "The fast align uses a reduced set of alignment modules and it can be pretty bad especially if the training data is small. So if you really strive for quality I would still recommend try compiling this Giza++ tool.",
      "difficulty": "medium",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.2,
        "diversity_score": 0.8696911196911197
      }
    },
    {
      "question": "What are the limitations of word alignment in terms of alignment modules?",
      "context": "The word alignments formally are restricted to a function. So for every source token you are indicating which of the target token corresponds to it or optionally you can make the alignment to point to no word so to a null.",
      "difficulty": "medium",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.5555555555555556,
        "diversity_score": 0.7754310344827586
      }
    },
    {
      "question": "How does Giza++ implement word alignment?",
      "context": "One Giza++ implements is a cascade of models. and the word level models, the IBM 1 till IBM 4 remained in use only for the word alignment.",
      "difficulty": "hard",
      "category": "factual",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.5,
        "diversity_score": 0.75
      }
    },
    {
      "question": "What are the implications of using IBM model 1 for word alignment?",
      "context": "So we'll discuss in close detail the IBM model 1, which relies on the so-called lexical probabilities, the probabilities of translations of individual words, and will only illustrate what the other models do.",
      "difficulty": "hard",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.6666666666666666,
        "diversity_score": 0.8017241379310345
      }
    }
  ],
  "selected_questions": [
    {
      "question": "What is the state-of-the-art tool for word alignment?",
      "context": "And now we can move to word alignment. So in word alignment you are already given a sentence in two languages and you need to align tokens, I'll keep saying words but I actually mean tokens in these two languages.",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.2,
        "diversity_score": 0.7372159090909091
      }
    },
    {
      "question": "Why is Giza++ considered complicated to compile?",
      "context": "The fast align uses a reduced set of alignment modules and it can be pretty bad especially if the training data is small. So if you really strive for quality I would still recommend try compiling this Giza++ tool.",
      "difficulty": "medium",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.2,
        "diversity_score": 0.8696911196911197
      }
    },
    {
      "question": "What are the limitations of word alignment in terms of alignment modules?",
      "context": "The word alignments formally are restricted to a function. So for every source token you are indicating which of the target token corresponds to it or optionally you can make the alignment to point to no word so to a null.",
      "difficulty": "medium",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.5555555555555556,
        "diversity_score": 0.7754310344827586
      }
    }
  ]
}