{
  "all_questions": [
    {
      "question": "What is the initial input to the self-attention mechanism?",
      "context": "So here is the summary of the self-attention. You have the input sequence of representations.",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.4,
        "diversity_score": 0.5989583333333334
      }
    },
    {
      "question": "How many heads are in the self-attention mechanism, and what do they consist of?",
      "context": "You have a predefined number of heads. Each of the heads is defined by the number by the weight matrices that specify what are keys queries and values.",
      "difficulty": "medium",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.2727272727272727,
        "diversity_score": 0.7663043478260869
      }
    },
    {
      "question": "What is the process for calculating the weights in the self-attention mechanism?",
      "context": "You compare all the keys with all the values. queries and that will give you the weights that you apply to the values.",
      "difficulty": "medium",
      "category": "factual",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.16666666666666666,
        "diversity_score": 0.6375
      }
    },
    {
      "question": "How is the final representation obtained after applying the self-attention mechanism?",
      "context": "You have eight of those. You concatenate them. You have one train again projection matrix which squashes it to the original dimension of the representation.",
      "difficulty": "medium",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.125,
        "diversity_score": 0.73125
      }
    },
    {
      "question": "What is the purpose of considering the previous decoder information in the self-attention mechanism?",
      "context": "In the decoder you do this twice and once you are considering the the self-attention the previous decoder information and in the second step you are considering also the information from the encoder.",
      "difficulty": "hard",
      "category": "inferential",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.5555555555555556,
        "diversity_score": 0.7297149122807017
      }
    }
  ],
  "selected_questions": [
    {
      "question": "What is the initial input to the self-attention mechanism?",
      "context": "So here is the summary of the self-attention. You have the input sequence of representations.",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.4,
        "diversity_score": 0.5989583333333334
      }
    },
    {
      "question": "How many heads are in the self-attention mechanism, and what do they consist of?",
      "context": "You have a predefined number of heads. Each of the heads is defined by the number by the weight matrices that specify what are keys queries and values.",
      "difficulty": "medium",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.2727272727272727,
        "diversity_score": 0.7663043478260869
      }
    },
    {
      "question": "How is the final representation obtained after applying the self-attention mechanism?",
      "context": "You have eight of those. You concatenate them. You have one train again projection matrix which squashes it to the original dimension of the representation.",
      "difficulty": "medium",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.125,
        "diversity_score": 0.73125
      }
    },
    {
      "question": "What is the purpose of considering the previous decoder information in the self-attention mechanism?",
      "context": "In the decoder you do this twice and once you are considering the the self-attention the previous decoder information and in the second step you are considering also the information from the encoder.",
      "difficulty": "hard",
      "category": "inferential",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.5555555555555556,
        "diversity_score": 0.7297149122807017
      }
    }
  ]
}