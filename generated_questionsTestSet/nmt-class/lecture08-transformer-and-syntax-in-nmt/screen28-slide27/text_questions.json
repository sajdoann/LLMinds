{
  "all_questions": [
    {
      "question": "What type of attention mechanism does the encoder-decoder use?",
      "context": "The standard thing that we would expect based on the sequence to sequence with attention",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.125,
        "diversity_score": 0.7927350427350428
      }
    },
    {
      "question": "How do the decoder's self-attention positions attend to all previous positions in the encoder?",
      "context": "And it would be very wrong to look at the words which I have not produced so far.",
      "difficulty": "medium",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.0,
        "diversity_score": 0.7048611111111112
      }
    },
    {
      "question": "What is the purpose of masking in decoder self-attention?",
      "context": "The decoder produces the output one step at a time. And it would be very wrong to look at the words which I have not produced so far.",
      "difficulty": "hard",
      "category": "analytical",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.16666666666666666,
        "diversity_score": 0.6833333333333333
      }
    },
    {
      "question": "Why is it necessary to prevent looking into the future in decoder self-attention?",
      "context": "So that's the encoder-decoder attention. Then we have the encoder self-attention. that is what we have discussed so far.",
      "difficulty": "hard",
      "category": "inferential",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.1111111111111111,
        "diversity_score": 0.7858974358974359
      }
    },
    {
      "question": "How does the decoder self-attention mechanism differ from the encoder self-attention mechanism?",
      "context": "Their keys values and queries are actually said to be identical. So there is only one matrix used for all of that.",
      "difficulty": "medium",
      "category": "analytical",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.0,
        "diversity_score": 0.8068181818181819
      }
    }
  ],
  "selected_questions": [
    {
      "question": "What type of attention mechanism does the encoder-decoder use?",
      "context": "The standard thing that we would expect based on the sequence to sequence with attention",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.125,
        "diversity_score": 0.7927350427350428
      }
    },
    {
      "question": "How does the decoder self-attention mechanism differ from the encoder self-attention mechanism?",
      "context": "Their keys values and queries are actually said to be identical. So there is only one matrix used for all of that.",
      "difficulty": "medium",
      "category": "analytical",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.0,
        "diversity_score": 0.8068181818181819
      }
    },
    {
      "question": "How do the decoder's self-attention positions attend to all previous positions in the encoder?",
      "context": "And it would be very wrong to look at the words which I have not produced so far.",
      "difficulty": "medium",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.0,
        "diversity_score": 0.7048611111111112
      }
    },
    {
      "question": "Why is it necessary to prevent looking into the future in decoder self-attention?",
      "context": "So that's the encoder-decoder attention. Then we have the encoder self-attention. that is what we have discussed so far.",
      "difficulty": "hard",
      "category": "inferential",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.1111111111111111,
        "diversity_score": 0.7858974358974359
      }
    }
  ]
}