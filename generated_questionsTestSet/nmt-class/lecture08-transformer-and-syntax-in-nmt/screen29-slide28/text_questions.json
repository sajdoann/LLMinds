{
  "all_questions": [
    {
      "question": "What is the purpose of using attention weights in a similar way as in the sequence-to-sequence algorithm?",
      "context": "So this is what the heads can do. And there have been multiple papers studying what the heads look at, what they consider.",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.09090909090909091,
        "diversity_score": 0.781578947368421
      }
    },
    {
      "question": "How does the network determine where to look for information when producing a representation?",
      "context": "So if you consider the layer 5 of the encoder and this and you look at one of the heads and you look at what the head is looking at, what weights it is assigning to the positions when it is producing the representation.",
      "difficulty": "medium",
      "category": "analytical",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.3,
        "diversity_score": 0.7678571428571428
      }
    },
    {
      "question": "How did the network figure out that 'it' was the antecedent of the pronoun?",
      "context": "And arguably when interpreting this sentence we as humans have to know that this it and this is the head number 1 on the layer 5 was actually able to figure that out.",
      "difficulty": "medium",
      "category": "analytical",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.3,
        "diversity_score": 0.755787037037037
      }
    },
    {
      "question": "What did the trained weights manage to identify in the antecedent of the pronoun?",
      "context": "So the weights, the trained weights, managed to identify the antecedent of the pronoun.",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.6666666666666666,
        "diversity_score": 0.6854166666666667
      }
    },
    {
      "question": "What did the network learn how to do when producing the next layer representation of the word it?",
      "context": "So the network learned how to find",
      "difficulty": "hard",
      "category": "analytical",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.21428571428571427,
        "diversity_score": 0.6729910714285714
      }
    }
  ],
  "selected_questions": [
    {
      "question": "What is the purpose of using attention weights in a similar way as in the sequence-to-sequence algorithm?",
      "context": "So this is what the heads can do. And there have been multiple papers studying what the heads look at, what they consider.",
      "difficulty": "easy",
      "category": "factual",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.09090909090909091,
        "diversity_score": 0.781578947368421
      }
    },
    {
      "question": "How did the network figure out that 'it' was the antecedent of the pronoun?",
      "context": "And arguably when interpreting this sentence we as humans have to know that this it and this is the head number 1 on the layer 5 was actually able to figure that out.",
      "difficulty": "medium",
      "category": "analytical",
      "evaluation": {
        "answerable": true,
        "context_relevance": 0.3,
        "diversity_score": 0.755787037037037
      }
    },
    {
      "question": "How does the network determine where to look for information when producing a representation?",
      "context": "So if you consider the layer 5 of the encoder and this and you look at one of the heads and you look at what the head is looking at, what weights it is assigning to the positions when it is producing the representation.",
      "difficulty": "medium",
      "category": "analytical",
      "evaluation": {
        "answerable": false,
        "context_relevance": 0.3,
        "diversity_score": 0.7678571428571428
      }
    }
  ]
}