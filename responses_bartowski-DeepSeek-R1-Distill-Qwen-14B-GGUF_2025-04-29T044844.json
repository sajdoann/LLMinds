{
    "demagog-statements-public/s_id21244/text.en.txt": [
        {
            "question": "What key principle does Petr Fiala highlight in his post-election speech regarding the relationship among Czech citizens despite their political differences?",
            "reference-answers": [
                "The key principle Petr Fiala highlights is that Czech citizens should unite and collaborate despite political differences, emphasizing their common identity as citizens of the Czech Republic."
            ]
        },
        {
            "question": "What does Petr Fiala emphasize in his post-election speech about governing and national unity?",
            "reference-answers": [
                "Petr Fiala emphasizes that a good government governs for everyone, even those who did not vote for it, and stresses national unity by highlighting that all citizens, regardless of political differences, must live together as neighbors and collaborators in the Czech Republic."
            ]
        },
        {
            "question": "What did Petr Fiala emphasize in his speech about the need for unity among Czech citizens despite differing political opinions?",
            "reference-answers": [
                "Petr Fiala emphasized that despite differing political opinions and party affiliations, all Czech citizens are united as neighbors, collaborators, and citizens of the Czech Republic, living together in the country. He stressed that a good government should represent and serve everyone, regardless of whether they voted for it."
            ]
        }
    ],
    "demagog-statements-public/s_id21452/text.en.txt": [
        {
            "question": "What is the reason for the discrepancy in the estimates of the value of tax reliefs mentioned by Miloš Zeman, Alena Schillerová, and the Ministry of Finance?",
            "reference-answers": [
                "The discrepancy in the estimates of the value of tax reliefs arises because there is no uniform methodology for calculating these reliefs. Different methodologies can lead to varying estimates, as seen in the differing figures mentioned by Miloš Zeman, Alena Schillerová, and the Ministry of Finance."
            ]
        },
        {
            "question": "What is the discrepancy between Miloš Zeman's estimate of 380 billion crowns and the Ministry of Finance's estimates, and what explains this difference?",
            "reference-answers": [
                "The discrepancy between Miloš Zeman's estimate of 380 billion crowns and the Ministry of Finance's estimates lies in the differing figures provided by various sources. While Zeman mentioned 380 billion crowns in his speeches, the former Minister of Finance, Alena Schillerová, later referred to amounts of 300 billion, 330 billion, and even 520 billion crowns in different contexts. This difference is explained by the lack of a uniform methodology for calculating tax reliefs, as noted in official documents. Consequently, the varying estimates arise from different analytical approaches and definitions used in the calculations."
            ]
        },
        {
            "question": "What was Miloš Zeman's proposal concerning tax exemptions, and what figure did he cite? How does this figure relate to the state budget deficit, and what discrepancies are noted in the estimates?",
            "reference-answers": [
                "Miloš Zeman's proposal was to abolish tax exemptions, which he claimed would save 380 billion crowns. This figure is nearly equal to the estimated state budget deficit of 500 billion crowns for the year. However, there are discrepancies in the estimates: Alena Schillerová, the former Minister of Finance, mentioned 300 billion and 330 billion in interviews, while the Ministry of Finance's 2018 estimate was over 500 billion. These differences arise due to the lack of a uniform methodology for calculating tax reliefs."
            ]
        }
    ],
    "demagog-statements-public/s_id21471/text.en.txt": [
        {
            "question": "What did Prime Minister Petr Fiala send to the President of the European Commission on December 22nd, and what was included in that communication?",
            "reference-answers": [
                "Prime Minister Petr Fiala sent a letter to the President of the European Commission on December 22nd, in which he expressed the Czech government's support for the inclusion of gas and nuclear energy in the EU taxonomy. The letter included a decision from the Czech Parliament, passed on December 15, 2021, which underscores the Czech Republic's commitment to nuclear energy for its decarbonization goals and supports gas as a transitional energy source."
            ]
        },
        {
            "question": "What did Czech Prime Minister Petr Fiala request in his letter to the President of the European Commission?",
            "reference-answers": [
                "Czech Prime Minister Petr Fiala requested that the President of the European Commission include gas and nuclear energy in the EU taxonomy, supporting the Czech government's position as reflected in the resolution passed by the Czech Parliament."
            ]
        },
        {
            "question": "What is the date of the letter sent by the Czech Prime Minister to the President of the European Commission, and what is the purpose of the resolution attached to this letter?",
            "reference-answers": [
                "The date of the letter sent by the Czech Prime Minister to the President of the European Commission is December 22nd. The purpose of the resolution attached to this letter is to support the inclusion of gas as a transitional source and nuclear energy as a sustainable source in the EU taxonomy."
            ]
        }
    ],
    "demagog-statements-public/s_id21477/text.en.txt": [
        {
            "question": "What did Schillerová propose regarding salary increases for state employees in the spring of 2021, and how did her proposal change by September 2021?",
            "reference-answers": [
                "In the spring of 2021, Schillerová proposed freezing the salaries of most state employees and only increasing them for teachers, healthcare workers, and social service workers as part of budget consolidation. By September 2021, she reversed her position and agreed to increase the salaries of all state employees, including firefighters and police officers, though the increase was less than she had initially planned."
            ]
        },
        {
            "question": "Question: What did Schillerová initially propose regarding salary increases for state employees, and how did the government's policy change later?",
            "reference-answers": [
                "Schillerová initially proposed freezing the salaries of most state employees, except for teachers, healthcare workers, and social service workers, as part of budget consolidation. Later, she agreed to increase salaries for all state employees, including firefighters and police officers. The government's policy shifted to a partial freeze on some salaries while increasing others, reflecting a more relaxed approach compared to her initial stance."
            ]
        },
        {
            "question": "What was the former Finance Minister Schillerová's initial stance on salary increases for state employees, and how did the Fiala government's actions differ from her proposal?",
            "reference-answers": [
                "The former Finance Minister Schillerová initially proposed freezing the salaries of most state employees, except for teachers, healthcare workers, and social service workers, as part of budget consolidation. However, by September 2021, she reversed her position and agreed to increase the salaries of all state employees, including firefighters and police officers, though debates continued over the rate of increase.\n\nThe Fiala government's actions differed by approving salary increases for some state employees, such as social workers, healthcare workers, teachers, and soldiers, firefighters, and police officers, but with lower increases compared to what Schillerová had planned in September 2021. Additionally, the Fiala government decided to freeze the salaries of civil servants in ministries and other offices starting from January 2022, which aligns with Schillerová's initial restrictive position but contradicts her later stance of increasing salaries for all state employees. Thus, the Fiala government's actions partially reflected Schillerová's initial restrictive position but did not fully align with her later proposal for broader salary increases."
            ]
        }
    ],
    "demagog-statements-public/s_id21480/text.en.txt": [
        {
            "question": "Does the text state that international and domestic factors each account for approximately half of the inflation in the Czech Republic?",
            "reference-answers": [
                "No, the text does not state that international and domestic factors each account for approximately half of the inflation in the Czech Republic. While some economists and the Prime Minister suggest that both factors are equally significant, the Czech National Bank (CNB) attributes the primary cause of inflation to international factors, such as supply chain disruptions and global raw material price increases, rather than domestic management."
            ]
        },
        {
            "question": "What do economists and the Czech National Bank agree on regarding the causes of inflation in the Czech Republic, and how do their views differ from the Prime Minister's statements?",
            "reference-answers": [
                "Economists and the Czech National Bank (CNB) agree that international factors, such as supply chain disruptions and rising raw material prices, play a significant role in the current inflation in the Czech Republic. They also agree that domestic factors, such as poor state management and the previous government's fiscal policy, contribute to inflation. However, their views differ from the Prime Minister's statements, which emphasize that both international and domestic factors are equally responsible for inflation. In contrast, the CNB attributes the main responsibility for inflation to international influences, such as global supply chain disruptions and increased raw material prices, rather than domestic factors. The CNB also notes that the government's budgetary policy may have contributed to inflation but does not see it as the primary cause."
            ]
        },
        {
            "question": "What are the main factors contributing to the current inflation in the Czech Republic, according to economists and the Czech National Bank, and how do they differ in their attribution of responsibility?",
            "reference-answers": [
                "According to economists and the Czech National Bank (CNB), the main factors contributing to the current inflation in the Czech Republic include:\n\n1. **International Factors**: \n   - Disruptions in supply chains, leading to delays and shortages of raw materials.\n   - Increased prices of raw materials such as energy, silicon, chips, and construction materials.\n   - Rising costs of transportation, including both ship and land logistics.\n\n2. **Domestic Factors**:\n   - A surge in consumer demand following the lifting of anti-epidemic measures.\n   - Fiscal policy decisions, such as changes in taxation and pension increases, which have affected the state budget and contributed to inflationary pressures.\n\nWhile both international and domestic factors are acknowledged as contributors, there is a divergence in how responsibility is attributed. Economists and the CNB generally agree that international influences, particularly supply chain disruptions and global raw material price increases, are the primary drivers of inflation. However, some economists also highlight the role of domestic factors, such as government fiscal policies, in exacerbating inflation. The CNB, in particular, does not consider poor state management as the main cause, instead emphasizing the global nature of supply-side inflation and the impact of increased consumer demand."
            ]
        }
    ],
    "demagog-statements-public/s_id21519/text.en.txt": [
        {
            "question": "What triggers the automatic increase in pensions and how is the increase calculated based on the provided mechanism?",
            "reference-answers": [
                "The automatic increase in pensions is triggered when the cumulative inflation, as measured by the consumer price index, exceeds 5% since the last valuation month. This mechanism is outlined in the Pension Insurance Act and is referred to as extraordinary pension valorization. The period for determining inflation begins in July 2021, following the last valuation in June 2021. When the cumulative inflation reaches over 5%, pensions are increased by the inflation rate in the specific month when this threshold was met."
            ]
        },
        {
            "question": "**Question:** What is the starting month of the period used to determine if pensions will be automatically increased due to inflation?",
            "reference-answers": [
                "July"
            ]
        },
        {
            "question": "What is the expected date for the next extraordinary pension increase, given that inflation is projected to exceed 5%?",
            "reference-answers": [
                "The next extraordinary pension increase is expected in June 2022."
            ]
        }
    ],
    "demagog-statements-public/s_id21531/text.en.txt": [
        {
            "question": "Which three ministries are primarily responsible for the COVID-19 compensation programs, and what are the specific programs each ministry manages?",
            "reference-answers": [
                "The three ministries primarily responsible for the COVID-19 compensation programs are:\n\n1. **Ministry of Labor and Social Affairs**: Manages the Antivirus A and Antivirus B programs. Antivirus A compensates employers for costs due to employee quarantines, while Antivirus B assists companies with limited operations due to quarantines.\n\n2. **Ministry of Industry and Trade**: Oversees the COVID 2021 subsidy program and the COVID Uncovered Costs program, aimed at companies with significant sales drops. It also managed the COVID Sport guarantee program for loans.\n\n3. **Ministry of Finance**: Runs the Compensation Bonus program for self-employed individuals and partners of small companies."
            ]
        },
        {
            "question": "Which three ministries are primarily responsible for sharing the compensation programs for entrepreneurs during the COVID-19 pandemic?",
            "reference-answers": [
                "The three ministries primarily responsible for sharing the compensation programs for entrepreneurs during the COVID-19 pandemic are the Ministry of Labor and Social Affairs, the Ministry of Industry and Trade, and the Ministry of Finance."
            ]
        },
        {
            "question": "Which three ministries are primarily responsible for the COVID-19 compensation programs, and what specific programs does each ministry handle?",
            "reference-answers": [
                "The three ministries primarily responsible for the COVID-19 compensation programs are:\n\n1. **Ministry of Labor and Social Affairs**: \n   - Antivirus A program: Compensates costs for employers whose employees were ordered to quarantine or isolate.\n   - Antivirus B program: Compensates companies for limitations in operations due to a significant number of employees in quarantine or reduced demand for services/products due to the pandemic.\n\n2. **Ministry of Industry and Trade**: \n   - COVID 2021 subsidy program: Intended for companies with a significant drop in sales (not launched again).\n   - COVID Uncovered Costs program: Aimed at covering uncovered costs for companies.\n   - COVID Sport guarantee program: Provides loan guarantees for sports-related businesses.\n   - COVID Záruka CK program: Guarantee program for travel agencies (terminated).\n\n3. **Ministry of Finance**: \n   - Compensation Bonus: Provides support for damaged self-employed individuals, partners of small companies, and persons working on DPČ and DPP."
            ]
        }
    ],
    "demagog-statements-public/s_id21793/text.en.txt": [
        {
            "question": "What are the three reasons given in the initial statement regarding Miloš Balák's case?",
            "reference-answers": [
                "1. Mr. Balák did not personally enrich himself in this case.  \n2. No economic damage was caused.  \n3. He received three years in prison for allegedly favoring a construction company, specifically the Kladno company ESG (now known as ESB), by allowing it to tour the Lánská obory before the tender was announced."
            ]
        },
        {
            "question": "What specific actions did Miloš Balák take to favor the ESB company, and what was the potential economic benefit for ESB?",
            "reference-answers": [
                "Miloš Balák favored the ESB company by determining the contractor in advance, allowing ESB to participate in project preparation, and providing them with non-public information. The potential economic benefit for ESB was a profit of nearly 9 million crowns, calculated by the police expert opinion, which was prevented due to the initiation of criminal proceedings."
            ]
        },
        {
            "question": "**Question:**  \nDid Miloš Balák personally enrich himself or cause economic damage in the case involving the construction company ESB?",
            "reference-answers": [
                "**Answer:**  \nNo, Miloš Balák did not personally enrich himself in this case. However, he was found guilty of manipulating a contract to favor the ESB company, which could have resulted in economic damage. The planned profit for ESB was nearly 9 million crowns, but due to the initiation of criminal proceedings, this potential damage did not materialize."
            ]
        }
    ],
    "demagog-statements-public/s_id21794/text.en.txt": [
        {
            "question": "Question: Name two individuals pardoned by Václav Havel and briefly describe the circumstances of their pardons.",
            "reference-answers": [
                "1. **Martin Odložil**: Pardoned by Václav Havel after being charged with grievous bodily harm resulting in the death of his father, Josef Odložil, during an argument. Havel granted clemency before the appeals court could decide, despite the case not being a murder charge. Odložil later admitted to killing his father.\n\n2. **Marta Chadimová**: Accused of fraud and forgery related to the restitution of a house. Havel pardoned her in 1995 due to her prolonged trial and serious illness. The trial never concluded, and she accepted the pardon in 2003."
            ]
        },
        {
            "question": "Question: How many pardons did Václav Havel, Václav Klaus, and Miloš Zeman grant during their presidencies, and what were the specific controversial cases involving Martin Odložil and Marta Chadimová that Václav Havel pardoned?",
            "reference-answers": [
                "Václav Havel granted over 1,000 pardons during his presidency, including 860 between 1993 and 2002. Václav Klaus pardoned 412 people, and Miloš Zeman granted 21 pardons. Havel pardoned Martin Odložil, who was convicted of grievous bodily harm leading to his father's death, despite later admitting to killing him. He also pardoned Marta Chadimová, accused of fraud related to restitution, though her trial never concluded."
            ]
        },
        {
            "question": "How many pardons did Václav Havel, Václav Klaus, and Miloš Zeman grant during their respective terms as Czech presidents?",
            "reference-answers": [
                "Václav Havel granted over 1,000 pardons, Václav Klaus granted 412, and Miloš Zeman granted 21 pardons during their respective terms."
            ]
        }
    ],
    "demagog-statements-public/s_id21804/text.en.txt": [
        {
            "question": "What did Marian Jurečka correct regarding the calculation of the income limit for the 5,000 CZK contribution?",
            "reference-answers": [
                "Marian Jurečka corrected that the income limit for the 5,000 CZK contribution would be calculated based on the gross annual income, not the net income as he initially stated."
            ]
        },
        {
            "question": "What did Marian Jurečka clarify about the income limit for the one-time contribution after initially providing incorrect information?",
            "reference-answers": [
                "Marian Jurečka clarified that the income limit for the one-time contribution is based on the gross annual income of the family, correcting his initial incorrect statement that it was based on net income."
            ]
        },
        {
            "question": "What was the initial answer given by Marian Jurečka regarding whether the 5,000 CZK contribution was based on gross or net income, and how did he later correct this statement?",
            "reference-answers": [
                "Marian Jurečka initially stated that the 5,000 CZK contribution was based on net income. He later corrected this, clarifying that it would be based on gross income and apologized for the earlier inaccuracy."
            ]
        }
    ],
    "demagog-statements-public/s_id22492/text.en.txt": [
        {
            "question": "What is the maximum duration for which the government can send soldiers abroad without parliamentary consent under the proposed amendment?",
            "reference-answers": [
                "The maximum duration for which the government can send soldiers abroad without parliamentary consent under the proposed amendment is 60 days."
            ]
        },
        {
            "question": "What is the main goal of the amendment proposed by Petr Fiala's government regarding the deployment of Czech soldiers abroad, and what are the key reasons behind it?",
            "reference-answers": [
                "The main goal of the amendment proposed by Petr Fiala's government is to expand the government's ability to deploy Czech soldiers abroad without prior parliamentary consent in additional situations beyond the currently defined three cases. The key reasons behind this proposal are to enable a faster and more flexible response in situations such as rescuing Czech citizens illegally detained abroad and to allow the Czech Republic to participate in the NATO Very Rapid Reaction Force (VJTF) even in cases that do not fall under the existing constitutional provisions. The amendment also aims to provide the government with broader operational authority to deploy troops abroad or permit foreign troops on Czech territory without specifying the purpose, while still requiring parliamentary approval after 60 days."
            ]
        },
        {
            "question": "Question: Is Andrej Babiš's statement about the government's proposal to send soldiers abroad without parliamentary consent accurate, or is it misleading given the broader scope of the proposal?",
            "reference-answers": [
                "Andrej Babiš's statement is misleading because while he mentioned the case of rescuing Czech citizens, the broader proposal allows the government to send soldiers abroad for up to 60 days without parliamentary consent in situations beyond just rescuing citizens, including NATO missions not covered by the current constitutional provisions."
            ]
        }
    ],
    "demagog-statements-public/s_id22620/text.en.txt": [
        {
            "question": "What has been the Constitutional Court's approach when it has found doubts about the legislative process in past cases?",
            "reference-answers": [
                "The Constitutional Court has, in past cases, provided non-binding statements (obiter dicta) when it found doubts about the legislative process. These statements may include reservations about the process or suggest shortcomings without altering the law's substance."
            ]
        },
        {
            "question": "What has the Constitutional Court done in cases where it has found doubts about the legislative process, but has not altered the substance of the law?",
            "reference-answers": [
                "In cases where the Constitutional Court has found doubts about the legislative process but has not altered the substance of the law, it has issued non-binding statements known as obiter dicta. These statements often include critical remarks about the legislative process, provide recommendations, or indicate how the Court would view potential corrections. For example, in past cases, the Court has criticized the legislative process as flawed or chaotic and has given a timeframe or outlined its expectations for future adjustments."
            ]
        },
        {
            "question": "What has the Constitutional Court done in the past when it found doubts about the legislative process, but did not change the substance of the law?",
            "reference-answers": [
                "The Constitutional Court has, in the past, issued non-binding statements known as obiter dicta when it found doubts about the legislative process but did not change the substance of the law. These statements often include criticisms or recommendations regarding the legislative process, and may also set deadlines or explain how corrections would be viewed."
            ]
        }
    ],
    "demagog-statements-public/s_id22632/text.en.txt": [
        {
            "question": "Did Marian Jurečka agree with Eva Davidová's statement about the government discussing the pension amendment in the week of March 20-26, 2023?",
            "reference-answers": [
                "No, Marian Jurečka did not agree with Eva Davidová's statement. He denied that the government would discuss the pension amendment in the week of March 20-26, 2023, and stated that negotiations with the opposition and social partners would take place first."
            ]
        },
        {
            "question": "**Question:** Was the government planning to discuss the amendment to the pension valorization mechanism in the week of March 20–26, 2023?",
            "reference-answers": [
                "The government was not planning to discuss the amendment to the pension valorization mechanism in the week of March 20–26, 2023. Minister Marian Jurečka clarified that the proposal would first be negotiated with the opposition, trade unions, and employers, and only after completing these consultations and the interdepartmental comment procedure would the amendment be submitted to the government."
            ]
        },
        {
            "question": "What clarification did Minister Jurečka provide on Twitter regarding the timing of the amendment submission, and how did it relate to previous statements from his spokesperson?",
            "reference-answers": [
                "Minister Jurečka clarified on Twitter that he would not submit the amendment until after completing negotiations with the opposition and social partners, as well as the proper interdepartmental comment procedure. This contradicted his spokesperson's earlier statement that the government might discuss the amendment in the near future."
            ]
        }
    ],
    "demagog-statements-public/s_id22635/text.en.txt": [
        {
            "question": "Which groups will primarily be affected by the proposed pension reform according to the statement?",
            "reference-answers": [
                "The proposed pension reform will primarily affect people in their forties, thirties, and younger individuals. The current pensioners and those aged 50 or older will not experience any fundamental changes."
            ]
        },
        {
            "question": "What group is the pension reform primarily targeting, based on the statements provided?",
            "reference-answers": [
                "The pension reform is primarily targeting younger generations, specifically people in their forties, thirties, and younger."
            ]
        },
        {
            "question": "What age groups will the pension reform affect, and what is assured for those 50 and older?",
            "reference-answers": [
                "The pension reform will primarily affect people in their forties, thirties, and younger. Those aged 50 and older, including current pensioners and individuals approaching retirement age, will not experience any fundamental changes."
            ]
        }
    ],
    "demagog-statements-public/s_id22878/text.en.txt": [
        {
            "question": "What percentage of cases experience delays in processing housing allowance applications due to clients not providing required documents, as stated by the Minister?",
            "reference-answers": [
                "98 percent of cases"
            ]
        },
        {
            "question": "What timeframe does the Minister claim is typical for processing housing allowance applications, and what reason does he provide for the majority of delays?",
            "reference-answers": [
                "The Minister claims that the typical processing timeframe for housing allowance applications is between 15 and 21 days in most regions, with Prague at around 30 days. He attributes the majority of delays to clients failing to provide requested documents or information in 98 percent of cases."
            ]
        },
        {
            "question": "Is the claim that in 98 percent of cases, the processing of housing allowance applications takes around 30 days in Prague because clients have not provided the required documents or information supported by the provided data?",
            "reference-answers": [
                "No, the claim is not supported by the provided data. The text states that the internal data from the Ministry of Labour and Social Affairs does not support the Minister's claim about the 98 percent figure or the reason for the 30-day processing time in Prague. The data only provides average processing times and does not exclude the possibility of individual cases being processed beyond the statutory deadline. Additionally, the text notes that the Ministry did not respond to further requests for statistics on cases where deadlines were met or not, leaving the claim unverifiable."
            ]
        }
    ],
    "demagog-statements-public/s_id22921/text.en.txt": [
        {
            "question": "How does the increase in the number of foreigners in the Czech Republic compare to the increase in criminal activity by foreigners from 2020 to 2023?",
            "reference-answers": [
                "The number of foreigners in the Czech Republic increased by 3.4 percentage points (from 5.9% in 2020 to 9.3% in 2023), while their share in criminal activity increased by 2.7 percentage points (from 8.2% in 2020 to 10.9% in the first half of 2023)."
            ]
        },
        {
            "question": "How does the increase in the share of foreigners in the Czech Republic's population compare to the increase in their share of criminal activity between 2020 and 2023?",
            "reference-answers": [
                "The share of foreigners in the Czech Republic's population increased by 3.4 percentage points between 2020 and 2023, while their share in criminal activity increased by 2.7 percentage points. Thus, the increase in the population share is slightly higher than the increase in their share of criminal activity."
            ]
        },
        {
            "question": "What does the data indicate about the relationship between the increase in the number of foreigners in the Czech Republic and the increase in criminal activity by foreigners from 2020 to 2023?",
            "reference-answers": [
                "The data indicates that while the number of foreigners in the Czech Republic increased by approximately 3.4 percentage points from 2020 to 2023, the share of criminal activity by foreigners increased by about 2.7 percentage points during the same period. Therefore, the increase in criminal activity by foreigners does not correspond proportionally to the increase in the number of foreigners in the population."
            ]
        }
    ],
    "demagog-statements-public/s_id22954/text.en.txt": [
        {
            "question": "What was the final budget deficit for next year that was agreed upon by the government?",
            "reference-answers": [
                "252 billion crowns"
            ]
        },
        {
            "question": "What was the final agreed budget deficit for next year according to the Ministry of Finance's draft?",
            "reference-answers": [
                "252 billion crowns"
            ]
        },
        {
            "question": "What was the final budget deficit agreed upon for the next year, and how does it compare to the initial proposal and the government's established ceiling?",
            "reference-answers": [
                "The final budget deficit agreed upon for the next year was 252 billion crowns. Compared to the initial proposal of 235 billion crowns and the government's established ceiling of 270 billion crowns, the final deficit is higher than the initial proposal but remains within the established ceiling."
            ]
        }
    ],
    "demagog-statements-public/s_id23177/text.en.txt": [
        {
            "question": "What was the name of the cooperative that Petr Fiala visited after the school in Pozdatin?",
            "reference-answers": [
                "Pooslaví cooperative"
            ]
        },
        {
            "question": "What was the main aim of the trade union strike mentioned in the text?",
            "reference-answers": [
                "The main aim of the trade union strike was to promote a higher budget for education."
            ]
        },
        {
            "question": "Question:  \nWho was one of the leading representatives at the agricultural enterprise that Prime Minister Petr Fiala visited during the trade union strike?",
            "reference-answers": [
                "Lukáš Jurečka"
            ]
        }
    ],
    "demagog-statements-public/s_id23626/text.en.txt": [
        {
            "question": "What was the main objective of the agreement between the opposition and the coalition around 2015?",
            "reference-answers": [
                "The main objective of the agreement was to automate the growth of politicians' salaries so that they would rise when the economy was doing well and would not increase when the economy was performing badly."
            ]
        },
        {
            "question": "What was the purpose of the agreement between the opposition and the coalition around 2015 regarding the growth of politicians' salaries?",
            "reference-answers": [
                "The purpose of the agreement was to create a mechanism where the salaries of politicians would automatically grow in line with the economy, ensuring increases when the economy was doing well and preventing growth when the economy was not."
            ]
        },
        {
            "question": "What were the key features of the agreement regarding the automatic growth of politicians' salaries discussed around 2015, and how was this mechanism implemented?",
            "reference-answers": [
                "The key features of the agreement regarding the automatic growth of politicians' salaries discussed around 2015 included linking salary increases to the economy's performance, with salaries growing during good economic times and remaining static during downturns. The mechanism was implemented through an amendment to the law in 2014, which reintroduced an automatic system for calculating salaries based on the average wage in the non-business sector. This system set specific multipliers for salary calculation, such as 2.25 times the average wage initially, increasing to 2.5 times by 2018, and reverting to 2.75 times from 2019. The STAN movement's deputies abstained from voting on this amendment, indicating their non-involvement or disagreement with the terms."
            ]
        }
    ],
    "demagog-statements-public/s_id23655/text.en.txt": [
        {
            "question": "How do the CZSO's demographic projections for 2023 differ from those of 2018 in terms of their implications for the Czech pension system?",
            "reference-answers": [
                "The CZSO's 2023 demographic projections for the Czech pension system are more pessimistic compared to the 2018 projections. While life expectancy is still expected to rise, the 2023 projection shows a slower increase than previously anticipated. The number of people of working age is projected to decline more significantly, particularly in the long term, with the 2023 projection indicating a larger decrease in the working-age population share compared to the 2018 projection. Additionally, the 2023 projection stabilizes total fertility at a lower rate (1.5 children per woman) compared to the 2018 projection, which assumed a higher stabilization rate (1.74). These factors collectively suggest that the pension system will face greater challenges in the future, with a reduced workforce and increased dependency ratio."
            ]
        },
        {
            "question": "Based on the CZSO demographic projections from 2023 and 2018, how has the demographic situation changed, and what are the implications for the pension system?",
            "reference-answers": [
                "The CZSO projections indicate that life expectancy will continue to rise, the working-age population will decline, and the fertility rate will stabilize at a low level. These factors pose challenges for the pension system, as fewer working-age individuals will contribute to funding pensions while more people will be drawing benefits. Although the 2023 projection shows slightly better prospects for the working-age population in the medium term (2031–2071) compared to the 2018 projection, it assumes worse development in the long term (2081–2100). Overall, the demographic situation has worsened from the perspective of the pension system, particularly in the long run."
            ]
        },
        {
            "question": "According to the CZSO projections, which projection (2023 or 2018) presents a worse situation for the pension system and over what period does this worse situation occur?",
            "reference-answers": [
                "The 2023 projection presents a worse situation for the pension system, particularly from 2081–2101."
            ]
        }
    ],
    "flat-earth-book/SECTION-1/text.txt": [
        {
            "question": "Question:  \nAccording to the text, what argument is presented regarding the cause of the \"spherical excess\" observed in levelling, and how is this phenomenon explained?",
            "reference-answers": [
                "The argument presented is that the \"spherical excess\" observed in levelling is not due to the Earth's curvature but is instead caused by the peculiarities of surveying instruments. The phenomenon is explained as arising from the fact that the line of sight in theodolites and spirit levels is not perfectly axial or central, leading to a divergence in the observed direction of lines, particularly when magnified through the instruments. This effect is similar to how a convex lens or magnifying glass distorts straight lines when held off-center, demonstrating that the apparent curvature is an artifact of the tools used rather than the Earth's actual shape."
            ]
        },
        {
            "question": "According to the text, what discrepancy exists between the theory of the Earth's rotundity and practical navigation, and what does this imply about the Earth's shape?",
            "reference-answers": [
                "The discrepancy between the theory of the Earth's rotundity and practical navigation lies in the significant difference between the theoretical circumference of the Earth at 45° latitude (14,282 miles) and the actual distances reported by navigators (22,000 miles). This implies that the Earth's shape is not spherical, as the practical results far exceed the theoretical predictions, suggesting instead a flat Earth."
            ]
        },
        {
            "question": "According to the text, what discrepancy exists between the theoretical circumference of the Earth as a globe and the results of practical navigation?",
            "reference-answers": [
                "The discrepancy is that the theoretical circumference of the Earth as a globe (25,000 miles) does not align with practical navigation results. Navigators report distances that suggest a much larger circumference, with an error of 7,718 miles and a total navigated distance of 22,000 miles, exceeding the theoretical limit."
            ]
        }
    ],
    "flat-earth-book/SECTION-10/text.txt": [
        {
            "question": "What is the primary cause of tides according to the text?",
            "reference-answers": [
                "The primary cause of tides, as explained in the text, is the Earth's fluctuation due to the pressure of the atmosphere acting upon it. This causes the Earth to slowly rise and fall in the water, resulting in the ebb and flow of tides."
            ]
        },
        {
            "question": "Question: According to the text, what is the primary cause of tides in the ocean?",
            "reference-answers": [
                "The primary cause of tides in the ocean, according to the text, is the constant pressure of the atmosphere upon the Earth, which causes the Earth to fluctuate or rise and fall slightly in the water. This movement results in the water moving towards the receding shores during the flood tide and receding during the ebb tide."
            ]
        },
        {
            "question": "What does the text suggest is the primary cause of tides, and what evidence is provided to support this explanation?",
            "reference-answers": [
                "The text suggests that the primary cause of tides is the constant pressure of the atmosphere upon the Earth, which causes the Earth to slowly rise and fall in the water. This motion leads to the movement of water towards the receding shores (flood tide) and the subsequent recession of water (ebb tide). \n\nEvidence supporting this explanation includes:\n1. Observations from barometric and pneumatic experiments, as well as divers' accounts, showing that storms only disturb the ocean's surface and not the deeper layers.\n2. The fluctuating motion observed in floating vessels, which vary in size and anchoring, indicating a connection between atmospheric pressure and water movement.\n3. The presence of tides in polar regions, as described by Dr. Kane, which are not connected to the Atlantic Ocean but still exhibit regular ebb and flood, suggesting the Earth's inherent motion."
            ]
        }
    ],
    "flat-earth-book/SECTION-11/text.txt": [
        {
            "question": "What evidence does the text provide regarding the Earth's internal condition and its potential for destruction by fire?",
            "reference-answers": [
                "The text provides evidence that the Earth's interior is in a molten, incandescent state with intense heat, as indicated by the increasing temperature with depth. For example, at a depth of about twelve miles, the temperature approaches an incandescent mass, and at ten miles, all rocks would be melted. This internal heat is supported by the existence of hot springs, which bring warm mineral waters to the surface, and by the temperature increasing at a rate of one degree Fahrenheit for every 545 feet of descent. Volcanic activity, such as the eruption of lava, ash, and gases, is described as a symptom of this internal combustion. Additionally, the text notes that the Earth's crust is relatively thin (9 to 12 miles) compared to the burning interior. The combustible materials within the Earth, such as coals, peat, mineral oils, and hydrocarbons, combined with the highly inflammable nature of the rocks and minerals, suggest that the internal fire could potentially extend and intensify, leading to a catastrophic, annihilating conflagration."
            ]
        },
        {
            "question": "What evidence is presented in the text to support the claim that the lower parts of the Earth are on fire?",
            "reference-answers": [
                "The text presents several pieces of evidence to support the claim that the lower parts of the Earth are on fire:\n\n1. **Temperature Data**: The text references experiments showing that the Earth's temperature increases as we descend below the surface. For example, at a depth of about twelve miles, the temperature approaches an incandescent state. Similarly, in coal mines, the temperature increases steadily with depth, indicating the presence of internal heat.\n\n2. **Hot Springs and Geothermal Activity**: The existence of hot springs and boiling water emerging from the Earth is cited as evidence of internal heat. For instance, the temperature of the Bath waters is recorded at 116°F, suggesting they rise from a significant depth.\n\n3. **Volcanic Eruptions**: The text describes volcanic eruptions as evidence of a fiery interior. Volcanoes eject lava, ash, and gases, which are products of a molten, incandescent mass beneath the Earth's surface.\n\n4. **Subterranean Combustion**: The text mentions that the Earth's lower parts are combustible, with highly inflammable materials such as coals, peat, and hydro-carbons existing deep within the Earth. This suggests ongoing combustion in the Earth's interior.\n\n5. **Comparison to Known Fires**: The text compares the Earth to a ship with a burning hold, emphasizing that the internal fire is surrounded by combustible materials, making it likely that the fire will continue to spread and intensify.\n\nThese points collectively support the claim that the lower parts of the Earth are on fire, with evidence drawn from geological observations, temperature measurements, and volcanic activity."
            ]
        },
        {
            "question": "What does the Earth's internal structure, as described in the text, imply about its ultimate fate?",
            "reference-answers": [
                "The Earth's internal structure, as described, implies that its ultimate fate will be an annihilating conflagration. The Earth's interior is composed of combustible materials surrounded by highly inflammable substances, and the intense internal fire, which is currently contained, will eventually spread and intensify. This will result in the entire Earth decomposing, volatilizing, and bursting into a vast, indescribable fire."
            ]
        }
    ],
    "flat-earth-book/SECTION-12/text.txt": [
        {
            "question": "According to the text, what is the main conclusion regarding the use of Foucault's pendulum experiments as evidence for the Earth's rotation?",
            "reference-answers": [
                "The main conclusion is that Foucault's pendulum experiments are unreliable evidence for the Earth's rotation due to inconsistencies, lack of connection to the Earth's motion, and alternative explanations for the pendulum's behavior."
            ]
        },
        {
            "question": "What is the period of revolution of the pendulum at Paris as mentioned in the text?",
            "reference-answers": [
                "The period of revolution of the pendulum at Paris is 32 hours."
            ]
        },
        {
            "question": "**Question:** How does the period of the pendulum’s revolution change with latitude, and what is the angular motion of the pendulum at a latitude of 30 degrees?",
            "reference-answers": [
                "The period of the pendulum’s revolution increases as latitude decreases. At 30 degrees latitude, the angular motion of the pendulum is 7½ degrees per hour."
            ]
        }
    ],
    "flat-earth-book/SECTION-13/text.txt": [
        {
            "question": "According to the text, why do lighthouses appear visible beyond the distance expected by the Earth's curvature, and what does this imply about the Earth's shape?",
            "reference-answers": [
                "Lighthouses appear visible beyond the distance expected by the Earth's curvature because the surface of the water is horizontal, and the line of sight is not obstructed by the supposed curvature of a globe. This implies that the Earth is a plane, as the observed visibility of lighthouses aligns with a flat Earth model rather than the spherical Earth doctrine."
            ]
        },
        {
            "question": "What does the author claim to demonstrate about the shape of the Earth based on the observations of lighthouses?",
            "reference-answers": [
                "The author claims to demonstrate that the Earth is flat by using the visibility of lighthouses beyond the distance expected on a curved (globe) Earth. Observations of lighthouses being seen farther than calculated, considering the supposed curvature of the Earth, are cited as evidence that the Earth must be a plane."
            ]
        },
        {
            "question": "What evidence is presented in the text to support the claim that the Earth is flat?",
            "reference-answers": [
                "The text presents several pieces of evidence to support the claim that the Earth is flat. First, it references observations of lighthouses, such as the Eddystone Lighthouse, which should be visible at certain distances according to Admiralty directions but were not seen despite clear weather. It also describes experiments with mirrors and telescopes, such as a vertical mirror reflecting the horizon parallel to a horizontal line, suggesting water surfaces are flat. Another experiment involved a telescope fixed on a pier observing a steamer, where the line of sight did not change as expected if the Earth were round. Additionally, the text calculates how much various lighthouses should be below the horizon based on Earth’s curvature but finds they are visible much farther, contradicting the idea of a globe. These observations and experiments are concluded to demonstrate that the Earth must be flat."
            ]
        }
    ],
    "flat-earth-book/SECTION-2/text.txt": [
        {
            "question": "Why does the Earth's supposed axial motion not cause a ball shot from an air gun to fall westward, based on the experiments described?",
            "reference-answers": [
                "The Earth's supposed axial motion does not cause a ball shot from an air gun to fall westward because, in the experiment, the ball consistently falls back near the gun or even into it, showing no westward displacement. If the Earth were moving eastward at over 700 feet per second, the ball should have landed significantly westward due to the Earth's motion during its flight. However, the observed result contradicts this, proving the Earth does not have axial motion."
            ]
        },
        {
            "question": "What conclusion does the author reach regarding the Earth's motion based on the experiments with dropping and throwing projectiles?",
            "reference-answers": [
                "The author concludes that the Earth does not move upon axes or in an orbit."
            ]
        },
        {
            "question": "What conclusion was drawn from the air gun experiment regarding the Earth's axial motion?",
            "reference-answers": [
                "The conclusion drawn from the air gun experiment was that the Earth does not have axial motion, as the ball fell back near the gun without the expected displacement, contradicting the hypothesis of the Earth's rotation."
            ]
        }
    ],
    "flat-earth-book/SECTION-3/text.txt": [
        {
            "question": "What conclusion can be drawn about the maximum distance of the visible objects in the firmament based on the described method?",
            "reference-answers": [
                "The maximum distance of the visible objects in the firmament, based on the described method, is not more than 6,000 miles."
            ]
        },
        {
            "question": "What is the maximum distance estimated for the Sun from Earth based on the observations and calculations described in the text?",
            "reference-answers": [
                "The maximum distance estimated for the Sun from Earth, based on the observations and calculations described, is under 4,000 miles."
            ]
        },
        {
            "question": "What does the text conclude about the size of the Sun, Moon, and stars compared to the Earth?",
            "reference-answers": [
                "The text concludes that the Sun, Moon, and stars are much smaller than the Earth, as their distances are measured to be less than 4,000 miles (for the Sun) and all visible objects in the firmament within 6,000 miles."
            ]
        }
    ],
    "flat-earth-book/SECTION-4/text.txt": [
        {
            "question": "What shape does the Sun appear to describe in the sky near the North Pole, as observed by Arctic navigators such as Captain Parry?",
            "reference-answers": [
                "The Sun appears to describe a circle in the sky near the North Pole."
            ]
        },
        {
            "question": "What evidence is provided in the text to support the claim that the Sun moves in a circular path over the Earth, and how does this relate to its motion before and after passing the meridian?",
            "reference-answers": [
                "The text provides evidence that the Sun moves in a circular path over the Earth by observing its motion as it describes an arc of a circle, particularly before and after passing the meridian. This is further supported by the observation of Arctic navigators, such as Captain Parry, who saw the Sun repeatedly describing a circle upon the southern horizon for 24 hours near the north pole. This motion confirms that the Sun completes a circular path, returning to the center of the arc it describes daily."
            ]
        },
        {
            "question": "What shape does the sun appear to move in when observed from northern latitudes near the North Pole, as described by Captain Parry and his officers?",
            "reference-answers": [
                "The sun appears to move in a circle when observed from northern latitudes near the North Pole, as described by Captain Parry and his officers."
            ]
        }
    ],
    "flat-earth-book/SECTION-5/text.txt": [
        {
            "question": "What is the Sun's altitude at the time of Southing on December 21st?",
            "reference-answers": [
                "The Sun's altitude at the time of Southing on December 21st is 12°.\n\nANSWER: 12°"
            ]
        },
        {
            "question": "When does the Sun’s path diameter reach its maximum?",
            "reference-answers": [
                "The Sun’s path diameter reaches its maximum on June 15th."
            ]
        },
        {
            "question": "What is the Sun's altitude at the time of southing on June 15th according to the table?",
            "reference-answers": [
                "The Sun's altitude at the time of southing on June 15th is 62°."
            ]
        }
    ],
    "flat-earth-book/SECTION-6/text.txt": [
        {
            "question": "**Question:**  \nWhy does the author argue that the Earth cannot be a globe based on the differences in day length and twilight between England and New Zealand?",
            "reference-answers": [
                "The author argues that the Earth cannot be a globe because the differences in day length and twilight between England and New Zealand cannot be explained by a spherical Earth model. If the Earth were a sphere, the day lengths and twilight characteristics at similar latitudes in both hemispheres should be the same. However, New Zealand experiences shorter days in summer and longer days in winter compared to England, with much shorter and more abrupt twilight. The author posits that a flat Earth, where the Sun's light travels faster over larger regions like New Zealand, accounts for these differences, making the Earth a plane rather than a globe."
            ]
        },
        {
            "question": "What reason is given in the text for the differences in day and night durations between New Zealand and England?",
            "reference-answers": [
                "The reason given is that if the Earth is flat, the sunlight travels over a larger circle (radius N W) for New Zealand compared to a smaller circle (radius N E) for England. Since the entire surface revolves underneath the Sun in the same time (24 hours), the velocity of the sunlight is greater over New Zealand, making the twilight more abrupt and the day durations shorter or longer accordingly, which contradicts the spherical Earth theory."
            ]
        },
        {
            "question": "What explanation does the text provide for the differences in day lengths between England and New Zealand, and how does this relate to the argument about the Earth's shape?",
            "reference-answers": [
                "The text explains that the differences in day lengths between England and New Zealand arise because, if the Earth were a flat plane, the Sun's light travels over a larger area for New Zealand due to its greater distance from the center. This causes the Sun's light to move faster, resulting in more abrupt twilight and shorter daylight hours in New Zealand compared to England. These differences are inconsistent with a spherical Earth, where day lengths at similar latitudes should be comparable. The flat Earth model accounts for the observed variations, supporting the argument that the Earth is not a globe."
            ]
        }
    ],
    "flat-earth-book/SECTION-7/text.txt": [
        {
            "question": "What is the reason for the apparent rising and setting of the Sun as explained in the text?",
            "reference-answers": [
                "The apparent rising and setting of the Sun are caused by the perspective effect due to the Earth's curvature and the parallelism of the Sun's path relative to the Earth's surface. As the Sun approaches the horizon, its light appears to ascend, reaching its highest point at noon when its apparent and actual positions coincide. After passing the meridian, it appears to descend and eventually set below the horizon. This phenomenon is analogous to how a flock of birds or a balloon appears to change altitude as they move away, due to the convergence of horizontal lines in the distance."
            ]
        },
        {
            "question": "What is the primary reason for the occurrence of sunrise and sunset as explained in the text?",
            "reference-answers": [
                "The primary reason for sunrise and sunset is the optical illusion caused by the Earth's curvature and the parallel nature of the Sun's path and the observer's line of sight. This makes the Sun appear to rise from and set below the horizon, even though it remains above the Earth's surface."
            ]
        },
        {
            "question": "Question: According to the text, why do sunrise and sunset occur?",
            "reference-answers": [
                "Sunrise and sunset occur because, due to perspective, the Sun, which is always above and parallel to the Earth's surface, appears to rise from and set below the horizon. This illusion happens as the Sun's path, being parallel to the Earth's surface, converges with the observer's line of sight towards the horizon, creating the appearance of the Sun ascending and descending during its daily journey."
            ]
        }
    ],
    "flat-earth-book/SECTION-8/text.txt": [
        {
            "question": "What is the reason for the apparent enlargement of the Sun when it is rising or setting, and how does this phenomenon relate to the Earth's atmosphere and visual perception?",
            "reference-answers": [
                "The apparent enlargement of the Sun when it rises or sets is due to its light passing through a greater thickness of the Earth's atmosphere, which is denser and more damp near the surface. This causes the light to be refracted and scattered, creating an optical illusion that makes the Sun appear larger. However, actual measurements confirm that the Sun's angular size remains constant. This perceived increase in size is due to the \"habit of sight,\" where the visual system misinterprets the lower position of the Sun as being larger, despite the angular dimensions remaining the same. Thus, the phenomenon is primarily an optical illusion related to atmospheric density and visual perception rather than a real change in size."
            ]
        },
        {
            "question": "What causes the Sun to appear larger when it is rising or setting compared to when it is on the meridian?",
            "reference-answers": [
                "The Sun appears larger when rising or setting because its light travels through a greater and denser portion of the atmosphere, which is more damp and contains more watery particles, causing the light to spread and dilate. Additionally, the apparent size is an optical illusion due to the habit of sight, where the horizon appears farther away, making the Sun seem larger, though actual measurements show its angular size remains constant."
            ]
        },
        {
            "question": "**Question:** Why does the sun appear larger when it is rising or setting than when it is on the meridian?",
            "reference-answers": [
                "The Sun appears larger when rising or setting due to the increased density and moisture of the atmosphere near the horizon, which causes the light to spread more, creating an optical illusion. Although the actual angular size remains the same, this atmospheric effect and visual perception make it seem larger."
            ]
        }
    ],
    "flat-earth-book/SECTION-9/text.txt": [
        {
            "question": "Question: What evidence is presented in the text to argue that the Moon is not a reflector of the Sun's light but might be self-luminous?",
            "reference-answers": [
                "The text presents evidence that the Moon is not merely a reflector of the Sun's light but might be self-luminous. Observations during lunar eclipses show the Moon remains visible and even appears to glow red, contrary to expectations if it were solely reflecting sunlight. Experiments concentrating the Moon's light failed to produce heat, unlike sunlight, suggesting a different nature. Additionally, the Moon's light does not affect certain chemical reactions, unlike sunlight, and its brightness increases in the Earth's shadow, implying self-luminosity."
            ]
        },
        {
            "question": "What evidence is provided in the text to explain why the Moon remained visible and illuminated during the total eclipse, challenging the traditional understanding of lunar eclipses?",
            "reference-answers": [
                "The text provides evidence that the Moon remained visible and illuminated during a total eclipse by describing its appearance as glowing, red, and hot, with visible surface features despite the eclipse. This challenges the traditional understanding as it suggests the Moon is self-luminous rather than merely reflecting sunlight. Experiments show that the Moon's light, when concentrated, does not raise temperatures, unlike the Sun's, indicating it emits its own light. Additionally, the text argues that if the Moon were a reflector, its light would behave like the Sun's, which it does not, further supporting the idea that the Earth's shadow cannot explain the phenomenon. Observations and experiments together suggest the Moon's luminosity is independent of the Sun."
            ]
        },
        {
            "question": "What evidence from the observations during the lunar eclipse challenges the traditional explanation that the Earth's shadow is responsible for the eclipse, and how does this evidence support an alternative theory about the moon's luminosity or another celestial body?",
            "reference-answers": [
                "The observations during the lunar eclipse provide significant evidence challenging the traditional explanation that the Earth's shadow causes the eclipse. During the eclipse, the Moon was still visible and appeared to glow with a reddish light, as described by witnesses like Mrs. Ward, who compared it to a \"red-hot penny,\" and Mr. Walkey, who noted it looked like it was on fire. This contradicts the expectation that the Earth's shadow would completely obscure the Moon, rendering it invisible.\n\nFurthermore, experiments showed that the Moon's light behaves differently from reflected sunlight. Concentrating the Moon's light using lenses did not raise the temperature, unlike the Sun's light, suggesting the Moon is not merely a reflector but may emit its own light. This supports the idea that the Moon is self-luminous, meaning an eclipse would not darken it but might instead cause its light to appear brighter in the shadowed regions.\n\nAdditionally, reports from astronomers, including the Royal Astronomical Society, suggest the possibility of another non-luminous body orbiting the Earth, which could cast a shadow during an eclipse. This alternative theory explains the visibility of the Moon during totality and challenges the traditional Earth-shadow explanation."
            ]
        }
    ],
    "nmt-book/chapter01-Introduction/text.txt": [
        {
            "question": "What year did neural machine translation systems first win in almost all language pairs at the Conference on Machine Translation (WMT) shared task?",
            "reference-answers": [
                "2016"
            ]
        },
        {
            "question": "How many pure neural machine translation systems were submitted to the WMT shared task in 2015, and how did this number change by 2017?",
            "reference-answers": [
                "In 2015, only one pure neural machine translation system was submitted to the WMT shared task. By 2017, almost all submissions were neural machine translation systems."
            ]
        },
        {
            "question": "How did the number and performance of neural machine translation systems submitted to the WMT competition change from 2015 to 2017?",
            "reference-answers": [
                "From 2015 to 2017, the number of neural machine translation systems submitted to the WMT competition increased significantly. In 2015, only one pure neural system was submitted, which was competitive but outperformed by traditional systems. By 2016, a neural system won in almost all language pairs, and in 2017, almost all submissions were neural machine translation systems, marking a clear shift in the dominance of neural approaches."
            ]
        }
    ],
    "nmt-book/chapter02-Neural-Networks/text.txt": [
        {
            "question": "What is the purpose of the error term associated with hidden nodes in a neural network during the training process?",
            "reference-answers": [
                "The purpose of the error term associated with hidden nodes in a neural network during the training process is to propagate the error backward through the network, enabling the computation of the necessary gradients for weight updates in the hidden layers. This allows the network to learn by adjusting the weights based on how the hidden nodes contribute to the overall error in the output nodes."
            ]
        },
        {
            "question": "What is the process for computing the derivative of the error with respect to the weights during back-propagation, and how does the chain rule apply to decompose this derivative?",
            "reference-answers": [
                "The process for computing the derivative of the error with respect to the weights during back-propagation involves applying the chain rule to decompose the derivative. This decomposition allows us to compute the gradient of the error with respect to each weight by considering the error terms in the subsequent layers and the activation values in the current layer. Specifically, the derivative for a weight is computed as the product of the error term from the next layer, the activation value of the connected node, and the derivative of the activation function. This approach ensures that we efficiently propagate the error backward through the network, enabling the computation of gradients for all weights in a systematic manner."
            ]
        },
        {
            "question": "What is the role of the chain rule in back-propagation, and how is the error term for a hidden node computed based on the error terms of subsequent nodes?",
            "reference-answers": [
                "The chain rule is fundamental in back-propagation as it allows us to decompose the computation of the gradient of the error with respect to the weights into smaller, manageable parts. This decomposition enables efficient computation of the gradients by propagating the error backward through the network. Specifically, the chain rule helps in computing the derivative of the error with respect to the weights of the hidden nodes by considering the contributions of the error terms from the subsequent nodes.\n\nThe error term for a hidden node is computed based on the error terms of the subsequent nodes it connects to. This is achieved by multiplying the error terms of the subsequent nodes by the weights connecting them to the hidden node and then summing these products. Additionally, the computation involves the derivative of the activation function applied to the weighted sum of the hidden node's inputs. This process ensures that the error term for a hidden node reflects its impact on the overall error through its connections to the subsequent nodes."
            ]
        }
    ],
    "nmt-book/chapter03-Computation-Graphs/text.txt": [
        {
            "question": "How do computation graphs facilitate the training of neural networks, and what role do deep learning frameworks play in automating gradient computations?",
            "reference-answers": [
                "Computation graphs represent neural networks as a series of operations between inputs, parameters, and mathematical functions, allowing for efficient processing and visualization of data flow. During training, these graphs facilitate the computation of gradients through a forward pass (computing outputs and error) and a backward pass (computing gradients for parameter updates). Deep learning frameworks automate this process by creating and compiling computation graphs, computing gradients using efficient algorithms, and optimizing computations, often leveraging GPU acceleration. This automation enables users to define and train complex neural networks with minimal code, focusing on model design rather than low-level implementation details."
            ]
        },
        {
            "question": "What is the primary purpose of the backward pass in the context of computation graphs for neural network training?",
            "reference-answers": [
                "The primary purpose of the backward pass in the context of computation graphs for neural network training is to compute the gradients of the error with respect to the model parameters, which are then used to update these parameters during training. This involves propagating the error backward through the computation graph, applying the chain rule to compute the derivatives for each node, and using these gradients to adjust the weights and biases, thereby enabling the model to learn by minimizing the error."
            ]
        },
        {
            "question": "What is the purpose of representing neural networks as computation graphs in the context of training, and how do these graphs facilitate the computation of gradients for model updates?",
            "reference-answers": [
                "Representing neural networks as computation graphs allows for efficient and automated computation of gradients needed for model training. These graphs break down the network into nodes (parameters, inputs, and operations) and edges (dependencies between operations). During training, the forward pass computes the model's output and error, while the backward pass computes gradients by propagating errors through the graph. This process is facilitated by frameworks like Theano or TensorFlow, which automatically compute derivatives using the chain rule, enabling efficient updates to model parameters. Thus, computation graphs simplify the implementation of gradient descent and make training neural networks more accessible, even for complex architectures."
            ]
        }
    ],
    "nmt-book/chapter04-Neural-Language-Models/text.txt": [
        {
            "question": "What is a key advantage of Long Short-Term Memory (LSTM) models over standard Recurrent Neural Networks (RNNs) in language modeling?",
            "reference-answers": [
                "A key advantage of Long Short-Term Memory (LSTM) models over standard Recurrent Neural Networks (RNNs) in language modeling is that they explicitly use memory cells with gates (input, forget, and output) to regulate the flow of information. This mechanism allows LSTM models to better handle long-term dependencies by selectively retaining or forgetting information, whereas standard RNNs lack such a clear mechanism and often struggle with vanishing gradients over long sequences."
            ]
        },
        {
            "question": "What is the purpose of gates in LSTM networks and how do they differ from those in GRU units?",
            "reference-answers": [
                "In LSTM networks, gates serve to control the flow of information through the cell. There are three types of gates: the input gate, which regulates how much new input affects the memory state; the forget gate, which controls how much of the prior memory is retained; and the output gate, which determines how strongly the memory is passed to the next layer. These gates allow the LSTM to explicitly manage what information is stored, forgotten, and output, providing a mechanism to handle long-term dependencies.\n\nIn contrast, GRU units use two gates: the update gate and the reset gate. The reset gate determines how much the previous state should be combined with the input, effectively deciding whether to focus on the current input or the previous state. The update gate controls how much of the previous state is updated with the new information. This simplification reduces the number of parameters compared to LSTM cells but still allows for effective memory management. While LSTM gates provide more explicit control over memory through separate functions for input, forgetting, and output, GRU gates combine these functions into fewer gates, potentially offering a more streamlined approach."
            ]
        },
        {
            "question": "What are the key limitations of recurrent neural networks (RNNs) in handling long-term dependencies, and how do LSTM and GRU models address these challenges?",
            "reference-answers": [
                "Recurrent neural networks (RNNs) face two key limitations in handling long-term dependencies. First, they suffer from the vanishing gradient problem, where gradients diminish as they propagate back through many time steps, making it difficult to learn dependencies over long sequences. Second, the single hidden state in RNNs serves both as memory and for processing inputs, which can lead to challenges in appropriately weighting recent versus older information.\n\nLong short-term memory (LSTM) networks address these issues by introducing an explicit memory cell with three gates: the input gate controls how much new input influences the memory, the forget gate regulates the retention or deletion of memory, and the output gate determines how strongly the memory is used for prediction. This structure allows LSTM to maintain and utilize long-term dependencies more effectively.\n\nGated recurrent units (GRU) offer a simpler alternative with fewer gates—combining the functions of updating the hidden state into two gates. The update gate decides how much of the past state to retain, while the reset gate determines the influence of the current input. GRU simplifies the architecture while still enabling the model to capture long-term dependencies."
            ]
        }
    ],
    "nmt-book/chapter05-Neural-Translation-Models/text.txt": [
        {
            "question": "What is the role of the attention mechanism in the neural machine translation model and how does it contribute to the decoder's context state?",
            "reference-answers": [
                "The attention mechanism in neural machine translation computes an association between the decoder's hidden state and each input word, producing attention values that indicate how relevant each input word is for the next output word. These values are normalized using softmax and used to weigh the input word representations, forming a context vector. This context vector serves as the decoder's context state, allowing it to consider all input words and capture long-range dependencies, thereby enhancing translation accuracy."
            ]
        },
        {
            "question": "What components are used in the attention mechanism to compute and normalize attention values in a neural machine translation model?",
            "reference-answers": [
                "The attention mechanism uses a feedforward layer with weight vectors U, V, and a bias value to compute attention scores, which are then normalized using the softmax function to form a probability distribution."
            ]
        },
        {
            "question": "What is the role of the attention mechanism in the decoder of a neural machine translation model?",
            "reference-answers": [
                "The attention mechanism in the decoder of a neural machine translation model computes an association between the decoder's hidden state and each input word. It generates a context state by weighting the input word representations based on their relevance to the current output word. This context is then used to inform the decoder's predictions, allowing it to focus on the most relevant parts of the input sequence during each time step. This mechanism enables the decoder to consider the entire input context when generating each output word, improving the alignment between the source and target languages."
            ]
        }
    ],
    "nmt-book/chapter06-Refinements/text.txt": [
        {
            "question": "What technique is described in the text for improving neural machine translation by incorporating linguistic information into the model?",
            "reference-answers": [
                "The text describes techniques for improving neural machine translation by incorporating linguistic information into the model. These include adding linguistic annotations such as part-of-speech tags, lemmas, morphological properties, syntactic structures, and semantic roles to input words, encoding these as factors in word embeddings. Additionally, output sentences can include syntactic structure tokens to promote grammatically correct output. Linguistically structured models, inspired by syntactic parsing, are also explored, using neural networks to handle language's recursive nature. These methods enhance the model's ability to utilize linguistic knowledge, improving translation quality, particularly when training data is limited."
            ]
        },
        {
            "question": "How do neural machine translation models benefit from the addition of linguistic annotations, and what are some examples of such annotations?",
            "reference-answers": [
                "Neural machine translation models benefit from the addition of linguistic annotations as they provide additional context and structured information that can help the model make more informed predictions. This enriched input can lead to improved translation quality, especially when the model is provided with pre-computed linguistic features that might otherwise require extensive training data to learn. Examples of such linguistic annotations include part-of-speech tags, lemmas, morphological properties, syntactic dependencies, and semantic roles. For instance, part-of-speech tags indicate the grammatical category of a word (e.g., noun, verb), while lemmas provide the base form of words. Morphological annotations might include features like singular or plural, and syntactic dependencies describe how words relate to each other in a sentence. These annotations can be encoded as factored representations and added to the input, allowing the neural model to utilize this structured information to enhance its predictions."
            ]
        },
        {
            "question": "What are the key approaches discussed in the text to improve neural machine translation systems, and how do they contribute to overcoming challenges in translation?",
            "reference-answers": [
                "The key approaches discussed in the text to improve neural machine translation systems include:\n\n1. **Domain Adaptation**: This involves adjusting the model for specific domains by mixing in-domain and out-of-domain data to prevent overfitting. Techniques like using ensembles of models, adding domain tokens, and encoding topic memberships as inputs are employed. These methods help improve performance in niche domains by guiding the model to focus on relevant information.\n\n2. **Linguistic Annotation**: Enhancing the model by adding linguistic information such as part-of-speech tags, lemmas, morphological properties, syntactic structures, and semantic annotations to the input and output. This enriches the context and helps the model understand the structure and meaning of words and sentences, particularly useful when data is limited.\n\n3. **Handling Multiple Language Pairs**: Training a single model on various corpora to translate between multiple languages. This approach reduces the need for separate models for each language pair and leverages shared components like encoders or attention mechanisms. Adding language tokens as input helps disambiguate, and deeper models may capture more abstract representations, enabling multi-language translation with less data required.\n\nThese approaches contribute to overcoming challenges by addressing domain-specific data scarcity, enhancing translation quality through structured linguistic insights, and efficiently utilizing diverse data to handle multiple languages."
            ]
        }
    ],
    "nmt-book/chapter07-Alternate-Architectures/text.txt": [
        {
            "question": "What is a key advantage of using self-attention over recurrent neural networks in neural machine translation?",
            "reference-answers": [
                "A key advantage of using self-attention over recurrent neural networks in neural machine translation is that self-attention allows the model to consider wide context and is highly parallelizable, whereas recurrent neural networks require a lengthy, sequential processing of each word, which is time-consuming and limits parallelization. Self-attention enables the model to capture the context of each word in a parallel manner, making it more efficient and capable of handling long-range dependencies."
            ]
        },
        {
            "question": "What is a key advantage of self-attention over recurrent neural networks in neural machine translation?",
            "reference-answers": [
                "Self-attention allows for parallel processing of the entire input sequence, overcoming the sequential limitations of recurrent neural networks, making it more efficient."
            ]
        },
        {
            "question": "What are the key architectural components discussed in the text for neural machine translation, and what are their advantages?",
            "reference-answers": [
                "The key architectural components discussed in the text for neural machine translation are:\n\n1. **Convolutional Neural Networks (CNNs):** These networks use convolutional layers to process input words in parallel, merging their representations into a single vector. The advantages include better parallelization and reduced computational time compared to recurrent neural networks. However, they struggle with long sentences due to the fixed window size of convolutions.\n\n2. **CNNs with Attention:** This architecture combines convolutional layers with an attention mechanism. The encoder uses multiple convolutional layers without shortening the sequence, allowing each layer to capture wider context. The decoder also uses convolutions instead of RNNs, maintaining parallelization benefits. The attention mechanism remains the same, providing context for each word. The advantage is the combination of computational efficiency with attention's context capabilities.\n\n3. **Self-Attention (Transformer):** This mechanism allows each word to attend to every other word in the input, capturing long-range dependencies without sequential processing. It uses multi-head attention, residual connections, and layer normalization for efficient training. The advantages include parallel processing, capturing wide context, and efficient training.\n\nThese components address computational efficiency, context handling, and long-range dependency capture, advancing neural machine translation models."
            ]
        }
    ],
    "nmt-book/chapter08-Current-Challenges/text.txt": [
        {
            "question": "How do statistical machine translation and neural machine translation systems compare in terms of performance across different domains, data needs, and robustness to noisy data?",
            "reference-answers": [
                "Statistical machine translation (SMT) and neural machine translation (NMT) systems exhibit different performance characteristics across domains, data needs, and robustness to noisy data:\n\n1. **Performance Across Domains**:  \n   - In-domain performance: SMT and NMT are comparable, with NMT excelling in IT and Subtitles domains, while SMT performs better in Law, Medical, and Koran domains.  \n   - Out-of-domain performance: NMT systems underperform significantly compared to SMT. For example, NMT achieves a BLEU score of 3.9 on the Law test set, while SMT scores 10.2.\n\n2. **Data Needs**:  \n   - NMT requires substantially more training data to achieve strong performance. It shows a steep learning curve, starting with poor results with small datasets (e.g., 1.6 BLEU score with 1 million words) but surpassing SMT with larger datasets (e.g., 31.1 BLEU with full data).  \n   - SMT performs well even with smaller amounts of data and shows consistent improvement with increasing data, indicating lower data requirements and a gentler learning curve.\n\n3. **Robustness to Noisy Data**:  \n   - SMT is robust to noisy data, with only a slight drop in performance (e.g., 0.7 BLEU points) when 50% of the training data is corrupted.  \n   - NMT is highly sensitive to noisy data, experiencing a significant performance degradation (e.g., a 5.3 BLEU point drop with 50% data perturbation), leading to hallucinated content and unreliable translations.\n\nIn summary, SMT is more reliable in out-of-domain scenarios, more robust to noise, and requires less data to achieve decent performance. NMT, while capable of surpassing SMT with sufficient data, struggles with generalization and robustness when data is limited or noisy."
            ]
        },
        {
            "question": "How do the data needs of statistical machine translation (SMT) and neural machine translation (NMT) compare according to the experiments described?",
            "reference-answers": [
                "Statistical machine translation (SMT) benefits from increasing amounts of training data, with a fixed improvement in BLEU scores when data is doubled, and it is robust to noisy data. In contrast, neural machine translation (NMT) exhibits a steeper learning curve, requiring significantly more training data to achieve good performance. While NMT can scale better with larger datasets, it struggles with smaller amounts of data and is more sensitive to noisy or corrupted training data."
            ]
        },
        {
            "question": "How do neural machine translation and statistical machine translation systems differ in their performance with respect to training data size, domain adaptability, and robustness to noise?",
            "reference-answers": [
                "Neural machine translation (NMT) and statistical machine translation (SMT) systems differ in their performance across three key aspects: training data size, domain adaptability, and robustness to noise.\n\n1. **Training Data Size**: NMT exhibits a steeper learning curve, meaning it requires more training data to achieve optimal performance. In experiments, NMT systems initially performed poorly with small datasets (e.g., 1 million words), whereas SMT systems showed better results even with limited data. However, as the amount of training data increased, NMT eventually outperformed SMT, particularly with full datasets. SMT, on the other hand, can achieve acceptable performance with smaller datasets, making it more efficient in scenarios with limited training data.\n\n2. **Domain Adaptability**: NMT systems show better in-domain performance for specific domains, such as IT and Subtitles, compared to SMT. However, NMT systems struggle significantly more with out-of-domain data, often producing unrelated or nonsensical translations when tested on domains different from those used in training. In contrast, SMT systems are more robust to out-of-domain data, maintaining relatively consistent performance across different domains.\n\n3. **Robustness to Noise**: SMT systems demonstrate greater resilience to noisy training data, such as misaligned sentences or corrupted content. Even when up to 50% of the training data was perturbed, SMT's performance only slightly degraded. In contrast, NMT systems are highly sensitive to noise, with a significant drop in BLEU scores when exposed to noisy data. This sensitivity can lead to hallucinations in the output, where the translations appear fluent but lack meaningful connection to the input.\n\nIn summary, while NMT offers advantages in leveraging larger datasets and can achieve superior performance with sufficient training, it is less adaptable to domain shifts and more susceptible to noise. SMT, though less data-intensive, is more versatile across domains and more robust to data corruption, making it a reliable choice in scenarios with limited or noisy training data."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen01-slide01/text.txt": [
        {
            "question": "What is the focus of the second lecture on statistical machine translation?",
            "reference-answers": [
                "The focus of the second lecture on statistical machine translation is to provide an overview of the approaches to statistical machine translation."
            ]
        },
        {
            "question": "What was the main focus of the second lecture on statistical machine translation?",
            "reference-answers": [
                "The main focus of the second lecture was an overview of the approaches to statistical machine translation."
            ]
        },
        {
            "question": "What will the upcoming lectures cover according to the second lecture on statistical machine translation?",
            "reference-answers": [
                "The upcoming lectures will elaborate on the details of the approaches to statistical machine translation discussed in this lecture."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen02-slide02/text.txt": [
        {
            "question": "What does SMT stand for, and how does it relate to NMT in the context of the talk?",
            "reference-answers": [
                "SMT stands for Statistical Machine Translation. In the context of the talk, SMT refers to the older statistical approaches to machine translation, while NMT stands for Neural Machine Translation, which is a newer, advanced form of statistical machine translation. Although NMT is part of SMT, conventionally, SMT is used to contrast with NMT, with SMT referring to non-neural statistical methods."
            ]
        },
        {
            "question": "Question: Is neural machine translation (NMT) considered a type of statistical machine translation (SMT), or are they considered separate approaches?",
            "reference-answers": [
                "Neural machine translation (NMT) is considered a type of statistical machine translation (SMT). While there can be terminological confusion, the convention is that SMT includes both older approaches and neural approaches, so NMT is encompassed within the broader category of SMT."
            ]
        },
        {
            "question": "What key aspect distinguishes statistical machine translation (SMT) from neural machine translation (NMT), considering both fall under statistical machine translation?",
            "reference-answers": [
                "The key aspect that distinguishes statistical machine translation (SMT) from neural machine translation (NMT), while both fall under statistical machine translation, is their approach. SMT traditionally relies on phrase-based translation, which is considered the older state-of-the-art method. In contrast, NMT uses neural architectures, making it the current state-of-the-art approach. Despite both being types of SMT, they are often contrasted, with SMT referring to non-neural approaches and NMT representing the neural methods."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen03-slide13/text.txt": [
        {
            "question": "What is the key distinction presented in the Wokoa Triangle and how does it influence the approach to machine translation?",
            "reference-answers": [
                "The key distinction presented in the Wokoa Triangle is between direct translation, where source text is translated to target text by processing characters directly, and the use of an interlingua, an abstract representation of the source text that allows for translation to multiple target languages. This distinction influences the approach to machine translation by highlighting that while the interlingua approach is more efficient for handling multiple languages (requiring N systems for analysis and N for generation), it involves a longer path with potential for error accumulation due to the complexity of developing systems for analysis, transfer, and synthesis. In contrast, direct translation, though requiring more systems (n-squared), is simpler and may be more effective for specific language pairs, especially before the advent of neural machine translation which changed the landscape."
            ]
        },
        {
            "question": "What is the main motivation behind the Wokoa Triangle model, and what challenges does it face in practical implementation?",
            "reference-answers": [
                "The main motivation behind the Wokoa Triangle model is to achieve efficiency and scalability in machine translation by reducing the number of systems required. Instead of creating n-squared systems for direct translation, the model uses an interlingua, requiring only N systems for analysis and N for generation, thus enabling translation between all pairs of languages. However, the challenges it faces include the practical difficulty of constructing a useful interlingua for domain-unconstrained translation, the complexity and potential error accumulation in the multi-step process of analysis, transfer, and synthesis, and the fact that direct translation may sometimes be more effective."
            ]
        },
        {
            "question": "What is the main trade-off discussed in the Wokoa Triangle regarding machine translation approaches?",
            "reference-answers": [
                "The main trade-off discussed in the Wokoa Triangle is between the efficiency of the interlingua approach, which requires N systems for analysis and N systems for generation, and the practicality and effectiveness of direct translation, which, despite requiring N squared systems, might perform better due to fewer accumulated errors in the transfer process."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen04-slide04/text.txt": [
        {
            "question": "What is the key difference between example-based and statistical machine translation in terms of how they handle translation units?",
            "reference-answers": [
                "The key difference is that example-based machine translation uses longer, human-translated units and modifies them, while statistical machine translation decomposes the text into smaller units and reconstructs the sentence from these smaller pieces."
            ]
        },
        {
            "question": "What is the main difference between example-based machine translation and statistical machine translation approaches?",
            "reference-answers": [
                "The main difference is that example-based machine translation uses the longest units from human translations and modifies them slightly, whereas statistical machine translation breaks down the text into smaller units and reconstructs the sentence from these smaller pieces."
            ]
        },
        {
            "question": "What is the main difference between example-based and statistical machine translation approaches?",
            "reference-answers": [
                "The main difference is that example-based machine translation uses the longest units from human translations and modifies them slightly, while statistical machine translation decomposes everything into much smaller units before reconstructing the sentence."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen05-slide05/text.txt": [
        {
            "question": "What was the relationship between engineers and linguists in the development of machine translation systems, as described in the text?",
            "reference-answers": [
                "Engineers focused on practical solutions and majority cases for high accuracy, while linguists were interested in exceptions and could distract engineers. Despite this, linguists provided valuable insights to prevent critical issues from being overlooked."
            ]
        },
        {
            "question": "What did Frederick Jelinek define machine translation (MT) as, and how did he approach it?",
            "reference-answers": [
                "Frederick Jelinek defined machine translation (MT) as linguistic modeling combined with statistical decision theory. He approached MT by focusing on the majority of cases to achieve high accuracy, often ignoring exceptions, and used equations to integrate linguistic and statistical methods."
            ]
        },
        {
            "question": "What were the key approaches and contributions of Warren Weaver, Noam Chomsky, and Frederick Jelinek to the field of machine translation?",
            "reference-answers": [
                "Warren Weaver introduced a statistical approach to machine translation, likening it to a deciphering problem. He used probabilities to model the translation process, treating the original message as hidden and the translated text as coded. This was a significant shift in thinking, as it moved away from earlier attempts to directly map languages.\n\nNoam Chomsky criticized the probabilistic approach, arguing that the notion of a \"probability of a sentence\" was ill-defined and useless. He believed that probabilities could not meaningfully capture the likelihood of sentences in a language, as it required context. Despite this critique, he acknowledged that probabilities could still be useful in practice, even without a solid theoretical foundation.\n\nFrederick Jelinek, a practical engineer, focused on the majority of cases rather than exceptions. He emphasized that while linguists might get distracted by exceptions, engineers prioritize broad coverage and high accuracy. His approach integrated linguistics with statistical models, viewing machine translation as a unified effort between linguistic modeling and statistical decision theory. This pragmatic perspective was crucial in developing practical machine translation systems, such as those at IBM."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen06-slide06/text.txt": [
        {
            "question": "What role do linguists play in the context of statistical machine translation models?",
            "reference-answers": [
                "Linguists play a role in statistical machine translation models by helping to break longer units, such as sentences, into smaller units and by identifying the important observations that the system should consider when determining probabilities."
            ]
        },
        {
            "question": "What role does linguistics play in the design of statistical machine translation models?",
            "reference-answers": [
                "Linguistics plays a crucial role in the design of statistical machine translation models by helping to break down longer language units, such as sentences, into smaller, more manageable units. Additionally, linguists identify the important observations that the system should focus on when determining which outputs are more probable. This contributes to the effective structuring and training of the probabilistic models used in machine translation."
            ]
        },
        {
            "question": "What aspect makes machine translation statistical, and how do linguists contribute to structuring the outputs through the distribution of probability mass?",
            "reference-answers": [
                "Machine translation is made statistical by relying on a probabilistic model that distributes probability mass across possible outputs. Linguists contribute by structuring these outputs through breaking down sentences into smaller units and identifying key observations that the system should consider, thereby influencing which translations are deemed more probable."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen07-slide07/text.txt": [
        {
            "question": "What is the significance of finding frequent minimum translation units in traditional statistical machine translation?",
            "reference-answers": [
                "Finding frequent minimum translation units in traditional statistical machine translation is significant because it allows for efficient reuse of translation information across multiple sentence pairs. These units are easy to process, as they minimize the need for reordering, and can be automatically discovered from large datasets, making the translation system more effective."
            ]
        },
        {
            "question": "What is the ultimate goal of traditional statistical machine translation, and what are the two main types of data used to achieve this?",
            "reference-answers": [
                "The ultimate goal of traditional statistical machine translation is to find minimum translation units that are frequent across many sentence pairs, allowing for reuse of information and easier processing without reordering. The two main types of data used are large collections of monolingual texts and smaller collections of parallel texts, where sentences have been translated."
            ]
        },
        {
            "question": "What were the key characteristics of the translation units sought by traditional statistical machine translation, and what types of data were primarily used to identify them?",
            "reference-answers": [
                "The key characteristics of the translation units sought by traditional statistical machine translation were that they were frequent across many sentence pairs, allowing for reuse of information, were easy to process without requiring reordering, and could be automatically discovered from the data. The primary data types used were large collections of monolingual texts and smaller collections of parallel texts, with optional use of automatic linguistic analysis to aid in identifying these units."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen08-slide08/text.txt": [
        {
            "question": "What is the key formula discussed in the text for statistical machine translation?",
            "reference-answers": [
                "The key formula discussed is the conditional probability of the target sentence given the source sentence, expressed as \\( P(E_1, E_2, \\dots, E_i | F_1, F_2, \\dots, F_j) \\)."
            ]
        },
        {
            "question": "What is the critical formula in statistical machine translation that everyone must remember and understand?",
            "reference-answers": [
                "The critical formula is the conditional probability of the target sentence given the source sentence, defined as \\( P(E_1, \\ldots, E_i \\mid F_1, \\ldots, F_j) \\). This formula represents the probability of generating the target sentence \\( E \\) given the source sentence \\( F \\), and it is this probability that needs to be maximized in statistical machine translation."
            ]
        },
        {
            "question": "How is the probability of a target sentence given a source sentence defined in the context of statistical machine translation?",
            "reference-answers": [
                "The probability of a target sentence given a source sentence in statistical machine translation is defined as \\( P(E|F) \\), where \\( E \\) is the target sentence and \\( F \\) is the source sentence. The task involves maximizing this probability to find the target sentence that is most likely given the source sentence. If multiple target sentences have the same maximum probability, any of those can be emitted."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen09-slide09/text.txt": [
        {
            "question": "What is the technical problem with setting translation probabilities to one if a pair has been seen and zero otherwise?",
            "reference-answers": [
                "The technical problem is that when a source sentence has multiple possible target translations, each translation pair is assigned a probability of one. This results in the sum of probabilities for all possible translations exceeding one, which is inconsistent with the axioms of probability."
            ]
        },
        {
            "question": "What is the technical problem that arises when a sentence in the brute force machine translation system has multiple translations, and how is it addressed in the described approach?",
            "reference-answers": [
                "The technical problem is that when a source sentence has multiple target translations, the probabilities assigned to each translation sum to more than one, which is not valid for a probability distribution."
            ]
        },
        {
            "question": "The text describes a technical issue with a brute force machine translation system that uses translation memories. The problem arises when a source sentence can be translated into multiple target sentences, causing the assigned probabilities for each translation to sum to more than one. This violates probability theory, which requires that the sum of probabilities for all possible outcomes should be exactly one. Therefore, the question is:\n\nWhat is the technical problem when a source sentence can be translated into multiple target sentences in a brute force machine translation system based on translation memories?",
            "reference-answers": [
                "The technical problem is that when a source sentence can be translated into multiple target sentences, the assigned probabilities for each translation can sum to more than one, which violates the requirement that the sum of probabilities for all possible outcomes should be exactly one."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen10-slide10/text.txt": [
        {
            "question": "What are the two main issues discussed in the text regarding probability distributions, and how are they addressed?",
            "reference-answers": [
                "The two main issues discussed are:  \n1. **Normalization**: The probabilities do not sum to one, which is addressed by normalizing the counts of sentence occurrences.  \n2. **Lack of smoothness**: The probability distribution is not smooth enough, leading to jumps in probability (e.g., from one to zero for slightly changed sentences), which causes issues with generalization.  \n\nThe text does not explicitly address how the second issue is resolved, only that it is a problem."
            ]
        },
        {
            "question": "What are the two main issues discussed in the text regarding the probability distribution of sentence pairs in the database?",
            "reference-answers": [
                "The two main issues are: 1) the need to normalize the probabilities so they sum to one, using sentence occurrence counts, and 2) the lack of smoothness in the probability distribution, which affects generalization and causes sudden drops in probability for even slightly altered sentence pairs."
            ]
        },
        {
            "question": "The probability distribution based only on sentence occurrences isn't smooth enough because it assigns high probabilities only to exact matches and zero probabilities to similar but unseen sentences, leading to a lack of generalization and abrupt changes in probability.",
            "reference-answers": [
                "The probability distribution based only on sentence occurrences isn't smooth enough because it assigns high probabilities only to exact matches and zero probabilities to similar but unseen sentences, leading to a lack of generalization and abrupt changes in probability."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen11-slide11/text.txt": [
        {
            "question": "Why is the probability of the source sentence ignored when using Bayes' law to find the highest scoring target sentence?",
            "reference-answers": [
                "The probability of the source sentence is ignored because it is a constant divisor in the calculation and does not affect the relative scoring among target sentences. When using Bayes' law within the argmax function, all target sentences are compared by the same constant (the source sentence probability), making it unnecessary to include it in the comparison for determining the highest score."
            ]
        },
        {
            "question": "Why can we ignore the probability of the source sentence when using Bayes' law to find the highest scoring target sentence in machine translation?",
            "reference-answers": [
                "We can ignore the probability of the source sentence because it is a constant divisor for all target sentences being considered. Since the argmax function only cares about the relative scores, dividing by the same constant doesn't affect which target sentence has the highest score, allowing us to omit it in the calculation."
            ]
        },
        {
            "question": "Which key statistical principle allows us to reverse the conditional probability in the formula for machine translation, and why can we disregard the probability of the source sentence when using the argmax function?",
            "reference-answers": [
                "The key statistical principle that allows us to reverse the conditional probability in the formula for machine translation is **Bayes' law**. We can disregard the probability of the source sentence when using the argmax function because all elements in the comparison are divided by the same constant (the probability of the source sentence), which does not affect the relative scores of the target sentences. Therefore, the argmax can find the highest scoring target sentence without actually dividing by this constant."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen12-slide12/text.txt": [
        {
            "question": "What is the main motivation for using a language model in the context of the noisy channel discussed in the text?",
            "reference-answers": [
                "The main motivation for using a language model in the context of the noisy channel is that it helps ensure the output is a grammatically correct sentence and provides a better estimation of probabilities using larger monolingual datasets, which aids in making the right translation decisions when uncertain."
            ]
        },
        {
            "question": "What is the main motivation for using a language model in the described approach, and how does it benefit the translation process?",
            "reference-answers": [
                "The main motivation for using a language model in the described approach is to determine whether the output is a grammatically correct sentence or a random sequence of symbols. This benefits the translation process by allowing components to be trained on different sets of data, particularly monolingual data which is easier to obtain in larger quantities. This results in a better estimation of probabilities, which helps in making more accurate translation decisions, especially when the parallel data is uncertain."
            ]
        },
        {
            "question": "What is the main motivation for using two tables in the noisy channel approach, and how does the language model assist in making translation decisions?",
            "reference-answers": [
                "The main motivation for using two tables in the noisy channel approach is to enhance translation accuracy by allowing separate training on different datasets. The language model, trained on abundant monolingual data, assists in making translation decisions by ensuring the output is a grammatically correct and sensible sentence. It helps choose the right translation when uncertain by providing context about the likelihood of sentences."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen13-slide13/text.txt": [
        {
            "question": "What are the two main components of the log-linear model, and how are they used to generate target language sentences?",
            "reference-answers": [
                "The two main components are the translation model (conditional probability derived from parallel text) and the language model (from monolingual text). They are used together to generate target language sentences by combining the translation and language probabilities to find the best matches."
            ]
        },
        {
            "question": "What are the two main models derived from parallel and monolingual text in the log-linear approach?",
            "reference-answers": [
                "The two main models are the translation model (derived from parallel text) and the language model (derived from monolingual text)."
            ]
        },
        {
            "question": "What are the two models derived from parallel and monolingual text in the log-linear model, and how are they used to generate target language sentences?",
            "reference-answers": [
                "The two models are the translation model (derived from parallel text) and the language model (derived from monolingual text). These models are used together to generate target language sentences by combining the translation model, which provides the conditional probability of translating a source sentence, and the language model, which ensures the target sentences are fluent and appropriate."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen14-slide14/text.txt": [
        {
            "question": "How is the probability of a sentence defined in an n-gram based language model?",
            "reference-answers": [
                "The probability of a sentence in an n-gram based language model is defined as the product of the probabilities of each n-gram that makes up the sentence."
            ]
        },
        {
            "question": "How does an n-gram language model calculate the probability of a given sentence?",
            "reference-answers": [
                "An n-gram language model calculates the probability of a given sentence by breaking it into smaller chunks called n-grams. For example, in a trigram model, the probability of the sentence is the product of the probabilities of each sequence of three words in the sentence, where each sequence predicts the next word based on the previous two. This way, the model decomposes the sentence into manageable units and calculates the overall probability by multiplying the probabilities of these smaller n-gram units."
            ]
        },
        {
            "question": "How is the probability of a sentence defined using n-grams in a simple language model?",
            "reference-answers": [
                "The probability of a sentence is defined by breaking it into smaller chunks (n-grams), and the probability of the entire sentence is the product of the probabilities of these smaller chunks. For example, in a trigram model, the probability is calculated as the product of the probability of the first two words, followed by the probability of each subsequent word given the previous two words, and so on."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen15-slide15/text.txt": [
        {
            "question": "How is the probability of a trigram calculated according to the text?",
            "reference-answers": [
                "The probability of a trigram is calculated by dividing the count of the trigram by the count of its antecedent (the preceding two words)."
            ]
        },
        {
            "question": "What is the formula used to determine the probability of a trigram in the described method?",
            "reference-answers": [
                "The probability of a trigram is calculated using the formula: P(w₁, w₂, w₃) = count(w₁ w₂ w₃) / count(w₁ w₂)"
            ]
        },
        {
            "question": "How is the probability of a trigram calculated according to the provided text?",
            "reference-answers": [
                "The probability of a trigram is calculated by dividing the count of the trigram by the number of occurrences of the antecedent of the history (the words at the beginning of the n-gram)."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen16-slide14/text.txt": [
        {
            "question": "Where does defining the probability of a sentence as a product of trigram probabilities struggle?",
            "reference-answers": [
                "Defining the probability of a sentence as a product of trigram probabilities struggles because the probability is not smooth enough, leading to a high risk that one of the trigrams will not be observed at all."
            ]
        },
        {
            "question": "What is a major issue with defining the probability of a sentence as a product of trigram probabilities?",
            "reference-answers": [
                "A major issue is that the probability isn't smooth enough, leading to a high risk that some trigrams won't be observed in the data."
            ]
        },
        {
            "question": "What is a significant issue when using trigram probabilities to calculate the probability of a sentence?",
            "reference-answers": [
                "A significant issue when using trigram probabilities to calculate the probability of a sentence is the risk of encountering an n-gram that is not observed in the training data, leading to a zero probability and causing problems in the model's calculations."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen17-slide15/text.txt": [
        {
            "question": "What is the difference between interpolation and backoff in the context of n-gram models, and under what circumstances might one be more effective than the other?",
            "reference-answers": [
                "In the context of n-gram models, interpolation refers to combining multiple n-gram models (e.g., trigram, bigram, unigram) with different weights to estimate probabilities. This means all models are considered simultaneously, and their contributions are averaged or weighted to calculate the final probability. On the other hand, backoff involves using the higher n-gram model (e.g., trigram) if it is known, and only resorting to lower n-gram models (e.g., bigram or unigram) if the higher n-gram is unknown or has a zero probability.\n\nInterpolation can sometimes be more effective when all n-grams are relatively rare or when the training data is limited, as it leverages information from all levels of n-grams. However, backoff might be more effective in situations where the higher n-gram models are more reliable, and the lower models are needed only as a fallback when the higher ones are insufficient, such as when dealing with unknown or rare n-grams."
            ]
        },
        {
            "question": "What is the main difference between backing off and interpolation when handling n-gram probabilities in the context of smoothing?",
            "reference-answers": [
                "The main difference is that backing off involves using shorter n-grams as a fallback when a higher n-gram is unknown, whereas interpolation uses all models simultaneously, blending their probabilities."
            ]
        },
        {
            "question": "Under what circumstances is interpolation generally considered better than backoff in the context of n-gram models?",
            "reference-answers": [
                "Interpolation is generally considered better than backoff in the context of n-gram models when using all models simultaneously provides a more accurate probability estimation, particularly if the data benefits from combining multiple n-gram levels rather than relying solely on a single lower n-gram when the higher one is unknown."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen18-slide16/text.txt": [
        {
            "question": "What empirical adjustment was made to the base law in the translation model, and what was the reason for this adjustment?",
            "reference-answers": [
                "The empirical adjustment made was squaring the probability of the language model. This was done because the language model, when trained on a much larger collection of data, became so effective that it was empirically better to trust it more than what the base law would recommend. Additionally, it was observed that swapping the directions (source to target or vice versa) in the training did not negatively impact the system's performance, suggesting that the direction might not matter much with large data."
            ]
        },
        {
            "question": "What were the two main observations made in the text regarding the translation model and language model, and what adjustments were empirically found to be effective despite violating the base law?",
            "reference-answers": [
                "The two main observations were that (1) a larger language model could be trusted more than the base law suggested, leading to squaring its probability, and (2) swapping the conditional direction in training did not harm performance, despite violating the base law. The adjustments empirically found to be effective were squaring the language model's probability and using the swapped conditional direction."
            ]
        },
        {
            "question": "What were the two main observations made by the student regarding the translation model, and what did these lead to?",
            "reference-answers": [
                "The two main observations were:\n1. Sometimes, the language model is so good that it's better to trust it more, leading to squaring its probability to give it more weight, which isn't possible under the base law.\n2. Swapping the direction of the conditional probability (from source given target to target given source) didn't affect performance, allowing the use of the swapped approach despite violating the base law.\n\nThese observations led to using a squared probability and allowing the order to be swapped in the translation model."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen19-slide17/text.txt": [
        {
            "question": "What is the key process used in LoglingualModal to compute the probability of a target sentence given a source sentence, and how is the result normalized to ensure it represents a probability distribution?",
            "reference-answers": [
                "The key process in LoglingualModal involves modeling the probability of a target sentence given a source sentence as a weighted combination of independent feature functions. These feature functions evaluate various aspects of the translation proposals. The result is normalized by dividing each sentence pair's score by the total sum of points distributed across all possible sentence pairs, ensuring it forms a valid probability distribution."
            ]
        },
        {
            "question": "How does LoglingerModal ensure that the scores it produces represent proper probabilities?",
            "reference-answers": [
                "LoglingerModal ensures that the scores represent proper probabilities by normalizing them. This involves summing all the points distributed among all possible input and output sentence pairs and then dividing each sentence pair's score by this total sum. This normalization step converts the scores into a probability distribution where the sum equals 1."
            ]
        },
        {
            "question": "What is the \"great thing\" mentioned in the text that makes the normalization process for the LoglingerModal framework more manageable?",
            "reference-answers": [
                "The great thing mentioned is that the framework automatically handles the normalization process, meaning you don't have to manually compute the sum of all points distributed across all possible sentence pairs, as the framework takes care of it for you."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen20-slide18/text.txt": [
        {
            "question": "Question: Why doesn't the summation need to be calculated when using the formula within the argmax function?",
            "reference-answers": [
                "The summation doesn't need to be calculated because it is identical across all sentences. Since the sum remains constant for every sentence, there's no need to compute it within the argmax function. This simplification allows us to focus on distributing points to define feature points without worrying about the summation."
            ]
        },
        {
            "question": "Why don't we need to calculate the summation of points when using the formula within the argmax?",
            "reference-answers": [
                "We don't need to calculate the summation of points when using the formula within the argmax because the summation of all the points distributed is identical across all sentences. Since this sum remains constant, we don't have to bother about summing it repeatedly. Therefore, we only need to focus on distributing points to define the feature points."
            ]
        },
        {
            "question": "Why don't we need to calculate the summation of all the points when using the argmax function?",
            "reference-answers": [
                "We don't need to calculate the summation of all the points when using the argmax function because the summation of all the points is identical across all sentences. Since this constant summation doesn't change, it doesn't affect the comparison within the argmax function. Therefore, we only need to focus on distributing points to define the feature points without worrying about the total sum."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen21-slide17/text.txt": [
        {
            "question": "What is the role of the weights in the log-linear model described in the text, and how is the language model incorporated as a feature function?",
            "reference-answers": [
                "In the log-linear model described, the weights play the role of specifying the relative importance of different components, allowing for the balancing of various aspects such as the language model and the translation model. The language model is incorporated as a feature function by creating a function that ignores the source side and focuses solely on the target side. This feature function uses the probability of n-grams to estimate the probability of the target candidate and is easily embedded into the log-linear framework by applying a log transformation to the calculation."
            ]
        },
        {
            "question": "Explain the role of the language model feature function in the log-linear model described in the text, including how it integrates n-gram probabilities.",
            "reference-answers": [
                "The language model feature function in the log-linear model focuses solely on the target side of the translation, disregarding the source. It uses n-gram probabilities to estimate the probability of the target candidate. To integrate these probabilities into the log-linear framework, a log is applied to the probability calculation, converting multiplicative probabilities into additive scores. This allows the original n-gram language model to be easily embedded within the log-linear model. The weights in the model determine the relative importance of this feature function, enabling a balance between the language model and other components, such as the translation model."
            ]
        },
        {
            "question": "What does the feature function in the log-linear model ignore when estimating the probability of the target candidate translation?",
            "reference-answers": [
                "The feature function in the log-linear model ignores the source when estimating the probability of the target candidate translation."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen22-slide18/text.txt": [
        {
            "question": "What is the purpose of the weights in the Logliner model?",
            "reference-answers": [
                "The purpose of the weights in the Logliner model is to indicate how important each feature function is, allowing the model to prioritize certain aspects when evaluating translations."
            ]
        },
        {
            "question": "**Question:**  \nWhy does the Logliner model not require normalization when scoring candidates' translations?",
            "reference-answers": [
                "The Logliner model does not require normalization because it focuses on identifying the highest-scoring candidate translation based on the weighted feature functions, without needing to convert the scores into probabilities."
            ]
        },
        {
            "question": "Why is normalization unnecessary in the Logliner model when searching for the best candidate?",
            "reference-answers": [
                "Normalization is unnecessary in the Logliner model when searching for the best candidate because we are only interested in identifying the candidate with the highest score. Since the model scores candidates based on feature functions with associated weights, the absolute values do not need to sum to one or represent probabilities. Instead, the relative scores are sufficient to determine the highest-scoring candidate, making normalization redundant."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen23-slide19/text.txt": [
        {
            "question": "What is the relationship between the Logdiner model and the noisy channel approach, as described in the text?",
            "reference-answers": [
                "The Logdiner model is a generalization of the noisy channel approach. The noisy channel approach is a special case of the Logdiner model when using specific feature functions and equal weights, leading to the product of probabilities. The Logdiner model allows for more flexibility, such as adjusting weights for different components, and in practice, both models were used together to improve results."
            ]
        },
        {
            "question": "Question: How does the Logdiner model generalize the noisy channel approach, and what are the two specific feature functions used in this generalization?",
            "reference-answers": [
                "The Logdiner model generalizes the noisy channel approach by using two specific feature functions with equal weights. These functions are:\n\n1. The log of the probability of the source given the target.\n2. The log of the probability of the target alone.\n\nWhen these functions are used in the formula, the sum of logs becomes the log of the product, which simplifies to the product of probabilities, making the noisy channel approach a special case. Additionally, using different weights (lambdas) allows for adjusting the influence of each model, and in practice, both models were used together for better results."
            ]
        },
        {
            "question": "What is the relationship between the Logdiner model and the noisy channel approach, and why was it beneficial to use both models simultaneously?",
            "reference-answers": [
                "The Logdiner model is a generalization of the noisy channel approach. The noisy channel approach is a special case of the Logdiner model, achieved by using specific feature functions and equal weights, which simplify back to the product of probabilities. Using both models simultaneously was beneficial because each model provided smoother results for different sentences based on the data, leading to better overall performance."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen24-slide20/text.txt": [
        {
            "question": "What is the purpose of the word penalty feature in phrase-based machine translation, and how does it interact with the language model's scoring?",
            "reference-answers": [
                "The word penalty feature in phrase-based machine translation is designed to control the preference for shorter or longer output sentences. It addresses the issue where the language model, which scores sentences based on n-gram frequencies, may unfairly favor longer sentences simply because they contain more words. By introducing a word penalty or bonus, the feature adjusts the scoring to counterbalance this bias. Specifically, it looks at the length of the source or target sentence and applies a penalty to longer sentences if needed, ensuring that the translation does not merely favor length over quality. This interaction balances the language model's inherent bias toward longer sentences, leading to more accurate and appropriate translations."
            ]
        },
        {
            "question": "What is the purpose of the word penalty feature in phrase-based machine translation?",
            "reference-answers": [
                "The purpose of the word penalty feature is to control whether shorter or longer outputs are preferred and to counterbalance the effect of the language model, which tends to give lower scores to longer sentences. It adjusts the scoring by considering the number of words in the source or target sentence, thereby ensuring a fair comparison between sentences of different lengths."
            ]
        },
        {
            "question": "What is the primary feature of phrase-based machine translation and how does it estimate the probability of translating a sentence?",
            "reference-answers": [
                "The primary feature of phrase-based machine translation is the phrase translation probability. This feature estimates the probability of translating a sentence by breaking it into smaller phrases. Each phrase is looked up in a phrase table, a dictionary of phrase pairs, to determine their frequency. The probabilities are then normalized to estimate the overall translation probability."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen25-slide21/text.txt": [
        {
            "question": "Why are language models not used to propose target sentences in phrase-based systems?",
            "reference-answers": [
                "Language models are not used to propose target sentences in phrase-based systems because they only observe word counts and do not deal with specific word relationships. They are primarily used for scoring, not for constructing candidate translations. Additionally, using a language model to propose sentences would be inefficient, as it might suggest words alphabetically or randomly, which could lead to poor translation quality."
            ]
        },
        {
            "question": "What is a limitation of using language models to propose target sentences in a machine translation system, and why are they primarily used for scoring instead?",
            "reference-answers": [
                "A limitation of using language models to propose target sentences is that they may generate nonsensical or irrelevant translations, as they don't consider the meaning or context of the source words. Language models are primarily used for scoring because they observe word counts rather than word meanings, making them ineffective for constructing meaningful candidate translations."
            ]
        },
        {
            "question": "What is used to construct candidate translations in a phrase-based machine translation system?",
            "reference-answers": [
                "Phrase translation probabilities are used to construct candidate translations in a phrase-based machine translation system."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen26-slide22/text.txt": [
        {
            "question": "What is the purpose of the \"tuning\" step in the traditional pipeline for training classical statistical machine translation systems?",
            "reference-answers": [
                "The purpose of the \"tuning\" step is to identify the weights (lambdas) in the model, which allows the system to score and combine translation units optimally for accurate translations."
            ]
        },
        {
            "question": "What is the process called after extracting translation units and language module data, and what does it involve in the traditional pipeline for training classical statistical machine translation systems?",
            "reference-answers": [
                "The process is called tuning, and it involves training the model to identify the weights (lambdas) for the feature functions, which are then used to score the best translations and combinations of translation units."
            ]
        },
        {
            "question": "What are the main steps in the traditional pipeline for training classical statistical machine translation systems, as outlined in the text?",
            "reference-answers": [
                "1. Find and collect parallel texts.  \n2. Align the texts at the sentence level.  \n3. Align the texts at the word level.  \n4. Extract translation units (phrases or syntactic units) based on the model's module.  \n5. Extract language model units from the target side only.  \n6. Perform tuning, which is the actual training to identify the weights (lambdas) of the model.  \n7. Create tables for the feature functions and use the identified weights to score and combine the translation units."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen27-slide23/text.txt": [
        {
            "question": "What type of system is being trained using the provided corpus?",
            "reference-answers": [
                "The type of system being trained using the provided corpus is a machine translation system."
            ]
        },
        {
            "question": "What is the content of the training corpus used to train the machine translation system?",
            "reference-answers": [
                "The training corpus includes sentences like \"Nemám žádného psa\" (\"I have no dog\") and \"Viděl kočku\" (\"He saw a cat\")."
            ]
        },
        {
            "question": "How many sentences are in the training corpus described in the text?",
            "reference-answers": [
                "The training corpus consists of two sentences: \"Nemám žádného psa, I have no dog.\" and \"Viděl kočku, he saw a cat.\" Therefore, there are 2 sentences in the training corpus.\n\nANSWER: 2"
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen28-slide24/text.txt": [
        {
            "question": "What is Psa and what is Kočku based on the context provided?",
            "reference-answers": [
                "Psa is a dog and Kočku is a cat."
            ]
        },
        {
            "question": "What does \"Psa\" translate to based on the context provided?",
            "reference-answers": [
                "dog"
            ]
        },
        {
            "question": "What does \"Psa\" and \"Kočku\" translate to in English according to the text?",
            "reference-answers": [
                "\"Psa\" translates to \"dog\" and \"Kočku\" translates to \"cat\" in English."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen29-slide25/text.txt": [
        {
            "question": "What does the word \"Kočku\" translate to in the given context?",
            "reference-answers": [
                "Kočku translates to ØKET."
            ]
        },
        {
            "question": "What does Kočku translate to based on the provided information?",
            "reference-answers": [
                "ØKET"
            ]
        },
        {
            "question": "What does the word \"Videl\" translate to based on the provided information?",
            "reference-answers": [
                "**ØSAU**"
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen30-slide26/text.txt": [
        {
            "question": "What is the translation of the sentence \"Nemám kočku\"?",
            "reference-answers": [
                "I don't have a cat."
            ]
        },
        {
            "question": "What is the translation of \"Nemám kočku\" from Czech to English?",
            "reference-answers": [
                "I don't have a cat."
            ]
        },
        {
            "question": "Does the speaker have a cat?",
            "reference-answers": [
                "No"
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen31-slide27/text.txt": [
        {
            "question": "What does the speaker mention about having a cat and their ability to translate?",
            "reference-answers": [
                "The speaker mentions that they don't have a cat and that they know how to translate."
            ]
        },
        {
            "question": "What does the statement mean when it says, \"which actually means I don't have a cat and you know how to translate ...\"?",
            "reference-answers": [
                "The statement means that there is a phrase or sentence which, when translated, reveals that the speaker does not have a cat and that the listener knows how to translate."
            ]
        },
        {
            "question": "What does the text translate to regarding the speaker's possession of a cat?",
            "reference-answers": [
                "The speaker does not have a cat."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen32-slide28/text.txt": [
        {
            "question": "What is mentioned about Nemaam in the text?",
            "reference-answers": [
                "Nemaam is mentioned as being there."
            ]
        },
        {
            "question": "Who is mentioned as being here in the text?",
            "reference-answers": [
                "Nemaam"
            ]
        },
        {
            "question": "Question: Who is mentioned as being present in the text?",
            "reference-answers": [
                "Nemaam"
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen33-slide29/text.txt": [
        {
            "question": "What is the main problem highlighted in the example with the phrase-based system?",
            "reference-answers": [
                "The main problem highlighted is the lack of or wrong independence in phrase-based systems."
            ]
        },
        {
            "question": "What is the main problem with phrase-based systems as illustrated by the example of translating \"I have a cat\" to \"nemám kočku\"?",
            "reference-answers": [
                "The main problem with phrase-based systems, as illustrated by the example, is the lack of or wrong independence, which leads to errors like dropping the negation in the translation of \"I have a cat\" to \"nemám kočku.\""
            ]
        },
        {
            "question": "What is the main issue highlighted by the example where the phrase-based system incorrectly translated \"nemám kočku\" as \"I have a cat\"?",
            "reference-answers": [
                "The main issue highlighted is that phrase-based systems lack or have wrong independence, which causes them to incorrectly drop the negation in translations, as seen in the example where \"nemám kočku\" was translated as \"I have a cat\" instead of \"I don't have a cat.\""
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen34-slide31/text.txt": [
        {
            "question": "What was the main problem with the approach described in the text, and how did it affect the translation process?",
            "reference-answers": [
                "The main problem with the approach was the wrong independence assumptions, specifically the assumption that phrases could be translated independently of their surroundings. This led to translating each phrase in isolation, which caused inaccuracies because the translation ignored the context of the source language and how phrases interact with each other."
            ]
        },
        {
            "question": "What was the primary issue with the translation approach described in the text?",
            "reference-answers": [
                "The primary issue was wrong independence assumptions, specifically translating phrases without considering their surroundings, relying solely on a target language model that doesn't account for the source context."
            ]
        },
        {
            "question": "What was the incorrect assumption made in the translation approach that led to inaccuracies?",
            "reference-answers": [
                "The incorrect assumption was that phrases could be translated independently of their surrounding context in the source language, leading to inaccuracies because the target language model, which assembles the translated pieces, does not consider the source context."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen35-slide30/text.txt": [
        {
            "question": "What is the language module's preference regarding output length and negations?",
            "reference-answers": [
                "The language module prefers shorter outputs and tends to avoid using negations."
            ]
        },
        {
            "question": "What is the reason the language module prefers to drop the negation in longer outputs?",
            "reference-answers": [
                "The language module prefers to drop the negation in longer outputs because longer outputs are generally scored lower."
            ]
        },
        {
            "question": "What does the text suggest about the candidate's happiness when they have a cat compared to not having one?",
            "reference-answers": [
                "The text suggests that the candidate is happier when they have a cat compared to when they do not have one."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen36-slide31/text.txt": [
        {
            "question": "What are the two strong independence assumptions about the phrases in the language model?",
            "reference-answers": [
                "The text does not provide specific details about the two strong independence assumptions about the phrases in the language model. Therefore, I cannot answer the question based on the information given."
            ]
        },
        {
            "question": "What are the two strong independence assumptions about the phrases in the language model?",
            "reference-answers": [
                "The two strong independence assumptions about the phrases in the language model are not explicitly stated in the provided text."
            ]
        },
        {
            "question": "What are the two strong independence assumptions about the phrases in the language model?",
            "reference-answers": [
                "The two strong independence assumptions about the phrases in the language model are not explicitly stated in the provided text."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen37-slide30/text.txt": [
        {
            "question": "What issue arises when trying to balance precision and recall by adjusting the size of translation units in statistical machine translation?",
            "reference-answers": [
                "When adjusting the size of translation units in statistical machine translation, a key issue arises: there is a trade-off between precision and recall. Larger units improve precision by capturing more context but reduce recall due to data sparsity, as the training corpus may lack sufficient varied data. Smaller units enhance recall but at the cost of precision. Attempts to balance this trade-off, such as introducing alignments, often fail, as maintaining both precision and recall becomes challenging without one negatively impacting the other."
            ]
        },
        {
            "question": "What trade-off is described in the text when using longer units for language processing, and what challenge did the researcher face when attempting to balance precision and recall?",
            "reference-answers": [
                "The trade-off described is between precision and recall: using longer units improves precision but reduces recall due to data sparseness. The researcher faced the challenge of finding a reasonable setting to balance these two, as any attempt to increase precision significantly decreased recall, making it impossible to achieve a satisfactory balance."
            ]
        },
        {
            "question": "What is the trade-off between precision and recall when adjusting the size of units in language processing, as discussed in the text?",
            "reference-answers": [
                "The trade-off involves balancing precision and recall. Larger units improve precision by reducing errors but risk data sparsity, lowering recall. Smaller units enhance recall by covering more data but may reduce precision. The author faced difficulty in balancing both without significant loss in either."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen38-slide31/text.txt": [
        {
            "question": "What are the two main problems identified with the formula used in the noisy channel approach, as described in the text?",
            "reference-answers": [
                "The two main problems identified are:  \n1. The separation of the language model from the translation model causes the language model to prefer shorter sentences, particularly those without negation, and disregard the source context.  \n2. The decomposition into phrases allows the system to propose incorrect outputs."
            ]
        },
        {
            "question": "What are the two main problems with the formula discussed in the text, and how do they affect the translation process?",
            "reference-answers": [
                "The two main problems with the formula discussed in the text are:\n\n1. **Separation of the language model from the translation model**: This causes the language model to prefer shorter sentences, often neglecting sentences with negation and ignoring the source context, leading to potential mistranslations or loss of meaning.\n\n2. **Decomposition into phrases**: This decomposition allows the system to propose incorrect outputs by breaking the text into smaller units that may not capture the full meaning or context, leading to flawed translations.\n\nThese issues significantly affect the translation process by causing inaccuracies, ignoring source context, and proposing incorrect outputs."
            ]
        },
        {
            "question": "What is the main issue identified with the base law approach in the translation model as described?",
            "reference-answers": [
                "The main issue with the base law approach in the translation model is that it separates the language model from the translation model, causing the language model to prefer shorter sentences and ignore the source, which negatively impacts translation quality."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen39-slide32/text.txt": [
        {
            "question": "What approach is used in current neural machine translation models to redefine the probability of the target sentence given the source?",
            "reference-answers": [
                "The approach used in current neural machine translation models to redefine the probability of the target sentence given the source is the language model decomposition. This involves treating the target sentence as a sequence of words generated one at a time, conditioned on the source sentence."
            ]
        },
        {
            "question": "What does the text suggest about redefining the probability of the target sentence in neural machine translation?",
            "reference-answers": [
                "The text suggests that the probability of the target sentence in neural machine translation should be redefined as a sequential generation process. Instead of the base decomposition, which assumes independence, the approach involves treating the target sentence as a sequence of words produced one at a time, conditioned on the source sentence. This method, used in contemporary neural machine translation models, effectively models the target sentence as a language model conditioned on the source, allowing for sensible predictions of each word in sequence."
            ]
        },
        {
            "question": "What approach is used in neural machine translation to redefine the probability of generating the target sentence given the source?",
            "reference-answers": [
                "The approach used in neural machine translation to redefine the probability of generating the target sentence given the source is the language model decomposition. This involves sequentially generating the target sentence one word at a time, conditioning each decision on the source and the previously generated words. This method is used in modern neural machine translation models, effectively making them language models conditioned on the source."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen40-slide33/text.txt": [
        {
            "question": "What term refers to neural networks' ability to approximate any function with one hidden layer given enough training data?",
            "reference-answers": [
                "The term that refers to neural networks' ability to approximate any function with one hidden layer given enough training data is **universal approximators**."
            ]
        },
        {
            "question": "What is the capability of a neural network with just one hidden layer in terms of function approximation?",
            "reference-answers": [
                "A neural network with just one hidden layer can approximate an arbitrary function to arbitrary precision, given enough training data."
            ]
        },
        {
            "question": "What is the capability of a neural network with a single hidden layer in terms of function approximation?",
            "reference-answers": [
                "A neural network with a single hidden layer is a universal approximator, capable of approximating any arbitrary function to arbitrary precision. However, training it might be impossible despite this capability."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen41-slide34/text.txt": [
        {
            "question": "How does the neural network determine whether a point is in the center or on the circumference of the circle?",
            "reference-answers": [
                "The neural network uses three hidden neurons, each representing a line that separates points. These lines are positioned based on weights, which are learned from the training data. When the network processes a point, each hidden neuron determines which side of its line the point is on. The final neuron checks if all hidden neurons correctly identified the point's side. If all are correct, the point is in the center; if any are incorrect, it's on the circumference."
            ]
        },
        {
            "question": "How does the neural network determine if a point is in the center of a circle or on its circumference?",
            "reference-answers": [
                "The neural network determines if a point is in the center or on the circumference by using three hidden neurons. Each neuron separates points on one side of a line defined by its weights. During training, these neurons adjust their lines to form a triangle that approximates the center area. The output neuron checks if all hidden neurons activated correctly (inside the triangle), indicating the center, or if any didn't (outside, circumference)."
            ]
        },
        {
            "question": "The neural network determines whether a point is in the center or on the circumference by using three hidden neurons, each acting as a line separator. These neurons' weights define their lines, which are adjusted during training. The combination of these lines forms a triangular area. If all three hidden neurons activate correctly (indicating the point is within the triangle), the output neuron classifies it as the center. If any neuron activates incorrectly, the point is classified as on the circumference.",
            "reference-answers": [
                "The neural network uses three hidden neurons, each acting as a line separator. These neurons have weights that define the position and slope of their lines. During training, these weights are adjusted so that the lines form a triangular area. The output neuron then checks if all three hidden neurons activate correctly (indicating the point is within the triangle). If they do, the point is classified as being in the center. If any neuron activates incorrectly, the point is classified as being on the circumference."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen42-slide35/text.txt": [
        {
            "question": "What happens to the color when the equation holds?",
            "reference-answers": [
                "The color becomes blue when the equation holds."
            ]
        },
        {
            "question": "Will the color be blue if the equation holds?",
            "reference-answers": [
                "Yes, the color will be blue if the equation holds."
            ]
        },
        {
            "question": "What condition must be met for the color to be blue according to the program?",
            "reference-answers": [
                "The color will be blue if the equation holds."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen43-slide36/text.txt": [
        {
            "question": "How many real numbers are needed to separate points in the center from points on a circular arrangement, and why is thirteen considered sufficient?",
            "reference-answers": [
                "To separate points in the center from points on a circular arrangement, thirteen real numbers are sufficient because the system's architecture, which includes weights for positions and slopes of lines, a non-linear function, and fine neuron weights, allows it to train automatically with these thirteen parameters."
            ]
        },
        {
            "question": "How many real numbers are required to separate points in the center from points on the circular using the described neural network architecture?",
            "reference-answers": [
                "Thirteen real numbers are required."
            ]
        },
        {
            "question": "How many real numbers are required for the system to separate points in the center from points on the circular?",
            "reference-answers": [
                "The system requires thirteen real numbers to separate points in the center from those on the circular."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen44-slide37/text.txt": [
        {
            "question": "Why is a hidden layer unnecessary when using perfect coordinates and squared inputs to determine if a point is inside or on a circle?",
            "reference-answers": [
                "When using perfect coordinates and squared inputs (such as x1 squared and x2 squared), the formula for determining if a point lies inside or on a circle (x1 squared + x2 squared - 1) can be calculated directly. Since the inputs are already squared, no hidden layer is needed because the necessary transformation is already applied, allowing the output layer to perform the exact calculation without additional processing."
            ]
        },
        {
            "question": "Question: Why is a hidden layer unnecessary when using perfect coordinates like x1 squared and x2 squared for determining a circle?",
            "reference-answers": [
                "A hidden layer is unnecessary when using perfect coordinates like x1 squared and x2 squared because the formula for a circle (x1 squared + x2 squared - 1 < 0) can be directly calculated using these inputs without any additional processing. The exact calculation distinguishes points inside from those on the circumference, eliminating the need for a hidden layer."
            ]
        },
        {
            "question": "What is the reason that a hidden layer is not required when using perfect inputs for the circle equation?",
            "reference-answers": [
                "The reason a hidden layer is not required when using perfect inputs for the circle equation is that the formula for the circle, \\( x^2 + y^2 = 1 \\) (or \\( x_1^2 + x_2^2 - 1 < 0 \\)), can be directly calculated using perfect coordinates or inputs, eliminating the need for any hidden layer."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen45-slide38/text.txt": [
        {
            "question": "Why can neural networks not directly linearly separate the center from the circumference if the coordinates are unfortunate?",
            "reference-answers": [
                "Neural networks cannot directly linearly separate the center from the circumference if the coordinates are unfortunate because, with such coordinates, there is no way to linearly combine them to achieve separation. Neural networks rely on linear separation, but if the coordinates are not suitable, they cannot perform this task directly."
            ]
        },
        {
            "question": "What is a key capability of neural networks when dealing with features that cannot be linearly separated?",
            "reference-answers": [
                "A key capability of neural networks is their ability to transform or change the features in such a way that allows linear separation, even when the original features cannot be linearly separated."
            ]
        },
        {
            "question": "What method does the text refer to as the \"lucky case\" that allows linear separation by changing the coordinates?",
            "reference-answers": [
                "The \"lucky case\" refers to the method where the data is transformed into a higher-dimensional space, enabling linear separation, typically through the use of the kernel trick."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen46-slide39/text.txt": [
        {
            "question": "What happens when a neural network has too many free parameters and insufficient training data?",
            "reference-answers": [
                "When a neural network has too many free parameters and insufficient training data, the network becomes too complex, leading to too many numbers to determine. With not enough data, the learning algorithm can't find the optimal positions for the decision boundaries, causing the network to fail in properly separating the data and thus never training effectively. This situation is considered a bad design of the network."
            ]
        },
        {
            "question": "What issue arises when a neural network has too many free parameters and insufficient training data?",
            "reference-answers": [
                "When a neural network has too many free parameters and insufficient training data, the learning algorithm is unable to find the optimal positions for the parameters, leading to undertraining. This results in a poorly designed network that fails to separate the data properly."
            ]
        },
        {
            "question": "What happens when a neural network is too complex, has too many parameters, and not enough training data?",
            "reference-answers": [
                "When a neural network is too complex, has too many parameters, and not enough training data, the network fails to train properly because the learning algorithm cannot find the optimal positions of the lines to separate the data. This results in a poorly designed network that is unable to achieve proper separation."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen47-slide40/text.txt": [
        {
            "question": "How does the network's capacity allow it to approximate shapes like circles using lines, as described in the text?",
            "reference-answers": [
                "The network's capacity allows it to approximate shapes like circles using lines by having multiple hidden layers. The first hidden layer creates separation boundaries (lines) based on coordinates like X, Y, and color. These lines can form simple features. With more neurons, the network can use more lines to create more complex features, such as approximating a circle with three or more lines. The network automatically determines the optimal number of lines needed for approximation during training, as it adjusts its parameters to best represent the shapes."
            ]
        },
        {
            "question": "How does the number of neurons in the first hidden layer affect the network's ability to approximate shapes, like circles, using linear boundaries?",
            "reference-answers": [
                "The number of neurons in the first hidden layer determines the network's capacity to approximate shapes like circles. More neurons allow the network to create more separation boundaries (lines), enabling it to combine these boundaries into more complex features. For example, with more neurons, the network can decide whether to approximate a circle with three or more lines, increasing its ability to model intricate shapes."
            ]
        },
        {
            "question": "What is the role of the number of neurons in the hidden layers when approximating shapes like circles?",
            "reference-answers": [
                "The number of neurons in the hidden layers determines the network's capacity to approximate shapes like circles by allowing it to use more lines for approximation. Having more neurons provides flexibility, enabling the network to decide whether to approximate a shape with three or more lines, depending on what is optimal for the classification task."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen48-slide41/text.txt": [
        {
            "question": "How is a sentence represented as a matrix in neural network translation according to the text?",
            "reference-answers": [
                "The sentence is represented as a matrix where each word is a column containing a single 1, indicating its position in the dictionary, with the rest being zeros. The matrix is tall and narrow, with each column corresponding to a word in the sentence, highlighting the sequence of words."
            ]
        },
        {
            "question": "How is a sentence represented as a matrix in neural network text translation according to the text?",
            "reference-answers": [
                "A sentence is represented as a matrix where each word is a column. Each column is a one-hot vector, with a single '1' indicating the position of the word in the dictionary, and all other entries are '0'. The matrix is tall and narrow, with each column corresponding to a word in the sentence."
            ]
        },
        {
            "question": "How are words represented as vectors in the described neural network model?",
            "reference-answers": [
                "In the described neural network model, words are represented as binary vectors (vectors of zeros and ones) where the length of each vector corresponds to the size of the vocabulary (dictionary). Each word is assigned a unique position in this vector space, with a single '1' at the position corresponding to the word and '0's elsewhere. For example, the sentence \"The cat is on the mat\" is represented as a matrix where each column contains a single '1' indicating the position of each word in the sequence."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen49-slide42/text.txt": [
        {
            "question": "What is the main issue with the matrix mentioned in the text?",
            "reference-answers": [
                "The main issue with the matrix mentioned in the text is its size, as it contains 1,300,000 elements for English and 2,000,000 elements for Czech, making it too large for current machines to handle reasonably quickly."
            ]
        },
        {
            "question": "Why is the matrix too large to handle quickly, and what are the number of elements for English and Czech?",
            "reference-answers": [
                "The matrix is too large to handle quickly because it contains 1,300,000 elements for English and 2,000,000 elements for Czech, which current machines cannot process reasonably fast due to the sheer size of the data."
            ]
        },
        {
            "question": "What issue arises with the matrix due to its size?",
            "reference-answers": [
                "The issue that arises with the matrix due to its size is that it becomes too large for current machines to handle quickly, with English requiring 1,300,000 elements and Czech requiring 2,000,000 elements."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen50-slide43/text.txt": [
        {
            "question": "What is the main issue with one-hot representation of words in terms of capturing relationships between them?",
            "reference-answers": [
                "The main issue with one-hot representation of words is that it fails to capture the relationships between words, treating them as equally distinct regardless of their actual meaning or similarity. This leads to a lack of context and nuanced understanding of how words relate to each other."
            ]
        },
        {
            "question": "What is a key limitation of using one-hot representation for word embeddings as discussed in the text?",
            "reference-answers": [
                "A key limitation of using one-hot representation for word embeddings is that it fails to capture the semantic or contextual relationships between words, treating each word as entirely separate without considering how similar or related they are to each other."
            ]
        },
        {
            "question": "What key issue does the text identify with one-hot word representation regarding word relationships?",
            "reference-answers": [
                "The key issue is that one-hot representation fails to capture the relationships or semantic similarities between words, treating each word as equally distinct without considering their contextual or meaningful connections."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen51-slide44/text.txt": [
        {
            "question": "What is the solution presented in the text, and how does it work using word embeddings in neural machine translation systems?",
            "reference-answers": [
                "The solution presented is the use of word embeddings in neural machine translation systems. Word embeddings map long word vectors into shorter, dense vectors (300-2,000 dimensions), serving as new features describing words. These embeddings are the first layer in a neural network, converting words into deeper representations. In neural MT, embeddings are automatically learned to best serve the translation task, capturing relationships like synonyms and handling ambiguity based on the target language. Pre-trained embeddings from language modeling tasks can also be used, though they are not tied to translation."
            ]
        },
        {
            "question": "What is the primary purpose of word embeddings in neural systems, as described in the text?",
            "reference-answers": [
                "The primary purpose of word embeddings in neural systems is to map long word vectors into much shorter, more manageable dimensions, capturing meaningful features that help in processing words efficiently for specific tasks."
            ]
        },
        {
            "question": "Question: How are word embeddings generated, and how do they adapt to reflect relationships between words in different languages during specific tasks?",
            "reference-answers": [
                "Word embeddings are generated by mapping long word vectors into much shorter ones, typically ranging from 300 to 2,000 dimensions. These dimensions serve as new features that describe the words. They can be pre-trained using various language modeling tasks or learned during the specific task being performed. When used in tasks like neural machine translation, the embeddings adapt by being trained to reflect relationships between words in different languages. For instance, if synonyms in the source language correspond to synonyms in the target language, they may end up with the same embeddings. Additionally, if words are ambiguous, the embeddings capture this based on the target language's context, allowing the system to distinguish and handle such cases effectively."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen52-slide45/text.txt": [
        {
            "question": "What is the solution proposed in the text to handle the problem of productive morphology in languages with infinite sets of correct words?",
            "reference-answers": [
                "The solution proposed is to break down the sentence into smaller subword units, such as syllables, morphemes, or individual characters, instead of translating entire words. This approach allows handling the infinite set of correct words by decomposing less frequent or longer words into smaller units that are already present in the dictionary, optimizing the translation process."
            ]
        },
        {
            "question": "**Question:**  \nWhy is subword tokenization considered a better approach than full-word tokenization in neural machine translation?",
            "reference-answers": [
                "Subword tokenization is considered a better approach than full-word tokenization in neural machine translation because it addresses the issue of productive morphology in languages where the number of possible words is effectively infinite. By breaking words into smaller subword units (such as syllables, morphemes, or characters), neural machine translation systems can handle unknown or rarely seen words by decomposing them into smaller, manageable units that are already part of the model's training data. This approach allows the model to generalize better and scale more effectively, as it reduces the dependency on having a finite dictionary of known words. Additionally, subword tokenization can capture meaningful segments of words that may not be present in the training data, making it particularly useful for languages with complex morphology."
            ]
        },
        {
            "question": "What problem arises in machine translation due to languages having productive morphology?",
            "reference-answers": [
                "The problem is that languages with productive morphology can create an infinite number of words, exceeding the finite capacity of neural machine translation systems, which can typically handle only 30,000 to 80,000 words."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen53-slide46/text.txt": [
        {
            "question": "What does the text mention about having variable?",
            "reference-answers": [
                "The text mentions that you have variable length."
            ]
        },
        {
            "question": "What does \"variable length\" refer to in the context of the provided information?",
            "reference-answers": [
                "\"Variable length\" refers to varying lengths in the context of the exam, such as questions or sections having different lengths."
            ]
        },
        {
            "question": "What does the text mention about variable length?",
            "reference-answers": [
                "It mentions variable length."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen54-slide45/text.txt": [
        {
            "question": "How does the system resolve the ambiguity in the source text when decomposing words, using the examples provided?",
            "reference-answers": [
                "The system resolves ambiguity by using the context of the sentence and the surrounding words. When a decomposed word like \"granti\" appears alone, it is translated as \"grants.\" However, when it appears with a joining mark and another word like \"mi,\" it recognizes the connection and translates it as \"migrants.\" Thus, the system employs context to disambiguate and determine the correct meaning of decomposed words."
            ]
        },
        {
            "question": "What is the main problem with the source text mentioned in the passage and how does the system use context to resolve it?",
            "reference-answers": [
                "The main problem with the source text is ambiguity, where words can have multiple meanings or decompositions. The system resolves this by using context, such as the surrounding words and sentence structure, to disambiguate and choose the correct meaning."
            ]
        },
        {
            "question": "What is the primary method used by the system to resolve word ambiguity in the source text?",
            "reference-answers": [
                "The system uses context to disambiguate words by analyzing the surrounding words and sentence structure."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen55-slide46/text.txt": [
        {
            "question": "What is the main challenge when training neural networks with variable-length inputs, and what solution is suggested in the text?",
            "reference-answers": [
                "The main challenge is the problem of vanishing gradients when training neural networks with variable-length inputs, as the network becomes much deeper and the same parameters are trained in multiple steps, leading to a long path from inputs to outputs. The suggested solution is to use particular neural network units to avoid this issue."
            ]
        },
        {
            "question": "What is the method used to process variable-length inputs in neural networks and what challenge does this pose during training?",
            "reference-answers": [
                "To process variable-length inputs in neural networks, the method involves using the same trained transformation matrix repeatedly on each input unit one at a time, effectively digesting the input step by step. This approach allows the network to handle inputs of varying lengths by processing each part individually. However, during training, this method poses the challenge of creating a much deeper network, which results in a longer path from inputs to outputs. This increased depth can lead to issues with vanishing gradients, making it difficult for the network to learn effectively during backpropagation."
            ]
        },
        {
            "question": "What is the primary challenge encountered when training a neural network with variable length inputs, as described in the text?",
            "reference-answers": [
                "The primary challenge is the problem of vanishing gradients due to the long path from inputs to outputs when training a neural network with variable length inputs."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen56-slide47/text.txt": [
        {
            "question": "What was the key innovation introduced by Cho in the phrase-based system?",
            "reference-answers": [
                "The key innovation introduced by Cho was the use of the encoder-decoder architecture to score phrases in the phrase-based system, which improved the translation process."
            ]
        },
        {
            "question": "What was the main innovation introduced by Kyun Kun Cho in the context of phrase-based systems using neural networks?",
            "reference-answers": [
                "The main innovation introduced by Kyun Kun Cho was the use of the encoder-decoder architecture to score phrases in phrase-based systems, enhancing the translation process."
            ]
        },
        {
            "question": "What numerical issue was addressed by the encoder-decoder architecture in neural machine translation?",
            "reference-answers": [
                "The numerical issue addressed was the challenge of handling varying phrase lengths in scoring, which the encoder-decoder architecture resolved by providing a more efficient and standardized approach."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen57-slide48/text.txt": [
        {
            "question": "What approach did the second team use when processing full sentences?",
            "reference-answers": [
                "The second team used an encoder-decoder approach where the entire sentence was fed into an encoder to produce a single representation, which was then decoded."
            ]
        },
        {
            "question": "What approach did the team use to represent a full sentence after processing it through an encoder?",
            "reference-answers": [
                "The team used an encoder that processes the entire sentence sequentially, converting each word into a dense embedding and combining these embeddings into a single vector representation of the sentence."
            ]
        },
        {
            "question": "How did the team process full sentences in their approach, and what were the key components involved?",
            "reference-answers": [
                "The team processed full sentences by feeding the entire input sentence into an encoder. Each word was represented as a one-hot vector, converted into a dense embedding, and these embeddings were used to form a sentence state. A matrix mixed these states with the current word at each step, resulting in a final sentence representation. Key components included the encoder, one-hot to dense embedding conversion, and the mixing matrix."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen58-slide49/text.txt": [
        {
            "question": "What is the role of the decoder in the neural network described, and how does it generate the target words one at a time?",
            "reference-answers": [
                "The decoder is a language model that generates target words one at a time. It starts with the vector representation of the source sentence and, at each step, uses its previous state and the words produced so far to predict the next word. It outputs a distribution over possible target words, selects the highest scoring one, and continues this process to produce the entire target sentence step by step."
            ]
        },
        {
            "question": "What is the role of the decoder in machine translation, as described in the text?",
            "reference-answers": [
                "The decoder's role is to act as a language model that generates the target sentence one word at a time. It uses the vector representation of the source sentence and the words produced so far to propose the next word. It provides a distribution over possible target words, selects the highest scoring one, and then uses this information to continue generating the target sentence step by step."
            ]
        },
        {
            "question": "What is the role of the decoder in the described neural network model?",
            "reference-answers": [
                "The decoder's role is to generate the target sentence by predicting each word one at a time, using the source sentence's vector representation and the context from previously produced words. It functions as a language model that steps through each word in sequence, considering the previous state and the words it has already generated."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen59-slide50/text.txt": [
        {
            "question": "How many parameters are mentioned in the network during the training process?",
            "reference-answers": [
                "50 million parameters."
            ]
        },
        {
            "question": "How long does it take to train the model on 50 million sentence pairs?",
            "reference-answers": [
                "It takes three weeks to train the model on 50 million sentence pairs."
            ]
        },
        {
            "question": "**Question:**  \nWhat are the key components of the described encoder-decoder architecture, and how does the training process work in this setting?",
            "reference-answers": [
                "The key components of the encoder-decoder architecture include the encoder, which processes the input sentence into a vector, and the decoder, which generates the target sentence from this vector. The training process involves initializing the model with random weights, making predictions, comparing them to the target, and adjusting weights through backpropagation. This process is repeated over millions of sentence pairs, gradually improving the model's performance."
            ]
        }
    ],
    "nmt-class/lecture02-smt-pbmt-nmt/screen60-slide51/text.txt": [
        {
            "question": "What is the primary goal of classical statistical machine translation, and how does it differ from the goal of neural machine translation?",
            "reference-answers": [
                "The primary goal of classical statistical machine translation is to find minimum translation units, which can be graph partitions, to operate on words or linguistic sentence representations. These units should be reusable across many sentences and ideally discovered unsupervised. The goal of neural machine translation, however, is to construct a neural network that directly translates input words into output words without relying on predefined translation units, allowing the network to learn all transformations independently."
            ]
        },
        {
            "question": "What is the key difference between classical statistical machine translation (CSMT) and neural machine translation (NMT) in terms of their approaches to translation?",
            "reference-answers": [
                "The key difference lies in their approaches: classical statistical machine translation (CSMT) relies on finding minimum translation units (like words or linguistic representations) that operate consistently across sentence pairs, decomposing the sentence into these units for translation. In contrast, neural machine translation (NMT) uses a neural network to directly learn transformations between input and output sequences without predefined translation units, balancing complexity, data size, and computation time to avoid overfitting and training challenges."
            ]
        },
        {
            "question": "What are the main differences between classical statistical machine translation and neural machine translation, and how do they approach the task of translation?",
            "reference-answers": [
                "Classical statistical machine translation (SMT) aims to find minimum translation units, which are graph partitions that can operate on words or linguistic sentence representations. These units should be consistent across many sentence pairs and ideally extracted unsupervised from data. The translation process involves decomposing sentences into these units and recombining their translations. On the other hand, neural machine translation (NMT) avoids predefined translation units and instead uses a neural network to directly process input words and generate output word sequences. NMT learns all necessary transformations internally but requires balancing model complexity, the size of training data, and computation time to avoid convergence issues and numerical problems. While classical SMT relies on decomposing sentences into pre-defined units, NMT focuses on end-to-end learning of the translation process."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen01-slide01/text.txt": [
        {
            "question": "What architectural component will be covered in the next lectures after discussing the sequence-to-sequence model with attention?",
            "reference-answers": [
                "transformer"
            ]
        },
        {
            "question": "What two main components are discussed in the lecture, and what future topic is mentioned as part of the series?",
            "reference-answers": [
                "The two main components discussed in the lecture are the basic sequence-to-sequence architecture and attention. The future topic mentioned as part of the series is the transformer."
            ]
        },
        {
            "question": "What is not the current state of the art of NeuralMT, as mentioned in the lecture?",
            "reference-answers": [
                "The basic sequence to sequence architecture with attention is not the current state of the art of NeuralMT."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen02-slide02/text.txt": [
        {
            "question": "In which year was the tutorial mentioned in the text presented?",
            "reference-answers": [
                "The tutorial mentioned in the text was presented three years ago."
            ]
        },
        {
            "question": "What are the five main topics covered in today's exam preparation session based on the provided text?",
            "reference-answers": [
                "The five main topics covered are:  \n1. Basic building blocks of neural networks  \n2. Processing text in neural networks  \n3. A new language model  \n4. Vanilla sequence to sequence architecture  \n5. Attention"
            ]
        },
        {
            "question": "What are the key concepts covered in today's exam preparation?",
            "reference-answers": [
                "The key concepts covered in today's exam preparation are:\n\n1. Basic building blocks of neural networks\n2. Processing text in neural networks\n3. New language model\n4. Vanilla sequence-to-sequence architecture\n5. Attention"
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen03-slide03/text.txt": [
        {
            "question": "What architecture are we building?",
            "reference-answers": [
                "The architecture we are building is the encoder-decoder architecture.\n\nANSWER: Encoder decoder architecture"
            ]
        },
        {
            "question": "What are we building?",
            "reference-answers": [
                "We are building the basic encoder decoder architecture."
            ]
        },
        {
            "question": "What architecture are we going to build based on the provided text?",
            "reference-answers": [
                "The architecture we are going to build is the basic encoder decoder architecture."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen04-slide04/text.txt": [
        {
            "question": "What is the process of constructing neural networks, starting from the beginning?",
            "reference-answers": [
                "The process of constructing neural networks starts from the very beginning, but the provided text does not offer further details. More information would be needed to explain the steps involved."
            ]
        },
        {
            "question": "What does the text say about how neural networks are constructed?",
            "reference-answers": [
                "The text indicates that the teacher is beginning an explanation about how neural networks are constructed, starting from the very beginning. However, the specific details about the construction are not provided in the text."
            ]
        },
        {
            "question": "What does the text suggest as the starting point for constructing neural networks?",
            "reference-answers": [
                "The starting point for constructing neural networks is the very beginning, which refers to the foundational aspects of how neural networks are constructed."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen05-slide03/text.txt": [
        {
            "question": "What does the lecture suggest about the relationship between the architecture, the mathematics involved, and linguistics in understanding sentence meaning?",
            "reference-answers": [
                "The lecture suggests that while the architecture and mathematics involved handle the meaning of sentences automatically through parallel data, there is no direct link between these elements and linguistics."
            ]
        },
        {
            "question": "Is there a direct link between the architecture, mathematics, or linguistics aspects and the meaning of sentences in the described system?",
            "reference-answers": [
                "No, there is no direct link between the architecture, mathematics, or linguistics aspects and the meaning of sentences in the described system. The system handles the meaning of sentences automatically by observing parallel data, and there is no direct connection between the architecture, the math, and any of the linguistics involved."
            ]
        },
        {
            "question": "What is the relationship between the architecture, the math, and linguistics, as discussed in the lecture?",
            "reference-answers": [
                "The lecture suggests that there is no direct link between the architecture, the math underneath, and linguistics. The system handles the meaning of the sentence automatically by observing parallel data, without requiring a direct connection to linguistic elements."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen06-slide05/text.txt": [
        {
            "question": "What are the key steps in the computation of a single fully connected layer in neural networks, as described in the text?",
            "reference-answers": [
                "The key steps in the computation of a single fully connected layer are:\n\n1. Multiply the input vector by the weight matrix.\n2. Add the bias vector to the result.\n3. Apply the non-linearity function element-wise."
            ]
        },
        {
            "question": "What are the three main components involved in the computation of a single fully connected layer in a neural network, as described in the text?",
            "reference-answers": [
                "The three main components are the input vector, the weight matrix, and the bias vector."
            ]
        },
        {
            "question": "What are the key components of a fully connected layer in neural networks as described in the text?",
            "reference-answers": [
                "The key components of a fully connected layer in neural networks as described in the text are:\n\n1. **Weight Matrix**: This matrix determines how to combine the input features.\n2. **Bias Vector**: This vector is added after the input features are weighted, acting as a basic activation level.\n3. **Non-Linearity Function (f)**: This function is applied element-wise to introduce non-linearity to the output vector."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen07-slide06/text.txt": [
        {
            "question": "What type of transformation occurs at one layer in a neural network as described in the text, and how does it involve the weight matrix, bias vector, and non-linearity?",
            "reference-answers": [
                "The transformation at one layer in a neural network involves adjusting the coordinate system through the multiplication of a weight matrix (W) with the input vector, which alters the space. The bias vector then shifts or transposes this coordinate system slightly. Finally, a non-linearity, such as the tanh activation function, squashes the infinite coordinate space into a finite range (e.g., between -1 and +1), with the exact shape of this transformation controlled by the weight matrix and bias vector."
            ]
        },
        {
            "question": "What is the main purpose of the transformations described in the text?",
            "reference-answers": [
                "The main purpose of the transformations described in the text is to adjust the coordinate system and squash the infinite canvas of coordinates into a finite space, enabling linear classification of the data."
            ]
        },
        {
            "question": "**Question:**  \nWhat type of transformation allows us to squash the infinite canvas of coordinates into a finite space, and how is the exact shape of this squashing controlled?",
            "reference-answers": [
                "The type of transformation that allows us to squash the infinite canvas of coordinates into a finite space is non-linearity. The exact shape of this squashing is controlled by the values in the weight matrix and the bias vector."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen08-slide07/text.txt": [
        {
            "question": "What role do weight matrices play in the structure of neural networks?",
            "reference-answers": [
                "Weight matrices play a crucial role in transforming the input data as it passes through each layer of the neural network. They allow the network to change the size of the vector, which is essential for learning and representing data at different levels of abstraction in the network."
            ]
        },
        {
            "question": "What does the weight matrix allow you to change in a neural network?",
            "reference-answers": [
                "The weight matrix allows you to change the size of the vector."
            ]
        },
        {
            "question": "What role do weight matrices play in allowing the network to change the size of the vector?",
            "reference-answers": [
                "Weight matrices play a crucial role in allowing the network to change the size of the vector by enabling dimension changes through matrix multiplication. The dimensions of the weight matrix determine the input and output vector sizes, effectively allowing the network to adjust the vector's dimensions as needed, which is essential for defining the structure of the network with multiple layers."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen09-slide05/text.txt": [
        {
            "question": "What method was used to scale the inner representation from two coordinates to three coordinates?",
            "reference-answers": [
                "The weight matrix was used to scale the inner representation from two coordinates to three coordinates."
            ]
        },
        {
            "question": "The weight matrix scaled the inner representation from two coordinates to three coordinates.",
            "reference-answers": [
                "The weight matrix scaled the inner representation from two coordinates to three coordinates."
            ]
        },
        {
            "question": "What effect does the weight matrix have on the inner representation in terms of coordinates?",
            "reference-answers": [
                "The weight matrix scales the inner representation from two coordinates to three coordinates."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen10-slide07/text.txt": [
        {
            "question": "What is the purpose of the backpropagation algorithm in training a neural network?",
            "reference-answers": [
                "The purpose of the backpropagation algorithm is to adjust the weights in the neural network so that the output gets closer to the expected output by calculating the derivatives of the loss function with respect to the network's parameters."
            ]
        },
        {
            "question": "What is the basic structure of a neural network and how does the training process adjust its weights to minimize the error between the network's output and the expected output?",
            "reference-answers": [
                "The basic structure of a neural network consists of a series of computation layers, where each layer involves matrices and non-linear functions. The network takes an input vector, processes it through these layers, and produces an output vector. During training, the network's output is compared to the expected output using a loss function, which measures the error. The backpropagation algorithm calculates the derivatives of the loss function with respect to the network's weights, indicating how each weight should be adjusted to minimize the error. Starting with random weights, the training process iteratively updates these weights based on the computed gradients, gradually improving the network's ability to produce outputs that match the expected results."
            ]
        },
        {
            "question": "What is the purpose of the backpropagation algorithm in the context of training a neural network?",
            "reference-answers": [
                "The purpose of the backpropagation algorithm is to adjust the weights in the neural network by computing the gradients of the loss function with respect to these weights, enabling the network to minimize the error between its predicted output and the expected output."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen100-slide83/text.txt": [
        {
            "question": "Why were the results of the two systems considered incomparable?",
            "reference-answers": [
                "The results of the two systems were considered incomparable because the size of the network, the training regime, and other settings were significantly different, even though the training and test data might have been the same."
            ]
        },
        {
            "question": "Why were the results of the two systems incomparable?",
            "reference-answers": [
                "The results of the two systems were incomparable because the size of the network and the training regime were very different, even though the training and test data might have been the same."
            ]
        },
        {
            "question": "Why were the results of the two systems considered incomparable?",
            "reference-answers": [
                "The results of the two systems were considered incomparable because the size of the network, the training regime, and other settings were significantly different, even though the training and test data might have been the same."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen101-slide84/text.txt": [
        {
            "question": "What is the main point made in the text about comparing the attention model and the encoder-decoder architecture?",
            "reference-answers": [
                "The main point is that the attention model is significantly better than the encoder-decoder architecture, and this comparison should be done in setups where the two are comparable."
            ]
        },
        {
            "question": "What is the key takeaway about comparing attention models to encoder-decoder architectures?",
            "reference-answers": [
                "The key takeaway is to always compare comparable setups when evaluating different architectures like attention models and encoder-decoder to ensure meaningful and accurate comparisons."
            ]
        },
        {
            "question": "What does the speaker suggest about comparing setups when evaluating different architectures?",
            "reference-answers": [
                "The speaker suggests that when evaluating different architectures, one should always compare setups that are comparable to each other."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen11-slide08/text.txt": [
        {
            "question": "What is the fundamental purpose of the four-layer neural network described in the text?",
            "reference-answers": [
                "The fundamental purpose of the four-layer neural network is to learn a function that can classify points on a coordinate plane into two colors (red or blue) by determining the appropriate decision boundary through training, allowing it to generalize and interpolate between known training examples."
            ]
        },
        {
            "question": "How does the four-layer neural network classify points on the x-y plane into red and blue categories?",
            "reference-answers": [
                "The four-layer neural network classifies points on the x-y plane by training itself to separate red and blue points. It starts with random weights and gradually updates them to transform the input space, squashing, pulling, and tearing the coordinates until a linear boundary forms. This allows the network to distinguish between red and blue points. The network's ability to interpolate enables it to accurately classify test points near the training examples, even if those points weren't explicitly part of the training data."
            ]
        },
        {
            "question": "How does a four-layer neural network with random initialization learn to separate two sets of points on an x-y plane into red and blue categories, and what role does the training process play in this separation?",
            "reference-answers": [
                "A four-layer neural network with random initialization begins training unable to clearly separate the red and blue points on the x-y plane. As the training progresses, the network adjusts its weights through iterative updates, effectively transforming the input space. These transformations squish, pull, and twist the data, making it increasingly separable. The network learns to interpolate, classifying points near the training examples accurately, even if those points weren't explicitly in the training set. The training process is crucial, as it enables the network to adjust its internal representations, ultimately allowing it to draw a line that linearly separates the red and blue points."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen12-slide09/text.txt": [
        {
            "question": "What is the basic approach used by recurrent neural networks to handle variable-length inputs in natural language processing?",
            "reference-answers": [
                "The basic approach used by recurrent neural networks (RNNs) to handle variable-length inputs in natural language processing involves using a state that is gradually updated. The network starts with an initial state, such as zero, and combines the current input with the current state using a transformation function. This function is applied repeatedly for each input element, allowing the network to process variable-length sequences. To handle different input lengths, the network is unrolled over time, creating a fixed-size structure. Inputs shorter than the fixed maximum length are padded with zeros, which do not affect the training process, enabling computational efficiency. This method allows RNNs to effectively manage sequences of varying lengths."
            ]
        },
        {
            "question": "What approach do recurrent neural networks use to handle variable-length input sequences, and how are the computations structured to accommodate this?",
            "reference-answers": [
                "Recurrent neural networks (RNNs) handle variable-length input sequences by maintaining a state that is updated over time. The network processes each input step-by-step, combining the current input with the previous state using a transformation function. Computationally, this is achieved by unrolling the network over time, creating a fixed structure that can handle inputs up to a maximum length. If the input is shorter than this maximum, the remaining positions are typically padded with zeros, which do not affect the training. This approach allows RNNs to efficiently process sequences of varying lengths."
            ]
        },
        {
            "question": "What is the basic approach used to handle variable-length inputs in natural language processing according to the text?",
            "reference-answers": [
                "The basic approach used to handle variable-length inputs in natural language processing is to use recurrent neural networks. These networks have a state that is gradually updated by combining the current input with the current state using a transformation function. This function is applied multiple times, once for each input value, allowing the network to process variable-length inputs. The computation is unrolled over time, meaning the network structure is repeated as needed to handle the input length, and padding (often zeros) is used to fill unused space during training."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen13-slide10/text.txt": [
        {
            "question": "What is the main problem with applying the non-linear transformation to the entire state in a vanilla recurrent neural network?",
            "reference-answers": [
                "The main problem with applying the non-linear transformation to the entire state in a vanilla recurrent neural network is that it causes the state representations to change drastically at each time step, as all elements of the state go through the non-linear transformation. This results in each state living in different vector spaces, leading to a lack of continuity in the representations, which makes training difficult and the learned representations hard to interpret."
            ]
        },
        {
            "question": "What is one problem with the vanilla RNN's transformation when it comes to the state representation?",
            "reference-answers": [
                "One problem with the vanilla RNN's transformation is that it applies a non-linear transformation to all elements of the state at each time step, causing the state representation to change radically and discontinuously, which makes training difficult and the learned representations hard to interpret."
            ]
        },
        {
            "question": "What is the key issue that makes training vanilla recurrent neural networks challenging, as described in the text?",
            "reference-answers": [
                "The key issue is that the state representations change drastically at each time step due to the nonlinearity, causing discontinuity and making training challenging."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen14-slide11/text.txt": [
        {
            "question": "What is the speaker suggesting by repeating \"one...technically...technically...\"?",
            "reference-answers": [
                "The speaker is suggesting that something is technically correct, perhaps with some nuance or hesitation, indicating that while it is technically true, there might be more to consider."
            ]
        },
        {
            "question": "How many times does the word \"technically\" appear in the TEXT?",
            "reference-answers": [
                "2"
            ]
        },
        {
            "question": "How many times does the word \"technically\" appear in the given text?",
            "reference-answers": [
                "2"
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen15-slide10/text.txt": [
        {
            "question": "What causes the vanishing gradient problem in deep neural networks during backpropagation?",
            "reference-answers": [
                "The vanishing gradient problem arises during backpropagation in deep neural networks because the gradients of the loss function with respect to the weights diminish as they pass through multiple layers. This occurs due to the use of non-linear activation functions, such as sigmoid or tanh, whose derivatives (used in the chain rule during backpropagation) can be very small for certain input ranges. As these small gradients are propagated backward through each layer, the multiplicative effect of these small derivatives across many layers leads to a significant reduction in gradient magnitude. This makes the weight updates in the earlier layers very small, thereby slowing down learning and contributing to the vanishing gradient problem."
            ]
        },
        {
            "question": "What happens to the values of parameter changes as they pass through non-linearities during backpropagation in a deep network?",
            "reference-answers": [
                "The values of parameter changes diminish as they pass through non-linearities during backpropagation in a deep network, leading to the vanishing gradient problem."
            ]
        },
        {
            "question": "Why does the vanishing gradient problem occur in deep networks during backpropagation?",
            "reference-answers": [
                "The vanishing gradient problem occurs in deep networks during backpropagation because the gradients of the parameters propagate backward through each layer, and as they pass through non-linear activation functions, their values get multiplied by the weights and the derivatives of these functions. In deep networks, where the number of layers corresponds to the depth of the network, this process repeats multiple times, causing the gradients to diminish significantly as they move backward. This diminishing effect becomes more pronounced in deeper layers, making it difficult for the earlier layers to learn effectively."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen18-slide07/text.txt": [
        {
            "question": "Why do the early layers in a deep network experience challenges during training when using backpropagation?",
            "reference-answers": [
                "The early layers in a deep network experience challenges during training with backpropagation because the updates to the parameters, when passed through the nonlinearity backwards, become too small. This leads to the vanishing gradient problem, where the early layers' parameters change very slowly or not at all, making training inefficient. Additionally, this can result in underflow issues, where the values become too small due to numerical errors, preventing any updates to the early layers."
            ]
        },
        {
            "question": "What is the vanishing gradient problem and how does it occur in deep networks?",
            "reference-answers": [
                "The vanishing gradient problem occurs in deep networks when gradients used to update weights become too small as they are propagated backward through layers, especially due to non-linearities applied at each step. This causes early layers to receive minimal updates, slowing or halting their training, and can lead to numerical underflow where updates cease entirely."
            ]
        },
        {
            "question": "Why do the early layers of a deep network face challenges during training?",
            "reference-answers": [
                "The early layers of a deep network face challenges during training because the gradients used to update the parameters become too small as they propagate backward through the network, a phenomenon known as the vanishing gradient problem. This results in the early layers, which are crucial for interpreting input values, not receiving meaningful updates and potentially experiencing numerical underflow issues, where the gradients become too small to affect the early layers' parameters effectively."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen21-slide11/text.txt": [
        {
            "question": "What is the reason for using two different nonlinearity functions (tanh and sigmoid) in the Gated Recurrent Unit (GRU)?",
            "reference-answers": [
                "The two different nonlinearity functions, tanh and sigmoid, are used in the GRU for distinct purposes. The tanh function squashes the input from minus infinity to plus infinity into the interval from -1 to 1, which helps in scaling the candidate state vector. The sigmoid function, which outputs values between 0 and 1, is suitable for the gates (reset and update) as it allows them to specify how much information to retain or discard, effectively controlling the flow of information in the network."
            ]
        },
        {
            "question": "What is the reason for using two different nonlinearity functions (tanh and sigmoid) in the gated recurrent unit (GRU)?",
            "reference-answers": [
                "The reason for using two different nonlinearity functions in the gated recurrent unit (GRU) is that they serve different purposes. The tanh function squashes the input from the range (-∞, ∞) to (-1, 1), which helps in scaling the output into a smaller interval. This is useful for the computation of the candidate state (the tilde{h}_t) in the GRU. On the other hand, the sigmoid function outputs values between 0 and 1, which is ideal for the gates (reset gate and update gate) as it allows them to determine the proportion of information to retain or discard, effectively controlling the flow of information within the unit."
            ]
        },
        {
            "question": "What is the role of the update gate in a Gated Recurrent Unit (GRU)?",
            "reference-answers": [
                "The update gate in a Gated Recurrent Unit (GRU) determines the proportion of the new candidate state that should be incorporated into the current state. It controls how much of the old state's information is retained and how much of the new candidate state is added. By using a sigmoid activation function, the update gate outputs values between 0 and 1, effectively deciding the mixing ratio for each element in the state vector, allowing the GRU to adaptively integrate new information while preserving relevant historical data."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen22-slide12/text.txt": [
        {
            "question": "What is the next step after processing variable input lengths in machine translation?",
            "reference-answers": [
                "The next step is processing the text that we are going to process in machine translation."
            ]
        },
        {
            "question": "What is the next text to be processed in machine translation after handling variable input lengths?",
            "reference-answers": [
                "The next text to be processed in machine translation after handling variable input lengths is the text that will be translated."
            ]
        },
        {
            "question": "What is the next step after processing variable input lengths in machine translation as mentioned in the text?",
            "reference-answers": [
                "processing the rare words and unknown tokens in the text"
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen23-slide13/text.txt": [
        {
            "question": "What is the main technical problem when representing sentences as matrices with column vectors for English and Czech languages?",
            "reference-answers": [
                "The main technical problem is that the vectors representing English words are of length 1 million and Czech words are even longer, around 2 million, making it impossible to process them efficiently."
            ]
        },
        {
            "question": "What issue is highlighted when using vectors of length 1 million (English) and over 2 million (Czech) for representing sentences?",
            "reference-answers": [
                "The issue highlighted is that processing vectors of such large lengths (1 million for English and over 2 million for Czech) becomes technically impossible due to their immense size."
            ]
        },
        {
            "question": "What problem arises when representing sentences as vectors in English and Czech languages?",
            "reference-answers": [
                "The problem that arises when representing sentences as vectors in English and Czech languages is that the vectors become impractically large to process. In English, the vector has a length of 1 million, and in Czech, it can be 2 million or more, making it impossible to handle due to their enormous size."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen24-slide14/text.txt": [
        {
            "question": "What method is suggested to reduce the size of one-hot representation vectors, and what specific homework assignment is mentioned for implementing this method?",
            "reference-answers": [
                "The method suggested to reduce the size of one-hot representation vectors is Byte Pair Encoding (BPE), and the homework assignment mentioned is to implement this method using BPE by Rico Sandrich."
            ]
        },
        {
            "question": "What method is suggested to reduce the vocabulary size in the context of one-hot representation vectors, and what was assigned as homework?",
            "reference-answers": [
                "The method suggested to reduce the vocabulary size is Byte Pair Encoding, and the homework assigned is to implement it."
            ]
        },
        {
            "question": "What is the method described for reducing the vocabulary size in language processing, and how does it work?",
            "reference-answers": [
                "The method described for reducing the vocabulary size in language processing is Byte Pair Encoding (BPE), an automatically trained approach. It works by starting with the alphabet as the initial set of units. The most frequent character pairs are then combined to form larger units. This process is repeated, combining units iteratively, until the vocabulary reaches the desired size. This results in a vocabulary that includes individual letters, frequent word parts, and even entire frequent words, effectively reducing the vocabulary size while maintaining the ability to produce all words."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen25-slide15/text.txt": [
        {
            "question": "What is the next step required after handling the sub-work units that result in 30,000 elements in the vector?",
            "reference-answers": [
                "The next step is to embed these vectors into a continuous space."
            ]
        },
        {
            "question": "The next step after creating a vector with 30,000 elements is to embed it into a continuous space, which can be achieved using Word2Vec.",
            "reference-answers": [
                "The next step after creating a vector with 30,000 elements is to embed these vectors into a continuous space, which can be achieved using Word2Vec."
            ]
        },
        {
            "question": "What is the next step needed when dealing with sub-work units that result in 30,000 elements in a vector?",
            "reference-answers": [
                "The next step is to embed these vectors into a continuous space."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen26-slide13/text.txt": [
        {
            "question": "What issue does the text identify with sparse word representations, and how does it suggest dense representations address this problem?",
            "reference-answers": [
                "The text identifies that sparse word representations lack measurable similarity between words, making computations crude and sensitive to input changes. Dense representations address this by capturing meaningful relationships, allowing the system to more effectively reuse translations based on similarity."
            ]
        },
        {
            "question": "Question: Why is the computation considered too crude according to the text?",
            "reference-answers": [
                "The computation is considered too crude because there is a lack of measurable or useful similarity between words. Since words like \"kitten\" are equally dissimilar to \"cat,\" \"is,\" \"mad,\" and \"the,\" the system cannot reliably capture meaningful relationships, making it unstable and not robust to small changes in input."
            ]
        },
        {
            "question": "How does the lack of measurable similarity between words affect the robustness of the system in handling small input changes?",
            "reference-answers": [
                "The lack of measurable similarity between words makes the system's computations too crude and not robust to small changes in the input. For example, if the input changes from \"cat\" to \"kitten,\" the system cannot adjust its translation because there is no measurable similarity between these words. This causes the system to potentially use the wrong translation, such as using \"cat\" instead of \"kitten,\" because it cannot recognize the subtle difference between them."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen27-slide15/text.txt": [
        {
            "question": "What do the embeddings capture in the context of the tasks they are trained for?",
            "reference-answers": [
                "Embeddings capture various features relevant to the specific tasks they are trained for. They can represent semantic, syntactic, and contextual aspects of words, such as verb tenses, colors, politeness levels, and sentiment. For instance, in sentiment analysis, embeddings might group words based on their emotional connotations rather than their meaning. Additionally, embeddings can capture relationships between words, like predicting neighboring words in a text. The exact features they represent depend on the training task, leading to different embeddings optimized for different downstream applications."
            ]
        },
        {
            "question": "What is the reason that using embeddings for different downstream tasks, such as sentiment analysis or politeness detection, leads to different trained values of the embeddings?",
            "reference-answers": [
                "The reason that using embeddings for different downstream tasks leads to different trained values is because the embeddings are trained based on the specific task's requirements. For example, in sentiment analysis, the embeddings learn to capture features related to positive or negative sentiment, while for politeness detection, they focus on features indicating politeness. The network empirically identifies task-specific features from the training data, leading the embeddings to adapt and represent aspects of the words that are most relevant to each particular task."
            ]
        },
        {
            "question": "How do the features captured by embeddings change depending on the specific task the neural network is trained for?",
            "reference-answers": [
                "The features captured by embeddings change depending on the specific task because the embeddings are trained to be most useful for that task. For example, in a task like predicting word neighbors, the embeddings might capture information about verb tenses or word colors. In another task, like sentiment analysis, the embeddings might group words based on whether they are positive or negative, rather than their specific meanings. Each task leads to different trained embeddings that reflect the relevant features for that particular task."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen28-slide16/text.txt": [
        {
            "question": "What is the purpose of the softmax normalization in the output layer of the neural network described in the text?",
            "reference-answers": [
                "The purpose of the softmax normalization in the output layer is to convert the raw output (logits or energies) into a probability distribution that sums to one, allowing the selection of the most probable word to emit."
            ]
        },
        {
            "question": "What is the expected vocabulary size for the output side and what method is used to normalize the outputs to produce a probability distribution?",
            "reference-answers": [
                "The expected vocabulary size for the output side is 30,000, and the method used to normalize the outputs to produce a probability distribution is the softmax normalization."
            ]
        },
        {
            "question": "What is the process of converting the hidden state of the neural network into a probability distribution over the vocabulary, and what function is used to normalize these probabilities?",
            "reference-answers": [
                "The hidden state of the neural network is projected and scaled up to the size of the vocabulary to generate a vector of word energies (logits). This vector is then normalized using the softmax function, which applies the exponential function to each energy and divides by the sum of all exponentials. This results in a probability distribution over the vocabulary where the probabilities sum to one."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen29-slide17/text.txt": [
        {
            "question": "What is Neural Language Modeling the basis of?",
            "reference-answers": [
                "The text does not provide the completion of the statement, so the basis of Neural Language Modeling is not specified here."
            ]
        },
        {
            "question": "What is Neural Language Modeling the basis of?",
            "reference-answers": [
                "The text provided does not specify what Neural Language Modeling is the basis of, as it cuts off after \"basis of...\". More context is needed to provide a precise answer."
            ]
        },
        {
            "question": "What is Neural Language Modeling stated to be the basis of?",
            "reference-answers": [
                "Neural Language Modeling is stated to be the basis of..."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen30-slide18/text.txt": [
        {
            "question": "What is the purpose of word embeddings in the described neural network translation system?",
            "reference-answers": [
                "The purpose of word embeddings in the described neural network translation system is to convert discrete word representations into continuous vectors, allowing the network to process and predict words more effectively by capturing their semantic meanings in a continuous space. They are used in the forward direction, both when predicting the next word and after a word is chosen to inform the next state of the network."
            ]
        },
        {
            "question": "What is the role of word embeddings in the NMT system described, and in which direction are they used?",
            "reference-answers": [
                "Word embeddings in the NMT system are used to map words from their one-hot representation into a continuous vector space. They are used in the forward direction, both at the input side to convert words into their continuous representations and at the output side when sampling the next word by converting it back into the continuous space using the embeddings. This ensures that the network processes words in a continuous space for both input and output."
            ]
        },
        {
            "question": "What is the role of word embeddings in NMT systems during both training and prediction?",
            "reference-answers": [
                "Word embeddings in NMT systems are used to convert words into continuous vectors in a shared space. During training, they transform the one-hot word representations into continuous space, allowing the network to process words more effectively. During prediction, after sampling a word from the predicted distribution, the network uses embeddings to map this word back into the continuous space, ensuring consistency in processing. The embeddings are used in the forward direction, not reversed, to facilitate both training and prediction."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen31-slide19/text.txt": [
        {
            "question": "What are the two possible views of the recurrent neural network language model discussed in the text?",
            "reference-answers": [
                "The two possible views are that the recurrent neural network language model can be seen as a loop function or a map function over sequential data, and it can also be viewed as a way to define a probability distribution over sequences of words."
            ]
        },
        {
            "question": "What are the two possible views of the recurrent neural network language model discussed in the text, and what do they represent?",
            "reference-answers": [
                "The two possible views of the recurrent neural network language model are:\n\n1. It can be seen as a loop function or a map function over sequential data, representing the model's ability to process sequences step by step.\n2. It can be viewed as a way to define a probability distribution over sequences of words, representing the model's ability to assign probabilities to different word sequences."
            ]
        },
        {
            "question": "What are the two possible views of a recurrent neural network language model, and how do they differ?",
            "reference-answers": [
                "The two possible views of a recurrent neural network language model are:\n\n1. **As a loop function or map function over sequential data**: This view emphasizes the sequential processing aspect of the model, where it operates as a function that processes input data one element at a time, updating its hidden state as it progresses through the sequence.\n\n2. **As a way to define a probability distribution over sequences of words**: This view focuses on the probabilistic nature of the model, where it assigns probabilities to different sequences of words, allowing for the generation or prediction of likely word sequences.\n\nThese views differ in their emphasis: one focuses on the sequential processing and functional aspect of the model, while the other highlights its role in defining probability distributions over word sequences."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen32-slide20/text.txt": [
        {
            "question": "What is the purpose of using two recurrent neural networks in a bidirectional recurrent neural network, and how are their states combined to achieve this purpose?",
            "reference-answers": [
                "The purpose of using two recurrent neural networks in a bidirectional recurrent neural network is to capture context from both directions of the input. This means one RNN processes the input from left to right, while the other processes it from right to left. The states from both RNNs at corresponding positions are concatenated to combine their information, allowing the model to utilize both forward and backward context effectively."
            ]
        },
        {
            "question": "How do bidirectional recurrent neural networks process input from multiple directions?",
            "reference-answers": [
                "Bidirectional recurrent neural networks process input by using two recurrent neural networks to digest the input from left to right and from right to left. After processing, they concatenate the states at the corresponding positions, allowing them to capture information from both directions simultaneously."
            ]
        },
        {
            "question": "How does a bidirectional recurrent neural network process the input to capture context from both directions?",
            "reference-answers": [
                "A bidirectional recurrent neural network processes the input by using two recurrent neural networks to digest the input from both the left to right and the right to left directions. After this digestion, the states at the corresponding positions are concatenated, allowing the model to capture context from both directions."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen33-slide22/text.txt": [
        {
            "question": "What is the advantage of using bidirectional recurrent neural networks in natural language processing tasks, as described in the text?",
            "reference-answers": [
                "The advantage of using bidirectional recurrent neural networks in natural language processing tasks is that they provide a bidirectional representation, allowing the network to access the complete sentence at every state. This means the network can decide to focus on either the beginning or the end of the sentence, or any part of it, enabling it to effectively handle tasks by considering the entire context and patterns within the sentence."
            ]
        },
        {
            "question": "What is the primary advantage of using bidirectional recurrent neural networks in natural language processing tasks?",
            "reference-answers": [
                "The primary advantage is that bidirectional recurrent neural networks provide access to the complete sentence at every state, allowing the network to focus on relevant parts and understand sequences and word order effectively."
            ]
        },
        {
            "question": "What makes bidirectional recurrent neural networks effective for natural language processing tasks?",
            "reference-answers": [
                "Bidirectional recurrent neural networks are effective for natural language processing tasks because they maintain a bidirectional representation that provides access to the complete sentence at every state. This allows the network to decide what to remember, focus on keywords or sequences, recognize patterns like verb-noun order, and effectively handle various NLP tasks with enough training data."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen34-slide23/text.txt": [
        {
            "question": "What architecture is used in the translation systems discussed?",
            "reference-answers": [
                "The architecture used in the translation systems discussed is the encoder-decoder architecture."
            ]
        },
        {
            "question": "What architecture is primarily used in translation systems, as mentioned in the text?",
            "reference-answers": [
                "The encoder-decoder architecture is primarily used in translation systems, as mentioned in the text."
            ]
        },
        {
            "question": "What architecture is primarily used in translation systems according to the text?",
            "reference-answers": [
                "The encoder-decoder architecture."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen35-slide24/text.txt": [
        {
            "question": "What is the main idea described in the TEXT for answering the question generated by the model?",
            "reference-answers": [
                "The main idea is that the model uses a conditional language model scheme to generate answers."
            ]
        },
        {
            "question": "The question is asking for the idea mentioned in the text. The text states that the idea is to exploit the conditional language model scheme again. Therefore, the answer should reflect that.\n\n**Answer:** The idea is to exploit the conditional language model scheme.",
            "reference-answers": [
                "The idea is to exploit the conditional language model scheme."
            ]
        },
        {
            "question": "What method is used to generate a single exam question based on the text?",
            "reference-answers": [
                "Which technique is employed to create an exam question using the given text?\n\nANSWER: Conditional language model scheme"
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen37-slide27/text.txt": [
        {
            "question": "What is the primary function of the encoder in the described network setup?",
            "reference-answers": [
                "The primary function of the encoder is to process the input sequence of symbols into a single vector representation."
            ]
        },
        {
            "question": "The process involves three main steps: \n1. The encoder converts the input sequence into a single vector.\n2. This vector is transferred to the decoder.\n3. The decoder generates the language model output using this vector.",
            "reference-answers": [
                "The process involves three main steps:  \n1. The encoder converts the input sequence into a single vector.  \n2. This vector is transferred to the decoder.  \n3. The decoder generates the language model output using this vector."
            ]
        },
        {
            "question": "What is the purpose of the encoder in the described network setup?",
            "reference-answers": [
                "The purpose of the encoder is to process the input sequence of symbols into a single vector representation, which is then used to condition the decoder for language model generation."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen38-slide28/text.txt": [
        {
            "question": "What is the purpose of sharing the embeddings between the source and target languages in the described neural machine translation model?",
            "reference-answers": [
                "The purpose of sharing the embeddings between the source and target languages in the described neural machine translation model is to allow for a single vocabulary that can handle words that should be copied, especially when translating between languages with different scripts. Additionally, sharing embeddings reduces the model's complexity by using one embedding matrix for both languages, simplifying training and updates during backpropagation."
            ]
        },
        {
            "question": "What is the encoder-decoder architecture in this model, and how are embeddings used?",
            "reference-answers": [
                "The encoder-decoder architecture consists of two recurrent neural networks (RNNs). The encoder processes the input sequentially, building up a state that represents the input. This state is then passed to the decoder, which generates the output sequence. The decoder starts with a beginning-of-sentence symbol and uses the encoder's state to predict each subsequent word, acting as a conditional language model.\n\nEmbeddings are used to convert discrete input and output tokens into continuous representations. Source embeddings transform input words for the encoder, while target embeddings transform output words for the decoder. These embeddings are often shared, especially when translating between languages with different scripts, to handle copied words and reduce the number of parameters. The shared embedding matrix is updated during backpropagation in both the encoder and decoder."
            ]
        },
        {
            "question": "**Question:**  \nWhat is the purpose of sharing embeddings between the source and target languages in the encoder-decoder architecture described, and how does the decoder function as a conditional language model in the context of neural machine translation?",
            "reference-answers": [
                "The purpose of sharing embeddings between the source and target languages is to reduce the number of parameters and improve efficiency, especially when translating between languages with different scripts. This sharing allows similar words to be represented consistently, facilitating direct translation. The decoder functions as a conditional language model by using the encoder's state as context, starting with a beginning-of-sentence symbol and iteratively predicting the most likely next word in the target language, conditioned on the source sentence."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen39-slide30/text.txt": [
        {
            "question": "What is the initialization of the decoder's state in the described conditional language model?",
            "reference-answers": [
                "The decoder's state is initialized with the value of the encoder's final state."
            ]
        },
        {
            "question": "What is the initial state of the decoder in a conditional language model, and what does it use to predict the next word in the target language?",
            "reference-answers": [
                "The initial state of the decoder in a conditional language model is the final state of the encoder. The decoder uses this initial state along with the last word (which is the beginning of the sentence symbol) to predict the next word in the target language."
            ]
        },
        {
            "question": "How is the decoder initialized and what is the process for generating the next word in the target language?",
            "reference-answers": [
                "The decoder is initialized with the state inherited from the encoder. This state is the same size as the encoder's state and is passed directly to the decoder as its initial state. To start generating the target language sentence, the decoder begins with the beginning of the sentence symbol as the last word. The process for generating the next word involves running the decoder cell to predict the next state and the current output based on the previous state and the last word. The current output is then used to create logits, which are passed through the output projection to get the probability distribution over the target vocabulary. The next word is selected using maximization, and this word is then embedded and used as the input for the next step in the decoder, along with the new state. This process continues until the decoder produces the end of sentence symbol."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen40-slide32/text.txt": [
        {
            "question": "What is the introductory phrase used by the speaker?",
            "reference-answers": [
                "So, uh,"
            ]
        },
        {
            "question": "What part of speech is \"with\" in the phrase \"So, uh, with...\"?",
            "reference-answers": [
                "preposition"
            ]
        },
        {
            "question": "What is the speaker about to discuss?",
            "reference-answers": [
                "The speaker's topic cannot be determined from the provided text."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen41-slide31/text.txt": [
        {
            "question": "What is the relationship between the input and target languages in the context of the provided TEXT?",
            "reference-answers": [
                "The target language tokens are from the same vocabulary as the source language tokens."
            ]
        },
        {
            "question": "What does the input consist of when the teacher prepares for the exam?",
            "reference-answers": [
                "The input consists of a sequence of source language tokens, which are sub-work units."
            ]
        },
        {
            "question": "Question: Are the input tokens in machine translation typically drawn from the same vocabulary as the output tokens?",
            "reference-answers": [
                "Yes, the input tokens in machine translation are typically drawn from the same vocabulary as the output tokens."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen42-slide32/text.txt": [
        {
            "question": "What is the initial state of the encoder before processing any input words?",
            "reference-answers": [
                "The initial state of the encoder before processing any input words is a vector of zeros."
            ]
        },
        {
            "question": "What is the purpose of the encoder's final state after processing the input sequence?",
            "reference-answers": [
                "The purpose of the encoder's final state after processing the input sequence is to represent the whole sentence."
            ]
        },
        {
            "question": "What is the initial state of the encoder, and how is the final state obtained after processing all words in the input sequence?",
            "reference-answers": [
                "The encoder is initialized with a vector of zeros as its initial state. After processing all the words in the input sequence through the RNN, the final state is obtained by arriving at the state after consuming the last word, which represents the whole sentence."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen43-slide33/text.txt": [
        {
            "question": "What matrix is used in the final step of the decoder's output projection to produce the output word, and what operation is performed on it to determine the highest scoring element?",
            "reference-answers": [
                "The matrix used in the final step of the decoder's output projection to produce the output word is the output vocabulary matrix. The operation performed is matrix multiplication of the state vector with this matrix, followed by taking the argmax to determine the highest scoring element."
            ]
        },
        {
            "question": "Where is the scaling to the output vocabulary applied, and what is the size of the matrix used for this scaling?",
            "reference-answers": [
                "The scaling to the output vocabulary is applied by multiplying the vector with the output vocabulary matrix. The size of this matrix is TI size times the vocabulary size."
            ]
        },
        {
            "question": "What are the two main uses of the decoder's output state in the described recurrent neural network?",
            "reference-answers": [
                "The decoder's output state is used in two main ways: it is utilized in the next computation step of the recurrent neural network and it serves as the output word in the output projection layer."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen44-slide34/text.txt": [
        {
            "question": "How is the output candidate scored during the training process described?",
            "reference-answers": [
                "The output candidate is scored by comparing the probability distribution of the expected word in the output distribution. The goal is for the expected word to have the highest probability in this distribution. If it doesn't, the model adjusts its computation to increase the expected word's probability."
            ]
        },
        {
            "question": "What is the initial step in training the system as described in the text?",
            "reference-answers": [
                "The initial step in training the system is having a big parallel corpus and aligning the sentences sentence by sentence."
            ]
        },
        {
            "question": "What is the process for training the system mentioned in the text, including how it uses a parallel corpus and adjusts based on the probability of expected outputs?",
            "reference-answers": [
                "The process for training the system involves using a large parallel corpus, which is aligned sentence by sentence. The training starts with a randomly initialized model. Input is processed through the entire network to generate an output candidate. The output is then scored to determine how well it matches the expected target. For each time step, the model produces a probability distribution over the vocabulary, ideally making the expected word the highest probability. If not, adjustments are made to the model to increase the probability of the expected word, improving the system's performance."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen45-slide35/text.txt": [
        {
            "question": "**Question:**  \nWhy is the output distribution described as \"ugly\" in the context of the provided discussion?",
            "reference-answers": [
                "The output distribution is described as \"ugly\" because it is overly simplistic and restrictive. It assigns a probability of 1 to the word seen in the training data and 0 to all other candidate tokens, creating a very peaked distribution. This approach does not account for the uncertainty or variety of possible translations, making it an inadequate representation of the true distribution."
            ]
        },
        {
            "question": "What is the nature of the output distribution defined in the text?",
            "reference-answers": [
                "The output distribution is defined as a very peaked distribution where it is set to 1 for the word seen at the position in the training data and 0 for all other candidate tokens. This distribution is not smooth or realistic as it only enforces the expected word at each position."
            ]
        },
        {
            "question": "How is the output distribution defined in this context, and what does this imply about its shape?",
            "reference-answers": [
                "The output distribution is defined such that it is set to 1 for the word seen at the position in the training data and 0 for all other candidate tokens. This results in a very peaked distribution, meaning it is sharply focused on the expected reference translation and does not account for other possible translations."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen46-slide36/text.txt": [
        {
            "question": "What is the loss considered to be, based on the cross-entropy between the distributions mentioned?",
            "reference-answers": [
                "The loss is the cross-entropy between the distribution produced by the soft-max function and the true distribution, which is one-hot encoded."
            ]
        },
        {
            "question": "What is the true distribution mentioned in the text and what does it consist of?",
            "reference-answers": [
                "The true distribution mentioned in the text is a one-hot distribution, which consists of all zeros except for a single one corresponding to the word that was expected."
            ]
        },
        {
            "question": "What is the loss in this context, and how is it defined using cross-entropy?",
            "reference-answers": [
                "The loss in this context is the cross-entropy between the distribution produced by the soft-max function (denoted as \\(\\hat{p}\\)) and the true distribution. The true distribution is defined as having all zeros and one for the word that is expected. This loss is mathematically defined as the cross-entropy between these two distributions."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen47-slide37/text.txt": [
        {
            "question": "What does taking the negative log of the probability of the expected word in the softmax output represent?",
            "reference-answers": [
                "The negative log of the probability of the expected word in the softmax output represents the loss incurred at that position."
            ]
        },
        {
            "question": "What is the loss incurred at a position when the negative log of the expected word's probability is taken?",
            "reference-answers": [
                "The loss incurred at a position is the negative log of the expected word's probability."
            ]
        },
        {
            "question": "What is the loss incurred at a position when using the negative log of the expected word's probability in the softmax output?",
            "reference-answers": [
                "The loss incurred at a position is the negative log of the probability of the expected word in the softmax output."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen48-slide38/text.txt": [
        {
            "question": "What is the losses at each output word proportional to during the computation of the derivatives?",
            "reference-answers": [
                "The losses at each output word are proportional to the negative logarithm of the probability of the word that should have been predicted."
            ]
        },
        {
            "question": "What formula is used to compute the loss at each output word during the computation of derivatives?",
            "reference-answers": [
                "The formula used to compute the loss at each output word during the computation of derivatives is the negative logarithm of the probability of the predicted word. Specifically, the loss at each word is proportional to \\(-\\log(p_i)\\), where \\(p_i\\) is the probability of the word that should have been predicted at that position."
            ]
        },
        {
            "question": "How is the loss for each position calculated in the context of word prediction?",
            "reference-answers": [
                "The loss for each position is calculated as the negative logarithm of the probability of the word that should have been predicted at that position."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen49-slide39/text.txt": [
        {
            "question": "What is an ugly consequence of training neural networks for machine translation to produce exact words at exact positions?",
            "reference-answers": [
                "The ugly consequence is that neural networks become rigid and inflexible, as they are forced to memorize the exact positions of words in the parallel corpus, which goes against the variability of language output. This leads to poor generalization, as any slight shift or variation in the output can heavily impact their performance."
            ]
        },
        {
            "question": "What is the main issue with the training criterion for neural networks in machine translation as discussed in the text?",
            "reference-answers": [
                "The main issue is that the training criterion forces neural networks to memorize the exact word positions in the parallel corpus, which goes against the natural variability and flexibility of language output in machine translation."
            ]
        },
        {
            "question": "What is a major issue with the training criterion of neural networks for machine translation tasks, according to the provided text?",
            "reference-answers": [
                "The major issue is that the training criterion forces neural networks to memorize exact word positions in parallel corpora, which fails to account for the variability and flexibility of language output, leading to inadequate generalization."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen50-slide40/text.txt": [
        {
            "question": "How does the decoder function during training compared to how it functions at runtime?",
            "reference-answers": [
                "During training, the decoder uses the probability distribution from the Softmax layer to estimate the loss by comparing the model's predictions with the expected words in the training data. It processes the entire expected sentence, checking the probability of each correct word and accumulating the loss based on how well the model predicted each step. At runtime, the decoder selects the highest scoring word from the probability distribution at each step and continues until the end of the sentence symbol is emitted, focusing solely on generating the most probable sequence of words without considering the training loss."
            ]
        },
        {
            "question": "What is the primary difference between how the decoder operates during training and at runtime?",
            "reference-answers": [
                "During training, the decoder uses the probability distribution to compute the loss by comparing the network's output with the expected words, while at runtime, it selects the highest scoring word from the distribution to generate the output, step by step, until the end of the sentence is reached."
            ]
        },
        {
            "question": "How does the decoder function differ between the training and runtime phases?",
            "reference-answers": [
                "During training, the decoder uses the probability distribution from the Softmax layer to calculate the loss by comparing the expected word's probability. If the expected word has a low probability, the loss increases. The network is then fed the correct word from the reference sentence to continue training.\n\nIn runtime, the decoder selects the highest scoring word from the probability distribution and uses it to generate the next state, continuing until the end of sentence symbol is emitted."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen51-slide42/text.txt": [
        {
            "question": "Question: What happens at each step during the decoding process?",
            "reference-answers": [
                "At each step during the decoding process, the decoder produces a distribution of the vocabulary and then you need to choose."
            ]
        },
        {
            "question": "What is chosen after the decoder produces a distribution of the vocabulary at each step during decoding?",
            "reference-answers": [
                "The next word is chosen based on the distribution of the vocabulary produced by the decoder at each step during decoding."
            ]
        },
        {
            "question": "What does the decoder need to choose after producing a distribution of the vocabulary at each step?",
            "reference-answers": [
                "The decoder needs to choose the next word or token from the vocabulary distribution it generated at each step."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen52-slide43/text.txt": [
        {
            "question": "What is the main strategy used during decoding?",
            "reference-answers": [
                "The main strategy used during decoding is to choose the highest scoring word at each time step."
            ]
        },
        {
            "question": "What is the method described for decoding?",
            "reference-answers": [
                "The method involves choosing the highest scoring word at each time step during decoding."
            ]
        },
        {
            "question": "What does the text say about decoding?",
            "reference-answers": [
                "Decoding involves simply choosing the highest scoring word at that time step."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen53-slide45/text.txt": [
        {
            "question": "What characteristic makes greedy decoding a suitable method for neural language models according to the text?",
            "reference-answers": [
                "Greedy decoding is a suitable method for neural language models because it is fast and memory efficient, requiring only the previous state and output without storing candidates."
            ]
        },
        {
            "question": "What is the efficient decoding method that only requires the previous state and output, and is aligned with the training of neural networks?",
            "reference-answers": [
                "greedy decoding"
            ]
        },
        {
            "question": "What is the reasoning behind using greedy decoding for neural networks according to the text?",
            "reference-answers": [
                "Greedy decoding is efficient and effective for neural networks because it doesn't require storing multiple candidates, only the previous state and output. It works well with how neural networks, especially language models, are trained to produce coherent sequences by making the best possible choice at each step."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen54-slide46/text.txt": [
        {
            "question": "What is the main difference between greedy decoding and beam search in terms of how they handle sentence beginnings and final outcomes?",
            "reference-answers": [
                "The main difference lies in their approach to sentence beginnings and final outcomes. Greedy decoding selects the most probable words at each step, focusing on immediate best choices, which might not lead to the optimal overall sentence. In contrast, beam search considers multiple candidate sentences, allowing for less probable beginnings that can lead to more probable final outcomes by exploring various paths."
            ]
        },
        {
            "question": "What is the key difference between beam search and greedy decoding when generating sentences with neural networks, as described in the text?",
            "reference-answers": [
                "The key difference is that beam search considers multiple possible sentence continuations at each step, keeping the most promising ones (a \"beam\" of candidates), while greedy decoding only selects the best-scoring word at each step without considering how it might affect the overall sentence."
            ]
        },
        {
            "question": "What is a potential advantage of using beam search over greedy decoding in neural network decoding?",
            "reference-answers": [
                "A potential advantage of using beam search over greedy decoding is that beam search allows the network to consider multiple less likely word choices early on, which can lead to a highly probable final sentence later. In contrast, greedy decoding focuses on selecting the most probable individual words at each step, which might not always result in the optimal overall sentence."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen55-slide47/text.txt": [
        {
            "question": "What is a hypothesis described as being in the given text?",
            "reference-answers": [
                "A hypothesis is described as a partially decoded sentence with some associated score."
            ]
        },
        {
            "question": "What is a hypothesis according to the text?",
            "reference-answers": [
                "According to the text, a hypothesis is a partially decoded sentence with some associated score."
            ]
        },
        {
            "question": "What is a hypothesis, according to the text?",
            "reference-answers": [
                "A hypothesis is a partially decoded sentence with some associated score."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen56-slide48/text.txt": [
        {
            "question": "What is the impact of increasing the beam size during beam search on the quality of the generated sentences in the context of recurrent language models?",
            "reference-answers": [
                "Increasing the beam size during beam search can lead to worse sentences because more candidates are considered, and the neural networks are trained to memorize the parallel corpus rather than adequately understanding and representing the sentences and their paraphrases jointly."
            ]
        },
        {
            "question": "What is a potential drawback of using beam search with a larger beam size in neural network-based language models?",
            "reference-answers": [
                "A potential drawback is that larger beam sizes can lead to worse sentences because more candidates are considered, and the neural networks, trained to memorize rather than deeply understand or represent paraphrases adequately, make suboptimal choices during the beam search process."
            ]
        },
        {
            "question": "Why does increasing the beam size in beam search result in worse sentence generation according to the text?",
            "reference-answers": [
                "Increasing the beam size in beam search results in worse sentence generation because considering more candidates leads to suboptimal training, as the neural networks are trained to memorize parallel corpora rather than understanding sentences or representing paraphrases adequately."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen57-slide49/text.txt": [
        {
            "question": "How is the score of a hypothesis updated when a new word is added during beam search?",
            "reference-answers": [
                "The score of a hypothesis is updated by multiplying the score of the prefix with the probability of the next word, resulting in a new score for the hypothesis extended by one word."
            ]
        },
        {
            "question": "What process involves extending hypotheses by multiplying the prefix score with the probability of the next word in beam search?",
            "reference-answers": [
                "The process is extending hypotheses by multiplying the prefix score with the probability of the next word in beam search."
            ]
        },
        {
            "question": "How is the score of a hypothesis updated when a new word is added during beam search?",
            "reference-answers": [
                "The score of a hypothesis is updated by multiplying the score of the prefix with the probability of the next word, resulting in a new score for the hypothesis extended by one word."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen58-slide50/text.txt": [
        {
            "question": "Question: What is the reason for preferring shorter hypotheses in this context?",
            "reference-answers": [
                "Shorter hypotheses are preferred because they contain fewer words, which results in a higher overall score."
            ]
        },
        {
            "question": "Why does the text say shorter hypotheses are preferable?",
            "reference-answers": [
                "Shorter hypotheses are preferable because they result in a higher overall score due to fewer words in the multiplication process."
            ]
        },
        {
            "question": "According to the text, why does it suggest preferring shorter hypotheses?",
            "reference-answers": [
                "It suggests preferring shorter hypotheses because having fewer words leads to a higher overall score."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen59-slide51/text.txt": [
        {
            "question": "What is the initial state of the beam in the beam search algorithm?",
            "reference-answers": [
                "The initial state of the beam in the beam search algorithm is a single empty hypothesis."
            ]
        },
        {
            "question": "What is the initial state of the beam in the beam search algorithm?",
            "reference-answers": [
                "The initial state of the beam in the beam search algorithm is a single empty hypothesis."
            ]
        },
        {
            "question": "What is the initial state of the beam search algorithm in terms of the number of hypotheses it begins with?",
            "reference-answers": [
                "The beam search algorithm begins with a single empty hypothesis."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen60-slide53/text.txt": [
        {
            "question": "What happens in each step when extending hypotheses in the beam search process?",
            "reference-answers": [
                "In each step of the beam search process, all the hypotheses in the beam are extended by the k most probable words, resulting in candidate hypotheses."
            ]
        },
        {
            "question": "What happens to the candidate hypotheses after each step in the described process?",
            "reference-answers": [
                "The candidate hypotheses are extended by the k most probable words in each step."
            ]
        },
        {
            "question": "What is done to the hypotheses in each step of the beam process?",
            "reference-answers": [
                "In each step of the beam process, all the hypotheses in the beam are extended by the k most probable words."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen61-slide55/text.txt": [
        {
            "question": "What process is described for managing candidates in the text?",
            "reference-answers": [
                "The process described for managing candidates involves scoring them and retaining only the top k candidates for the next step."
            ]
        },
        {
            "question": "What is the term used to describe the process of selecting candidates based on their scores and keeping only a certain number of them for the next step?",
            "reference-answers": [
                "The term used is \"beam search.\""
            ]
        },
        {
            "question": "What process involves evaluating and retaining a specific number of candidate hypotheses based on their scores?",
            "reference-answers": [
                "The process described is beam search. It involves evaluating candidate hypotheses based on their scores and retaining a specific number (k) of them for the next step, as mentioned in the text. Beam search is a common technique used in various machine learning applications, particularly in sequence models, to efficiently explore the most promising candidates by maintaining a \"beam\" of top hypotheses at each step."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen62-slide56/text.txt": [
        {
            "question": "What should be done with a hypothesis if it reaches the end of a sentence symbol while reading?",
            "reference-answers": [
                "If a hypothesis reaches the end of a sentence symbol while reading, it should be placed in a separate list of finished hypotheses, and reading continues."
            ]
        },
        {
            "question": "What should be done if a hypothesis reaches the end of a sentence symbol?",
            "reference-answers": [
                "If a hypothesis reaches the end of a sentence symbol, it should be placed in a separate list of finished hypotheses, and reading should continue."
            ]
        },
        {
            "question": "What is done with a hypothesis when it reaches the end of sentence symbols?",
            "reference-answers": [
                "It is put to a separate list of finished hypotheses and you keep reading."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen63-slide57/text.txt": [
        {
            "question": "What are the two conditions under which the process stops?",
            "reference-answers": [
                "The process stops under two conditions:\n1. When the time runs out.\n2. When all k-best hypotheses in the finished list and the current beam together have finished, ending with end-of-sentence symbols."
            ]
        },
        {
            "question": "What happens after all the beams are finished?",
            "reference-answers": [
                "After all the beams are finished, the process stops when there are k-best hypotheses in the finished list and the current beam, and they end with end-of-sentence symbols. The highest scoring hypothesis is then selected from those."
            ]
        },
        {
            "question": "Under what conditions does the beam search process stop according to the text?",
            "reference-answers": [
                "The beam search process stops under two main conditions: either when the time runs out or when there are k-best hypotheses that end with end of sentence symbols, at which point the highest scoring one is selected."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen64-slide58/text.txt": [
        {
            "question": "What happens to the hypothesis after it is sorted by the score?",
            "reference-answers": [
                "The best hypothesis is emitted after sorting by the score."
            ]
        },
        {
            "question": "What is the standard approach for generating the best hypothesis?",
            "reference-answers": [
                "The standard approach involves sorting the hypotheses by their scores and emitting the best one."
            ]
        },
        {
            "question": "What is the standard approach for selecting the best hypothesis?",
            "reference-answers": [
                "The standard approach involves sorting the hypotheses by their scores and then selecting the hypothesis with the highest score to emit as the best one."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen65-slide59/text.txt": [
        {
            "question": "What issue was faced with the initial approach, and what solution was proposed to address it?",
            "reference-answers": [
                "The initial approach faced the issue of degraded performance with longer sentences. The solution proposed was to change the structure of the network to retain performance even for longer sentences."
            ]
        },
        {
            "question": "Why did the approach's performance degrade with longer sentences, and what change was made to the network structure to maintain performance?",
            "reference-answers": [
                "The approach's performance degraded with longer sentences because its quality declined heavily as sentence length increased. To maintain performance, the network structure was changed to retain its effectiveness even for longer sentences."
            ]
        },
        {
            "question": "What issue did the approach face with longer sentences, and what solution was proposed to maintain performance?",
            "reference-answers": [
                "The approach faced degraded performance with longer sentences, and the proposed solution was to change the structure of the network to retain performance."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen66-slide60/text.txt": [
        {
            "question": "What is the purpose of attention in the described context?",
            "reference-answers": [
                "The purpose of attention is to allow the network to consider all source positions with the same magnitude or importance at any time step of the output."
            ]
        },
        {
            "question": "What is the main goal of attention in a network when processing source positions over time?",
            "reference-answers": [
                "The main goal of attention in a network when processing source positions over time is to allow the network to consider all source positions with the same magnitude or importance at any time step of the output."
            ]
        },
        {
            "question": "What is the goal of attention in the network as described?",
            "reference-answers": [
                "The goal of attention in the network is to allow it to consider all source positions with the same importance at any time step of the output."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen71-slide40/text.txt": [
        {
            "question": "What is the main problem discussed in the text regarding neural network processing of input sentences?",
            "reference-answers": [
                "The main problem is that the fixed-size representation of the input sentence becomes less effective over time as the decoder processes the output, leading to a loss of connection with the original input and degraded output quality towards the end."
            ]
        },
        {
            "question": "What is a potential solution mentioned in the text to address the problem of the decoder \"fabulating\" or losing connection with the input sentence?",
            "reference-answers": [
                "A potential solution is to inject the encoding of the input sentence at every step of the network, allowing the decoder to reconsider the input at each stage and maintain access to the initial sentence representation."
            ]
        },
        {
            "question": "What two methods are proposed to mitigate the issue of the decoder forgetting the input over time?",
            "reference-answers": [
                "The two methods proposed are: injecting the encoding of the input sentence at every step of the network and processing the input sentence in reverse order."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen76-slide60/text.txt": [
        {
            "question": "What is the purpose of the decoder's attention mechanism when processing the source sentence?",
            "reference-answers": [
                "The purpose of the decoder's attention mechanism is to allow it to choose which source words to focus on at each time step, based on the target sentence's dependencies and the decoder's state, rather than forcing long-distance dependencies into a single vector."
            ]
        },
        {
            "question": "What is the purpose of the decoder's attention mechanism in the context of source word queries?",
            "reference-answers": [
                "The purpose of the decoder's attention mechanism in the context of source word queries is to allow the decoder to dynamically determine which source words to focus on at each time step, enabling it to process the target sentence's dependencies without being constrained by long-distance dependencies in a single vector."
            ]
        },
        {
            "question": "How does the decoder utilize attention to determine which source words to focus on at each time step during the translation process?",
            "reference-answers": [
                "The decoder uses attention by considering which source words are relevant at each time step. It treats the target sentence's dependencies, such as subject-verb agreement, as a language model property and uses these as queries to focus on specific source words in the encoder's output. This allows the decoder to dynamically determine which source words to attend to during the translation process."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen77-slide61/text.txt": [
        {
            "question": "What are the main components mentioned in the description of the system inspired by Neural Turing machines?",
            "reference-answers": [
                "The main components mentioned are the controller, read heads, write heads, and memory."
            ]
        },
        {
            "question": "The read and write heads in Neural Turing Machines are responsible for accessing and modifying the memory, with the read heads identifying what to retrieve and the write heads managing the storage of information.",
            "reference-answers": [
                "The read and write heads in Neural Turing Machines are responsible for accessing and modifying the memory, with the read heads identifying what to retrieve and the write heads managing the storage of information."
            ]
        },
        {
            "question": "The main components of a neural network inspired by Neural Turing Machines include a controller, memory, and read/write heads.",
            "reference-answers": [
                "The main components of a neural network inspired by Neural Turing Machines include a controller, memory, and read/write heads."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen78-slide62/text.txt": [
        {
            "question": "What are the two primary methods by which the neural Turing machine can address its memory?",
            "reference-answers": [
                "The two primary methods are addressing by position and addressing by content (through similarity)."
            ]
        },
        {
            "question": "What are the two methods by which the memory in a neural Turing machine can be addressed?",
            "reference-answers": [
                "The two methods are addressing by position and addressing by content (similarity)."
            ]
        },
        {
            "question": "What are the two primary methods by which neural Turing machines address their memory?",
            "reference-answers": [
                "The two primary methods by which neural Turing machines address their memory are by position and by content (similarity)."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen79-slide63/text.txt": [
        {
            "question": "What is at the heart of attention, as indicated by the text?",
            "reference-answers": [
                "The text does not provide enough information to determine what is at the heart of attention."
            ]
        },
        {
            "question": "What is at the heart of attention, according to the text?",
            "reference-answers": [
                "The text does not provide enough information to answer the question."
            ]
        },
        {
            "question": "What is at the heart of attention, as suggested by the text?",
            "reference-answers": [
                "The text does not provide enough information to answer the question about what is at the heart of attention."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen80-slide64/text.txt": [
        {
            "question": "What does the text suggest about the effectiveness of content-based addressing?",
            "reference-answers": [
                "The text suggests that content-based addressing works well."
            ]
        },
        {
            "question": "What does the text identify as the focus?",
            "reference-answers": [
                "content-based addressing"
            ]
        },
        {
            "question": "What is the main focus of the discussion in the text?",
            "reference-answers": [
                "The main focus of the discussion in the text is on content-based addressing."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen81-slide66/text.txt": [
        {
            "question": "What is the purpose of the attention mechanism in neural networks, and how does it differ from traditional methods of condensing variable-length inputs?",
            "reference-answers": [
                "The attention mechanism in neural networks serves to dynamically focus on relevant parts of the input sequence during decoding. Unlike traditional methods, which condense variable-length inputs into a fixed-size representation (like using the last element or max pooling), attention creates a weighted sum of encoder states where the weights are automatically determined based on the decoder's current state and output. This allows the decoder to adaptively consider different parts of the input at each step, providing a more flexible and context-sensitive approach compared to static aggregation techniques."
            ]
        },
        {
            "question": "What is the role of the attention mechanism in the decoder?",
            "reference-answers": [
                "The attention mechanism in the decoder dynamically determines which parts of the encoder's states to emphasize at each step. It creates a weighted sum of these states, with the weights automatically adjusted based on the decoder's current state and previous output. This allows the decoder to focus on different parts of the input sentence as it progresses, enabling more context-aware and nuanced processing compared to static methods like taking the last element or max pooling."
            ]
        },
        {
            "question": "**Question:**  \nWhat is the key difference between initializing the decoder with a fixed representation and using an attention mechanism in neural machine translation?",
            "reference-answers": [
                "The key difference is that initializing the decoder with a fixed representation uses a single, static summary of the entire input sentence (like the last state or a max-pool), which may miss important parts of the sentence. In contrast, the attention mechanism allows the decoder to dynamically query and weigh different parts of the encoder's states at each step, enabling it to focus on relevant input sections as needed during translation."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen82-slide67/text.txt": [
        {
            "question": "What is the purpose of using softmax in the attention mechanism described in the text?",
            "reference-answers": [
                "The purpose of using softmax in the attention mechanism is to normalize the attention energies so that they sum to one, allowing them to be used as weights to combine the encoder states into a context vector."
            ]
        },
        {
            "question": "What is the role of attention energies in the attention mechanism described?",
            "reference-answers": [
                "The attention energies serve as importance weights that determine how much each encoder state contributes to the context vector. They specify which input states are more important and are normalized to sum to one, allowing them to be used as weights to combine the encoder states into a fixed-size context vector."
            ]
        },
        {
            "question": "What are the inputs to the attention mechanism, and how are the attention energies normalized and used to create the context vector in the described model?",
            "reference-answers": [
                "The inputs to the attention mechanism are the decoder state and all the encoder states, which are concatenations of the left and right RNNs in the bidirectional recurrent neural network. The attention energies are normalized using softmax, ensuring they sum to one. These normalized weights are then used to combine the encoder states, forming the context vector as a fixed-size representation that aggregates the input sentence according to the attention weights."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen83-slide68/text.txt": [
        {
            "question": "What is the main difference in how the decoder operates compared to the encoder in this approach?",
            "reference-answers": [
                "The main difference is that the decoder can access the encoder's output at any time step, allowing it to refer back to the entire encoded sequence as needed."
            ]
        },
        {
            "question": "How did the attention mechanism alter the encoder-decoder approach?",
            "reference-answers": [
                "The attention mechanism changed the encoder-decoder approach by using the encoder's output in all its positions, allowing the decoder to access these positions whenever needed, while the encoder remained bidirectional and the decoder operated similarly but with this new approach."
            ]
        },
        {
            "question": "What modification is made in the encoder-decoder approach to include the attention mechanism?",
            "reference-answers": [
                "The modification involves using the encoder's output at all its positions, allowing the decoder to access this output at every time step through the attention mechanism."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen84-slide67/text.txt": [
        {
            "question": "Question: How many items are accessible at each time step?",
            "reference-answers": [
                "One item is accessible at each time step."
            ]
        },
        {
            "question": "How many items are accessible at each time step?",
            "reference-answers": [
                "One"
            ]
        },
        {
            "question": "How many items are accessible at each time step?",
            "reference-answers": [
                "One"
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen85-slide68/text.txt": [
        {
            "question": "What can the network do at every step regarding the input words?",
            "reference-answers": [
                "The network can at every step attend to any of the words in the input."
            ]
        },
        {
            "question": "What allows the network to attend to any word in the input at every step?",
            "reference-answers": [
                "The network can attend to any word in the input at every step because the context vectors are scaled with a matrix to match the decoder state, and the output projection has access to that information."
            ]
        },
        {
            "question": "What method allows the network to attend to any of the words in the input at every step?",
            "reference-answers": [
                "attention"
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen86-slide69/text.txt": [
        {
            "question": "What does the attention mechanism in the described model learn to do during translation?",
            "reference-answers": [
                "The attention mechanism in the described model learns to align the decoder's processing with the encoder's states, allowing it to focus on the most relevant parts of the source sentence for each output word during translation. It dynamically decides which encoder states are most useful as the decoder builds the target sentence word by word."
            ]
        },
        {
            "question": "What is the primary alignment that the attention mechanism learns in the described neural machine translation model?",
            "reference-answers": [
                "The primary alignment that the attention mechanism learns is an alignment between the states of the encoder and the decoder."
            ]
        },
        {
            "question": "What is the primary focus of the alignment mechanism in attention-based neural machine translation models when generating each output word?",
            "reference-answers": [
                "The primary focus of the alignment mechanism in attention-based neural machine translation models when generating each output word is to align the decoder's state with specific encoder states at each position in time, effectively creating an alignment between the encoder and decoder states."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen88-slide72/text.txt": [
        {
            "question": "To create an appropriate exam question, please provide the specific TEXT content that outlines the differences between the two positions.",
            "reference-answers": [
                "According to the TEXT, outline the main differences between the two positions in terms of their roles, qualifications, and work environments."
            ]
        },
        {
            "question": "Identify and explain the differences between the two positions discussed in the text, focusing on their key responsibilities and required skills.",
            "reference-answers": [
                "The two positions discussed in the text are \"Project Manager\" and \"Team Lead.\" Here are the differences in their key responsibilities and required skills:\n\n**Project Manager:**\n- **Key Responsibilities:** Focuses on the overall planning, execution, and completion of projects. They manage resources, timelines, and budgets, ensuring that projects are delivered on time and within scope. They also communicate with stakeholders and ensure that the project aligns with organizational goals.\n- **Required Skills:** Strong leadership, organizational, and communication skills. They need to be able to handle multiple tasks, make strategic decisions, and manage teams effectively.\n\n**Team Lead:**\n- **Key Responsibilities:** Focuses on guiding and motivating a team to achieve specific goals. They ensure that team members are performing effectively, provide feedback, and help resolve conflicts. They also monitor the quality of work and ensure that tasks are completed to the required standards.\n- **Required Skills:** Strong leadership, technical expertise, and communication skills. They need to be able to mentor team members, provide constructive feedback, and ensure that the team works cohesively.\n\nIn summary, while both positions involve leadership and communication, the Project Manager focuses on the overall project management and strategic planning, whereas the Team Lead focuses on the day-to-day management and guidance of the team."
            ]
        },
        {
            "question": "What does the text mention as the main reason for the differences in positions?",
            "reference-answers": [
                "The main reason for the differences in positions is not directly provided in the text."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen90-slide72/text.txt": [
        {
            "question": "In which year did the Roman Empire split into the Western and Eastern Roman Empires?",
            "reference-answers": [
                "The Roman Empire split into the Western and Eastern Roman Empires in 395 AD."
            ]
        },
        {
            "question": "What activity is the teacher engaged in for the exam?",
            "reference-answers": [
                "I'm unable to answer the question as no text has been provided."
            ]
        },
        {
            "question": "What instruction does the user give to the assistant in the provided text?",
            "reference-answers": [
                "The instruction is to write a question and answer pair about the photosynthesis process."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen91-slide73/text.txt": [
        {
            "question": "What is the nature of the distribution mentioned in the text?",
            "reference-answers": [
                "The distribution mentioned is probabilistic."
            ]
        },
        {
            "question": "What is the relationship between the distribution over positions and the alignment from SMT in the provided text?",
            "reference-answers": [
                "The distribution over positions is probabilistic, while the alignment from SMT is discrete."
            ]
        },
        {
            "question": "What is the probabilistic model describing in terms of distribution and alignment?",
            "reference-answers": [
                "The probabilistic model describes a distribution over these positions, with the alignment from SMT being discrete."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen92-slide74/text.txt": [
        {
            "question": "What grammatical structure is mentioned in the text?",
            "reference-answers": [
                "The grammatical structure mentioned is declarative."
            ]
        },
        {
            "question": "What term is used in the text to describe this type of sentence?",
            "reference-answers": [
                "The term used in the text to describe this type of sentence is **declarative**."
            ]
        },
        {
            "question": "What is described as declarative in the provided text?",
            "reference-answers": [
                "This is declarative."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen93-slide76/text.txt": [
        {
            "question": "In which year did the Roman Empire split into the Western and Eastern Roman Empires?",
            "reference-answers": [
                "The Roman Empire split into the Western and Eastern Roman Empires in 395 AD."
            ]
        },
        {
            "question": "What activity is the teacher engaged in for the exam?",
            "reference-answers": [
                "I'm unable to answer the question as no text has been provided."
            ]
        },
        {
            "question": "What instruction does the user give to the assistant in the provided text?",
            "reference-answers": [
                "The instruction is to write a question and answer pair about the photosynthesis process."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen95-slide76/text.txt": [
        {
            "question": "How does the role of the language model in neural machine translation differ from its role in classical statistical machine translation?",
            "reference-answers": [
                "In neural machine translation, the language model is the primary driver, actively determining which words to produce. It works alongside attention mechanisms, with alignment learned simultaneously during training. In contrast, classical statistical machine translation first required alignment as a prerequisite step before training, after which the language model was used to evaluate and select the best candidates. Thus, the language model in neural systems is integral to the word production process, whereas in classical systems, it functioned as a post-alignment candidate selector."
            ]
        },
        {
            "question": "How does the role of the language model in neural machine translation differ from its role in classical statistical machine translation?",
            "reference-answers": [
                "In neural machine translation, the language model plays an active role in deciding which words to produce, serving as the main driving force. In contrast, in classical statistical machine translation, the language model was used to evaluate and select better translation candidates after alignment, which was a prerequisite step before training."
            ]
        },
        {
            "question": "How does the use of alignment or attention in neural machine translation differ from its role in classical statistical machine translation?",
            "reference-answers": [
                "In classical statistical machine translation, alignment was a prerequisite step performed before the system was trained, using phrase tables to align source and target segments. The language model then discriminated between candidates based on these alignments. In neural machine translation, however, the language model is the main driver, and both alignment and attention are learned simultaneously with translation, effectively as a side effect, without being a separate prerequisite step."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen96-slide77/text.txt": [
        {
            "question": "\"Based on the text, why can't attention be interpreted as word alignment?\"",
            "reference-answers": [
                "The attention cannot be interpreted as word alignment because it represents alignments between the state of the encoder and the state of the decoder, rather than between the words themselves."
            ]
        },
        {
            "question": "What is the relationship between attention and word alignment as explained in the text?",
            "reference-answers": [
                "The text explains that attention and word alignment are not directly related because attention represents alignments between the encoder and decoder states, rather than between words themselves. This is a crucial distinction to keep in mind."
            ]
        },
        {
            "question": "What is the reason that attention peaks one word after the word indicated by the blue squares, and how does this relate to the encoder and decoder states?",
            "reference-answers": [
                "The attention peaks one word after the word indicated by the blue squares because the attention mechanism is not directly aligning words but rather reflecting the alignment between the states of the encoder and decoder. This means the attention is capturing the relationship between the encoder's state and the decoder's state, rather than aligning words directly."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen97-slide78/text.txt": [
        {
            "question": "What did the neural network learn about attention when given the double input?",
            "reference-answers": [
                "The neural network learned that attention could be applied to two diagonals in the input, specifically the original source sentence and the pre-translated candidate, without any explicit rule. It naturally attended to both, sometimes focusing more on the pre-translation as it trusted it."
            ]
        },
        {
            "question": "What did the neural network learn about attention when provided with pre-translations and original source texts?",
            "reference-answers": [
                "The neural network learned that attention can effectively utilize both the source text and pre-translations, focusing on both inputs without explicit rules. It naturally attended to two diagonals, sometimes prioritizing the pre-translation due to its perceived reliability, demonstrating the flexibility and adaptability of attention mechanisms."
            ]
        },
        {
            "question": "What did the neural network learn to follow instead of a single diagonal?",
            "reference-answers": [
                "two diagonals"
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen98-slide79/text.txt": [
        {
            "question": "What does the attention mechanism in the model focus on when generating captions for an image?",
            "reference-answers": [
                "The attention mechanism in the model focuses on specific parts of the image relevant to the elements being described in the caption, attending to different regions as needed for each word."
            ]
        },
        {
            "question": "The model uses a two-dimensional attention mechanism to focus on specific parts of the image while generating captions, allowing it to concentrate on relevant areas for each word and ignore less pertinent details, such as the giraffe when discussing trees.",
            "reference-answers": [
                "The model employs a two-dimensional attention mechanism to focus on specific regions of the image while generating captions. This allows it to concentrate on relevant areas for each word and ignore less pertinent details, such as the giraffe when discussing trees."
            ]
        },
        {
            "question": "What type of attention mechanism does the image captioning system use, and how does it focus on specific areas of the image when generating captions?",
            "reference-answers": [
                "The image captioning system uses a two-dimensional attention mechanism. This means the network can focus on specific regions of the image spatially, attending to areas relevant to the words it is generating. For example, when the model produces the word \"dog,\" it concentrates on the part of the image where the dog is located. Similarly, if describing \"trees\" in an image that includes a giraffe, the model considers the entire image but disregards the giraffe area, demonstrating its ability to selectively attend to pertinent sections."
            ]
        }
    ],
    "nmt-class/lecture03-nmt-seq2seq-attn/screen99-slide81/text.txt": [
        {
            "question": "What is the main reason the attention mechanism's scores seem worse compared to another system when analyzing two papers?",
            "reference-answers": [
                "The main reason the attention mechanism's scores seem worse compared to another system is that the comparison may be misleading due to differences in how the systems are evaluated or the context in which they are applied."
            ]
        },
        {
            "question": "What does the text suggest about comparing scores from two papers using different mechanisms?",
            "reference-answers": [
                "The text suggests that comparing scores from two papers using different mechanisms might make the attention mechanism seem much worse than a vanilla system, but it implies that there is a reason for this which isn't elaborated on."
            ]
        },
        {
            "question": "Question: Why does the attention mechanism appear to perform worse when compared to a vanilla system in the context of the two papers?",
            "reference-answers": [
                "The attention mechanism appears to perform worse than the vanilla system when compared in the context of the two papers because the scores from the comparison suggest that the attention mechanism's performance is notably lower."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen01-slide01/text.txt": [
        {
            "question": "What part of the techniques discussed in the lecture remains relevant even for neural machine translation and were highly critical for the previous state-of-the-art phrase-based approach?",
            "reference-answers": [
                "The alignment techniques, specifically the alignment of documents, sentences, and then words, remain relevant even for neural machine translation and were highly critical for the previous state-of-the-art phrase-based approach."
            ]
        },
        {
            "question": "What role do alignment techniques play in neural machine translation according to the lecture?",
            "reference-answers": [
                "Alignment techniques remain relevant and critical in neural machine translation, as they were for the previous state-of-the-art phrase-based approaches."
            ]
        },
        {
            "question": "The previous state-of-the-art approach to machine translation mentioned in the lecture is the phrase-based approach.",
            "reference-answers": [
                "The previous state-of-the-art approach to machine translation mentioned in the lecture is the phrase-based approach."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen02-slide02/text.txt": [
        {
            "question": "What are the main topics discussed in the talk?",
            "reference-answers": [
                "The main topics discussed in the talk are:\n\n1. **Data**: Focusing on Czech English data and the corpus of CENK.\n2. **Effect of Data on Translation Quality**: Discussing how data influences translation outcomes.\n3. **Techniques**: Including document collection, document alignment, sentence alignment, and word-level alignment.\n4. **Adequacy of Word Alignment**: Examining the sufficiency of word alignment tasks.\n5. **Linguistically Adequate Processing**: Highlighting the importance of linguistically appropriate processing."
            ]
        },
        {
            "question": "**Question:**  \nWhat are the two main topics discussed in the talk?",
            "reference-answers": [
                "The two main topics discussed in the talk are the status of Czech English data and the corpus of CENK, and the techniques involved in alignment and word-level processing."
            ]
        },
        {
            "question": "The two main topics discussed in the talk about the corpus of CENK are the status of Czech-English data and the techniques for processing this data, including document alignment, sentence alignment, word-level alignment, and the adequacy of word alignment tasks.",
            "reference-answers": [
                "The two main topics discussed in the talk about the corpus of CENK are the status of Czech-English data and the techniques for processing this data, including document alignment, sentence alignment, word-level alignment, and the adequacy of word alignment tasks."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen03-slide03/text.txt": [
        {
            "question": "What is the purpose of word alignment in the context of phrase-based machine translation?",
            "reference-answers": [
                "The purpose of word alignment in phrase-based machine translation is to determine which words in one language correspond to words in another, enabling the extraction of consistent phrase translations from a parallel corpus. These alignments, which can be one-to-one or involve multiple words, are used to build a phrase dictionary that helps cover input sentences during translation."
            ]
        },
        {
            "question": "What is the purpose of word alignment in the context of phrase-based machine translation?",
            "reference-answers": [
                "The purpose of word alignment in phrase-based machine translation is to determine which words in the source language correspond to which words in the target language. This alignment helps in constructing a dictionary of phrases, ensuring consistency between the source and target languages, which is essential for extracting these phrases and using them to translate input sentences effectively."
            ]
        },
        {
            "question": "What is the purpose of word alignment in phrase-based machine translation and how does it contribute to the construction of phrase dictionaries?",
            "reference-answers": [
                "The purpose of word alignment in phrase-based machine translation is to determine how words in the source language correspond to words in the target language. It contributes to the construction of phrase dictionaries by extracting phrases that are consistent with these word alignments. This allows the translation model to use these phrases effectively to cover input sentences."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen04-slide04/text.txt": [
        {
            "question": "What does the speaker say they will start with?",
            "reference-answers": [
                "The speaker will start with data."
            ]
        },
        {
            "question": "What is the first step mentioned in the text?",
            "reference-answers": [
                "The first step mentioned is to start with data."
            ]
        },
        {
            "question": "What does the speaker identify as the primary element they will begin with?",
            "reference-answers": [
                "The speaker identifies **data** as the primary element they will begin with."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen05-slide05/text.txt": [
        {
            "question": "What types of user-supplied data were collected for the Czech-English translation project?",
            "reference-answers": [
                "The types of user-supplied data collected for the Czech-English translation project were primarily translations or localizations of open-source projects (e.g., KDE or GNOME), which consisted mainly of labels and items, and corrections to machine translations from the English Wikipedia into Czech via the Davaho project."
            ]
        },
        {
            "question": "What was the purpose of the Davaho project, and what happened to it?",
            "reference-answers": [
                "The purpose of the Davaho project was to translate the English Wikipedia into Czech and allow users to correct the translations. However, the project failed because the translations were not as accurate as expected and users did not provide sufficient corrections. As a result, the project is now inactive."
            ]
        },
        {
            "question": "What were the primary sources of data for the Czech-English translation corpus discussed in the text?",
            "reference-answers": [
                "The primary sources of data for the Czech-English translation corpus were legal texts from the European Union, stories and news commentaries from sources like Reader's Digest, Project Gutenberg, and Palmknihy, user-supplied translations from open-source projects like KDE and GNOME, and the Davaho project, which involved translating the English Wikipedia into Czech with user corrections."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen06-slide06/text.txt": [
        {
            "question": "What does the text indicate about the proportion and nature of community-supplied localization texts in terms of their contribution to the overall data?",
            "reference-answers": [
                "The text indicates that community-supplied localization texts made up 11% of the sentence pairs but only less than 2% of the tokens (words) in the overall data. This suggests that while they contributed a significant number of sentences, they were composed of very short texts, making them less suitable for translating longer, more complex sentences."
            ]
        },
        {
            "question": "What percentage of the sentence pairs were contributed by community-supplied localization texts?",
            "reference-answers": [
                "11%"
            ]
        },
        {
            "question": "What percentage of the data was composed of legal texts?",
            "reference-answers": [
                "The legal texts made up more than two-thirds of the data, which is approximately 67%. \n\nANSWER: The legal texts composed approximately two-thirds, or about 67%, of the data."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen07-slide07/text.txt": [
        {
            "question": "What percentage of the translations were precise and flawless, and what was the vandalism level?",
            "reference-answers": [
                "70% of the translations were precise and flawless, and the vandalism level was below 3%."
            ]
        },
        {
            "question": "What percentage of the translations were considered precise and flawless?",
            "reference-answers": [
                "70%"
            ]
        },
        {
            "question": "What percentage of the translated segments were considered precise and flawless?",
            "reference-answers": [
                "70%"
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen08-slide08/text.txt": [
        {
            "question": "Why does the text suggest that signed-in contributions to KDE and GNOME localizations are of higher quality than anonymous contributions?",
            "reference-answers": [
                "The text suggests that signed-in contributions to KDE and GNOME localizations are of higher quality than anonymous contributions because contributors, being identifiable and part of the community, strive to make a good impression and avoid vandalism, resulting in nearly professional quality."
            ]
        },
        {
            "question": "What does the text suggest about the quality of contributions when people sign in using their own names, and what does it note about the domain segments?",
            "reference-answers": [
                "The text suggests that when people sign in using their own names, the quality of their contributions is very high, almost professional, as they want to make a good impression. It also notes that the domain segments are very short."
            ]
        },
        {
            "question": "What is the level of vandalism in the contributions when people sign in under their own names?",
            "reference-answers": [
                "The level of vandalism in the contributions when people sign in under their own names is obviously no vandalism, resulting in almost professional quality."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen09-slide26/text.txt": [
        {
            "question": "How many tokens were included in the data release, and how many more could be added using community translations of proprietary texts?",
            "reference-answers": [
                "The data release included approximately 21 or 22 million tokens, and an additional 28 million tokens could be added using community translations of proprietary texts."
            ]
        },
        {
            "question": "What did the speaker observe about the availability of data for machine translation training, and what challenges were highlighted regarding licensing?",
            "reference-answers": [
                "The speaker observed that a significant amount of data is available for machine translation training, much of which is already translated and can be found online. However, the licensing conditions for this data are often unclear, posing challenges in determining its legal availability for reuse. The speaker highlights the need for data creators to clearly label their data with permissive licenses to ensure it can be used for good purposes, such as research and non-commercial applications. They also note that laws like the Czech authorship law, which allow data use for research and non-commercial purposes, provide a positive example for others to follow."
            ]
        },
        {
            "question": "What main issue did the authors identify concerning the availability of data for machine translation training, and what did they recommend to address it?",
            "reference-answers": [
                "The main issue the authors identified is that while a large amount of data is available for machine translation training, much of it is not clearly licensed, leading to uncertainty about its usability. They recommended that data providers clearly label their data with permissive licenses to enable broader use for research and non-commercial purposes."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen10-slide10/text.txt": [
        {
            "question": "What issue arises with community-supplied data, and what is the legal status of subtitles according to the text?",
            "reference-answers": [
                "The issue with community-supplied data is that people are not used to labeling their data with proper copyright information, and some of this data has an unclear legal status as it is based on proprietary text. According to the text, subtitles are in a shaky legal position because they are created by translating the content of movies, which requires permission from the creators. However, for research and non-commercial purposes in the Czech Republic, it is considered acceptable to use subtitles."
            ]
        },
        {
            "question": "What is the large data source mentioned in the text, and what is the issue with its legal status?",
            "reference-answers": [
                "The large data source mentioned is movie subtitles. The issue with its legal status is that they are created from proprietary content without permission, leaving them in a shaky legal position."
            ]
        },
        {
            "question": "What issue does the text mention regarding community-supplied data and copyright labeling, using subtitles as an example?",
            "reference-answers": [
                "The text mentions that community-supplied data, particularly subtitles, are often not properly labeled with copyright information, leading to a shaky legal position. Despite their widespread use, these subtitles may not be legally permissible without the rights holders' permission, even though they are acceptable for non-commercial purposes in some contexts like the Czech Republic."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen11-slide11/text.txt": [
        {
            "question": "What are the two axes used to evaluate translation quality in the described method?",
            "reference-answers": [
                "The two axes used to evaluate translation quality are BLEU score and out-of-ocabulary rate."
            ]
        },
        {
            "question": "Question: What are the two axes used to evaluate translation quality, and how do they relate to the performance on the in-domain test set?",
            "reference-answers": [
                "The two axes used to evaluate translation quality are BLEU (Bilingual Evaluation Understudy) scores, which measure automatic translation quality, and the out-of-vocabulary rate, which indicates the percentage of words not covered by the system. For the in-domain test set, higher BLEU scores and lower out-of-vocabulary rates are observed, as the training data (green subset) closely matches the test set, improving translation quality."
            ]
        },
        {
            "question": "What are the two main axes used to evaluate translation quality in the described system, and what do they represent?",
            "reference-answers": [
                "The two main axes used to evaluate translation quality in the described system are BLEU (automatically estimated translation quality of the whole sentences) and out-of-ocabulary rate (the percentage of words of the source that are not covered in any way by the system). BLEU measures the quality of the translation, while the out-of-ocabulary rate indicates how many words the system cannot translate."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen12-slide12/text.txt": [
        {
            "question": "What happens to the BLEU score and translation quality when only relying on the correctly labeled Community Supply Translations?",
            "reference-answers": [
                "The BLEU score will be terribly low and the translation quality will be poor due to a high out-of-vocabulary rate of about 5%, meaning 5% of words will not be translated and the translations will not make much sense."
            ]
        },
        {
            "question": "What does the 5% out-of-vocabulary rate imply about the translations when relying only on correctly labeled Community Supply Translations?",
            "reference-answers": [
                "A 5% out-of-vocabulary (OOV) rate implies that 5% of the words in the translations cannot be processed or translated, resulting in poor translation quality and a low BLEU score."
            ]
        },
        {
            "question": "What is a direct consequence of relying only on the correctly labeled Community Supply Translations, which constitutes a very small fraction of the data?",
            "reference-answers": [
                "A direct consequence is that the BLEU score will be terribly low and the out-of-vocabulary rate will be about 5%, meaning 5% of words will not be translated and the translations will not make much sense."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen13-slide13/text.txt": [
        {
            "question": "What positive outcome resulted from significantly reducing the out-of-vocabulary rate?",
            "reference-answers": [
                "The translation quality is not getting much better."
            ]
        },
        {
            "question": "What has been the effect of subtitle mismatches on translation quality and what improvement has been made in this context?",
            "reference-answers": [
                "The subtitle mismatches have negatively impacted the translation quality, as they do not align with the domain of interest, which has hindered improvements in translation quality. However, there has been a significant reduction in the out-of-vocabulary rate, which is a positive improvement."
            ]
        },
        {
            "question": "What has been significantly reduced in the subtitles, according to the text?",
            "reference-answers": [
                "The out-of-vocabulary rate has been significantly reduced in the subtitles."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen14-slide14/text.txt": [
        {
            "question": "What is the effect on translation quality when including proprietary translations outside the domain?",
            "reference-answers": [
                "Including proprietary translations outside the domain improves translation quality slightly. The word sequences are more common, making the sentences better formed. However, the vocabulary rate is reduced to a similar level."
            ]
        },
        {
            "question": "What effect does including proprietary translations have on translation quality and the vocabulary rate?",
            "reference-answers": [
                "Including proprietary translations improves translation quality slightly and reduces the vocabulary rate to a similar level."
            ]
        },
        {
            "question": "One effect of including proprietary translations out of the domain is that the translation quality improves slightly, with more common word sequences and slightly better-formed sentences, although the vocabulary rate remains similar.",
            "reference-answers": [
                "One effect of including proprietary translations out of the domain is that the translation quality improves slightly, with more common word sequences and slightly better-formed sentences, although the vocabulary rate remains similar."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen15-slide15/text.txt": [
        {
            "question": "What are we achieving when we use all these datasets except for the in-domain training data?",
            "reference-answers": [
                "When using all datasets except the in-domain training data, we achieve BLOF 9 and the lowest autofocular rate so far."
            ]
        },
        {
            "question": "What outcome is achieved when all datasets, except for the in-domain training data, are used?",
            "reference-answers": [
                "When all datasets, except for the in-domain training data, are used, the outcome is BLOF 9 and the lowest autofocular rate so far."
            ]
        },
        {
            "question": "What happens when we use all datasets except for the in-domain training data?",
            "reference-answers": [
                "When we use all datasets except for the in-domain training data, we achieve BLOF 9 and the lowest autofocular rate so far."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen16-slide16/text.txt": [
        {
            "question": "What is the ideal situation for the test set when it comes to training data?",
            "reference-answers": [
                "The ideal situation is when the training data is within the same domain as the test set, ensuring better translation quality."
            ]
        },
        {
            "question": "What is considered the ideal situation for improving translation quality according to the text?",
            "reference-answers": [
                "The ideal situation for improving translation quality is when the training data is from the same domain as the test set being translated."
            ]
        },
        {
            "question": "What is the impact of having training data within the same domain as the test set of interest on translation quality?",
            "reference-answers": [
                "Having training data within the same domain as the test set of interest significantly improves translation quality, as it is the ideal situation for achieving better results."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen17-slide17/text.txt": [
        {
            "question": "What is the consequence of extending your collection with professional translations?",
            "reference-answers": [
                "The consequence of extending your collection with professional translations is that you are not damaging the sentence quality much."
            ]
        },
        {
            "question": "What happens to the sentence quality when you extend your collection with professional translations?",
            "reference-answers": [
                "The sentence quality is not damaged much."
            ]
        },
        {
            "question": "What does the text suggest about extending your collection with professional translations?",
            "reference-answers": [
                "The text suggests that extending your collection with professional translations does not significantly harm the sentence quality."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen18-slide18/text.txt": [
        {
            "question": "What effect does including community-supplied translations have on translation quality according to the text?",
            "reference-answers": [
                "Including community-supplied translations can slightly reduce translation quality as they may introduce imperfections, leading to a less reliable estimate of translation quality."
            ]
        },
        {
            "question": "What is the effect on translation quality when including auto-domain data and community-supplied translations?",
            "reference-answers": [
                "Including auto-domain data and community-supplied translations reduces the vocabulary problem and the auto-vocabulary rate, allowing movement outside of the idle wording of sentences, which can slightly reduce translation quality. Additionally, the automatically estimated translation quality cannot be fully relied upon due to overlapping whiskers."
            ]
        },
        {
            "question": "What is the effect of including auto-domain data and community-supplied translations on translation quality and why can't we fully rely on the automatic quality estimation?",
            "reference-answers": [
                "Including auto-domain data and community-supplied translations reduces the vocabulary problem and the auto-vocabulary rate, allowing the translation to move slightly beyond repetitive phrasing. However, this inclusion can slightly reduce translation quality due to the use of non-ideal wordings. Additionally, the automatic translation quality estimation is uncertain because the whiskers overlap, making it unreliable."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen19-slide19/text.txt": [
        {
            "question": "What is the effectiveness of using in-domain training data for out-of-domain translation tasks?",
            "reference-answers": [
                "The effectiveness of using in-domain training data for out-of-domain translation tasks is limited. Using in-domain training data results in poor translation quality and a mismatched vocabulary for out-of-domain tasks."
            ]
        },
        {
            "question": "What is the main limitation of using in-domain training data for out-of-domain translation tasks, as described in the text?",
            "reference-answers": [
                "The main limitation is that in-domain training data is of little use for out-of-domain translation tasks, resulting in poor translation quality and vocabulary mismatch."
            ]
        },
        {
            "question": "What issue arises when using in-domain training data for out-of-domain translation tasks?",
            "reference-answers": [
                "The issue is that using in-domain training data for out-of-domain translation tasks results in poor translation quality and a vocabulary mismatch, leading to a low BLEU score and a high out-of-vocabulary rate."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen20-slide20/text.txt": [
        {
            "question": "What is the comparison between the BLEU scores of professional translations and community translations as mentioned in the text?",
            "reference-answers": [
                "The BLEU scores for professional translations are higher compared to community translations, as the community translations are less fluent or more out of domain."
            ]
        },
        {
            "question": "What is the reason for the difference in BLEU scores between professional translations and community translations according to the text?",
            "reference-answers": [
                "The difference in BLEU scores is because professional translations are more accurate and fluent, while community translations may be less professional and out of domain, leading to lower scores."
            ]
        },
        {
            "question": "How do the BLEU scores of community translations compare to those of professional translations based on the plots described?",
            "reference-answers": [
                "The BLEU scores for community translations are lower than those for professional translations, as the professional translations yield higher scores."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen21-slide21/text.txt": [
        {
            "question": "What advice does the text provide about using data for translation when you do not know your domain?",
            "reference-answers": [
                "When you do not know your domain, the text advises using all the data you can. This approach ensures the best BLEU score, highest translation quality, and the lowest out-of-vocabulary rate."
            ]
        },
        {
            "question": "What advice is given about using in-domain data when you know your domain?",
            "reference-answers": [
                "When you know your domain, it is advisable to primarily use in-domain data and carefully check if adding more data will be beneficial."
            ]
        },
        {
            "question": "When you know your domain, what is the recommended approach?",
            "reference-answers": [
                "The recommended approach is to primarily use in-domain data and carefully check if adding more data will help."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen22-slide22/text.txt": [
        {
            "question": "What is the total amount of text in Czech and English combined in version 2.0 of Cheng?",
            "reference-answers": [
                "5 gigawords"
            ]
        },
        {
            "question": "What is the total number of words in the Czech and English combined corpus of Cheng 2.0?",
            "reference-answers": [
                "The total number of words in the Czech and English combined corpus of Cheng 2.0 is 5 gigawords."
            ]
        },
        {
            "question": "What is the combined size of the Czech and English corpus in version 2.0 of Cheng?",
            "reference-answers": [
                "The combined size of the Czech and English corpus in version 2.0 of Cheng is 5 gigawords."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen23-slide23/text.txt": [
        {
            "question": "What methods are needed to create a corpus that is automatically sentence and word level aligned?",
            "reference-answers": [
                "The TEXT provided does not contain information about the specific methods needed to create a corpus that is automatically sentence and word level aligned."
            ]
        },
        {
            "question": "What methods are needed to create a corpus that is automatically sentence and word level aligned?",
            "reference-answers": [
                "I'm sorry, but there isn't enough information provided in the text to answer this question. The text only mentions \"the history of Cheng\" and does not include any details about the methods needed to create a corpus that is automatically sentence and word level aligned. If you provide more context or information, I can better assist you."
            ]
        },
        {
            "question": "What are the methods needed to create an automatically aligned corpus at both sentence and word levels?",
            "reference-answers": [
                "To create an automatically aligned corpus at both sentence and word levels, the following methods are needed:\n\n1. **Sentence-Level Alignment**: \n   - Use alignment tools or algorithms to align sentences between the source and target texts. This often involves dynamic programming techniques or other alignment models to find the best matches between sentences in both languages.\n\n2. **Word-Level Alignment**:\n   - After sentence alignment, words within each aligned sentence pair are further aligned. This can be done using word alignment tools or editors that analyze the structure of the sentences and map individual words or phrases between the languages.\n\n3. **Post-Editing**:\n   - Manually review and adjust the aligned sentences and words to ensure accuracy and completeness, especially when automatic alignment may not perfectly capture subtle nuances or rare words.\n\nThese methods together ensure that the corpus is both sentence and word level aligned, facilitating effective linguistic analysis and corpus-based research."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen24-slide24/text.txt": [
        {
            "question": "What tool is mentioned as still being in heavy use for gathering parallel corpora?",
            "reference-answers": [
                "bitexter"
            ]
        },
        {
            "question": "What tool is mentioned as still being in heavy use today for creating parallel corpora?",
            "reference-answers": [
                "The tool mentioned as still being in heavy use today for creating parallel corpora is bitexter."
            ]
        },
        {
            "question": "What is the name of the tool that is still in heavy use today for automatically gathering parallel texts?",
            "reference-answers": [
                "bitexter"
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen25-slide25/text.txt": [
        {
            "question": "What was the main limitation of using Common Crawl for extracting parallel sentences, and what approach was suggested to overcome this issue?",
            "reference-answers": [
                "The main limitation of using Common Crawl for extracting parallel sentences was that it only samples random pages from the web and does not contain full site copies, making it difficult to find both source and target language versions of the same text. To overcome this, the suggested approach was to first identify URLs where both languages are present and then perform a full re-crawl of these specific domains to obtain larger datasets."
            ]
        },
        {
            "question": "What is the main reason for the small number of Czech-English sentence pairs extracted from the Common Crawl data?",
            "reference-answers": [
                "The main reason for the small number of Czech-English sentence pairs extracted from the Common Crawl data is that Common Crawl only samples from the web and does not contain the full version of each website. This means the chance of having both the source and target language versions of the same text is low, resulting in a limited number of extracted sentence pairs."
            ]
        },
        {
            "question": "What was the main challenge faced when using Common Crawl for aligning texts, and what solution was proposed to address this issue?",
            "reference-answers": [
                "The main challenge was that Common Crawl only samples random pages from the web and does not contain full site copies, resulting in a low probability of finding source and target versions of the same text. The proposed solution was to first identify URLs and then perform a full re-crawl of the web domains to obtain larger datasets."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen26-slide26/text.txt": [
        {
            "question": "What is the next step after finding pairs of texts when aligning sentences for any pair of languages without prior knowledge?",
            "reference-answers": [
                "The next step after finding pairs of texts when aligning sentences for any pair of languages without prior knowledge is to align sentences."
            ]
        },
        {
            "question": "How do we align sentences between two languages when we lack prior knowledge of the languages?",
            "reference-answers": [
                "To align sentences between two languages without prior knowledge, you can use statistical methods such as comparing sentence lengths to find similar pairs or employing machine translation models that learn alignments from the data."
            ]
        },
        {
            "question": "What is the next step in aligning sentences for any pair of languages when no prior knowledge of the languages is available?",
            "reference-answers": [
                "The next step is to align sentences by using statistical methods, such as frequency-based approaches, to identify sentence correspondences between the paired texts."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen27-slide27/text.txt": [
        {
            "question": "What is the purpose of the danda symbol in Hindi, and how does it compare to the full stop in English?",
            "reference-answers": [
                "The danda symbol in Hindi, similar to the full stop in English, serves as a sentence delimiter, marking the end of a sentence in the Devanagari script."
            ]
        },
        {
            "question": "What symbol in Hindi text acts as the equivalent of the full stop in English sentences?",
            "reference-answers": [
                "The symbol in Hindi text that acts as the equivalent of the full stop in English sentences is the danda."
            ]
        },
        {
            "question": "What symbol in Devanagari is used to separate sentences, and how does it aid in aligning English and Hindi sentences?",
            "reference-answers": [
                "The symbol in Devanagari used to separate sentences is the danda, which is equivalent to the full stop in English. This aids in aligning English and Hindi sentences by providing a comparable method to break text into individual lines, facilitating easier comparison and translation."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen28-slide28/text.txt": [
        {
            "question": "What is the key observation regarding sentence alignment in translations when comparing two languages?",
            "reference-answers": [
                "The key observation is that sentence length is the critical parallel aspect in translations, allowing alignment by comparing sentence lengths rather than words."
            ]
        },
        {
            "question": "What is the critical parallel aspect observed when aligning sentences from two languages that are translations of each other?",
            "reference-answers": [
                "The critical parallel aspect observed when aligning sentences from two languages that are translations of each other is that the sentence lengths are parallel."
            ]
        },
        {
            "question": "What is the critical parallel aspect observed when aligning sentences in translated texts, as described in the text?",
            "reference-answers": [
                "The critical parallel aspect observed when aligning sentences in translated texts is sentence length."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen29-slide29/text.txt": [
        {
            "question": "What is the name of the standard tool for sentence alignment mentioned in the text, and who is its developer?",
            "reference-answers": [
                "The standard tool for sentence alignment mentioned is not fully named, but its developer is Daniel Varga."
            ]
        },
        {
            "question": "What is the standard tool for sentence alignment mentioned in the text, and what is its relation to the algorithms discussed?",
            "reference-answers": [
                "The standard tool for sentence alignment mentioned is HoneyLine. It is related to the algorithms discussed as it is an implementation of the Galen Church algorithm, which is a baseline approach for sentence alignment using dynamic programming."
            ]
        },
        {
            "question": "What is the basis of the classical algorithm by Galen Church for sentence alignment, and what is the standard tool currently used for this task?",
            "reference-answers": [
                "The basis of the classical algorithm by Galen Church for sentence alignment is a dynamic programming approach similar to the edit distance algorithm, aiming to find the minimum edit distance when grouping sentences into left and right parts without examining individual words. The standard tool currently used for this task is HoneyLine, developed by Daniel Varga, which remains popular due to its reliability and ease of deployment despite improvements in the algorithm and implementation."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen30-slide30/text.txt": [
        {
            "question": "What is the primary tool recommended for high-quality word alignment, and what is a limitation of its alternative?",
            "reference-answers": [
                "The primary tool recommended for high-quality word alignment is Giza++. A limitation of its alternative, Fast_align, is that it can be pretty bad, especially if the training data is small."
            ]
        },
        {
            "question": "Which state-of-the-art word alignment tool is recommended for high-quality results, and what are the limitations of its alternative, Fast Align? Additionally, explain the role of the IBM 1 model in word alignment and how alignment can be directional.",
            "reference-answers": [
                "The state-of-the-art word alignment tool recommended for high-quality results is **Giza++**, despite its age, as it provides better accuracy, especially with smaller training datasets. Its alternative, **Fast Align**, is simpler to use but is generally less accurate, particularly when the amount of training data is limited. \n\nThe **IBM 1 model** plays a foundational role in word alignment by relying on lexical probabilities, which are the probabilities of translating individual words. It establishes many-to-one alignments, meaning each source word can map to multiple target words, but not vice versa. This directional nature of IBM 1 is important, as it creates a one-way link from source to target, and additional techniques are needed to achieve bidirectional, many-to-many alignments."
            ]
        },
        {
            "question": "What is the state-of-the-art tool for unsupervised word alignment and what is the basis of the IBM Model 1 in this process?",
            "reference-answers": [
                "The state-of-the-art tool for unsupervised word alignment is Giza++. The basis of the IBM Model 1 in this process relies on lexical probabilities, which are the probabilities of translations of individual words."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen31-slide31/text.txt": [
        {
            "question": "What is IBM Model 1 particularly good for?",
            "reference-answers": [
                "IBM Model 1 is particularly good for finding a dictionary of word-level translations."
            ]
        },
        {
            "question": "What is a key limitation of IBM Model 1 and what does the teacher recommend regarding it?",
            "reference-answers": [
                "A key limitation of IBM Model 1 is that it disregards the positions of words in sentences, making its lexical alignments inadequate when dealing with multiple copies of the same word. The teacher recommends re-implementing IBM Model 1 in one's preferred programming language and using a specific dataset for this exercise, as it helps in understanding the expectation maximization loop."
            ]
        },
        {
            "question": "What is a key limitation of IBM Model 1 that makes it inadequate for full translation?",
            "reference-answers": [
                "A key limitation of IBM Model 1 is that it does not consider the positions of words in sentences, which makes it inadequate for full translation."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen32-slide31/text.txt": [
        {
            "question": "How old are the slides by Philip Cain?",
            "reference-answers": [
                "The slides by Philip Cain are 11 years old."
            ]
        },
        {
            "question": "How old are the slides by Philip Cain?",
            "reference-answers": [
                "The slides by Philip Cain are 11 years old."
            ]
        },
        {
            "question": "Who created the slides and how old are they?",
            "reference-answers": [
                "The slides were created by Philip Cain and they are 11 years old."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen33-slide32/text.txt": [
        {
            "question": "What method does the text suggest for obtaining lexical translation probabilities?",
            "reference-answers": [
                "The text suggests obtaining lexical translation probabilities by evaluating the likelihood of each translation being appropriate in a specific context, potentially through probability scoring to determine the most probable translation."
            ]
        },
        {
            "question": "Question: How can dictionaries be improved to assist users in selecting the most appropriate translation for a word?",
            "reference-answers": [
                "Dictionaries can be improved by incorporating probability scores for each translation, indicating the likelihood of each word being appropriate in a given context. This would enable users to select the most probable translation efficiently without having to manually verify each option through back-translation."
            ]
        },
        {
            "question": "What method is suggested for determining the correct translation of a word using a dictionary, as demonstrated with the example of 'house'?",
            "reference-answers": [
                "The suggested method is to use back-translation by looking up the proposed target word in the original language to assess its appropriateness in context. For example, checking the meaning of 'shell' in German and determining it is not suitable for the sentence \"I bought a new house.\" Additionally, considering the probability of each translation based on context would help select the most appropriate one."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen34-slide32/text.txt": [
        {
            "question": "What is the most common translation of the word \"house\" in the given corpus?",
            "reference-answers": [
                "The most common translation of the word \"house\" in the given corpus is \"house.\""
            ]
        },
        {
            "question": "What is the most common translation of the German word for \"house\" in the given corpus?",
            "reference-answers": [
                "The most common translation of the German word for \"house\" in the given corpus is **house**."
            ]
        },
        {
            "question": "What is the most common translation of the German word for 'house' according to the given text?",
            "reference-answers": [
                "house"
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen35-slide33/text.txt": [
        {
            "question": "What was defined and what is the goal of collecting it?",
            "reference-answers": [
                "The probability, specifically the lexical probability of an English word given a German word, was defined. The goal is to collect this probability for all possible cases."
            ]
        },
        {
            "question": "What have we defined and want to collect for all possible cases?",
            "reference-answers": [
                "We have defined the lexical probability of the English word given the German word and want to collect this probability for all possible cases."
            ]
        },
        {
            "question": "What have we defined in terms of probability, and what is our aim regarding it?",
            "reference-answers": [
                "We have defined the lexical probability of the English word given the German word, and our aim is to collect this probability for all possible cases."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen36-slide34/text.txt": [
        {
            "question": "What is the purpose of alignment in the context of the corpus described in the text?",
            "reference-answers": [
                "The purpose of alignment is to show how words and sentences in the corpus correspond to each other, particularly indicating which words translate to which in different languages."
            ]
        },
        {
            "question": "What does alignment refer to in the context of the corpus discussed in the text?",
            "reference-answers": [
                "Alignment refers to the process of connecting words or phrases between sentences in a corpus, typically across different languages, to indicate how they correspond or translate to each other."
            ]
        },
        {
            "question": "What is alignment in the context of a corpus, and how does the example provided demonstrate this concept?",
            "reference-answers": [
                "Alignment in the context of a corpus refers to the process of connecting or aligning words and sentences between different parts of the corpus, often across languages. It involves creating lines or connections that show how words in one language correspond to words in another. For example, given the sentence pair \"das Haus ist klein\" in German and \"the house is small\" in English, alignment would indicate which words correspond to each other, such as aligning \"das Haus\" with \"the house\" and \"klein\" with \"small.\" This demonstrates how alignment helps in understanding the relationship between words in different parts of the corpus."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen37-slide35/text.txt": [
        {
            "question": "What is the formal definition of alignment as described in the text?",
            "reference-answers": [
                "Alignment is defined as a function that maps between the English target word at some position into the source word at some other position."
            ]
        },
        {
            "question": "What is the alignment function in the context of machine translation?",
            "reference-answers": [
                "The alignment function is a mapping from the English target word at a particular position to the source word at another position."
            ]
        },
        {
            "question": "How is alignment captured according to the provided information?",
            "reference-answers": [
                "Alignment is captured as a function that maps the English target word at some position to the source word at some other position."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen38-slide36/text.txt": [
        {
            "question": "What is the mapped position for position 5?",
            "reference-answers": [
                "The mapped position for position 5 is position 5."
            ]
        },
        {
            "question": "What does the mapping in the text describe?",
            "reference-answers": [
                "The identity mapping."
            ]
        },
        {
            "question": "What is the term for a mapping where each position is mapped to itself?",
            "reference-answers": [
                "identity mapping"
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen39-slide37/text.txt": [
        {
            "question": "What is the alignment function's capability with regard to word reordering?",
            "reference-answers": [
                "The alignment function allows us to reorder the words in any way."
            ]
        },
        {
            "question": "How does the alignment function handle word reordering?",
            "reference-answers": [
                "The alignment function allows us to reorder the words in any way, so any reordering is okay. For example, \"Klein is the house, the house is small\" can be easily captured by the alignment function."
            ]
        },
        {
            "question": "**Question:**  \nIs any reordering of words allowed by the alignment function?",
            "reference-answers": [
                "Yes, any reordering of words is allowed by the alignment function."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen40-slide38/text.txt": [
        {
            "question": "Question: Why can the translation function produce one-to-many translations?",
            "reference-answers": [
                "The translation function can produce one-to-many translations because multiple words in the target language can map to the same position in the source language. This means that the function does not need to be a bijection, as it easily captures the one-to-many relationship."
            ]
        },
        {
            "question": "Does the function need to be a bijection in this context? Explain why or why not using the information provided.",
            "reference-answers": [
                "No, the function does not need to be a bijection. This is because the function can produce one-to-many translations, where multiple words in the target language can map to the same position in the source language. Therefore, the function does not need to be a bijection as it captures the one-to-many nature of the translations."
            ]
        },
        {
            "question": "What is a reason that a function does not have to be a bijection?",
            "reference-answers": [
                "A function does not have to be a bijection because it can produce one-to-many translations, where multiple words in the target language can map to the same position in the source language."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen41-slide39/text.txt": [
        {
            "question": "What happens to source language words if there is no corresponding target language word?",
            "reference-answers": [
                "If there is no corresponding target language word for a source language word, that source language word will be dropped because there will be no alignment link."
            ]
        },
        {
            "question": "Question: Will dropping the source language words that do not have corresponding target language words cause the target language to lose some of its words?",
            "reference-answers": [
                "No, dropping the source language words that do not have corresponding target language words will not cause the target language to lose any of its words."
            ]
        },
        {
            "question": "What happens to the source language words if none of the target language words maps to them?",
            "reference-answers": [
                "The source language words will be dropped if none of the target language words maps to them."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen42-slide40/text.txt": [
        {
            "question": "How can we indicate that a word in the source sentence has no counterpart in the target language?",
            "reference-answers": [
                "We can map the word to a position of zero in the target language, indicating that it has no counterpart."
            ]
        },
        {
            "question": "What is another method mentioned for handling words in the target language?",
            "reference-answers": [
                "Another method mentioned for handling words in the target language is explicitly dropping words in the target language and mapping them to a position of zero, indicating that they have no counterpart in the other language."
            ]
        },
        {
            "question": "What method is used to explicitly drop words in the target language by mapping them to zero, indicating no counterpart in the other language?",
            "reference-answers": [
                "The method used is explicitly dropping words in the target language and mapping them to zero to indicate that they have no counterpart in the other language."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen43-slide41/text.txt": [
        {
            "question": "What is the definition of the conditional probability of the target sentence and alignment given the source sentence in the IBM Model 1, and how does it incorporate lexical translation probabilities, alignment functions, and normalization?",
            "reference-answers": [
                "The conditional probability of the target sentence and alignment given the source sentence in the IBM Model 1 is defined as the product of the lexical translation probabilities for each aligned word pair, multiplied by a normalization constant. Specifically, for a given source sentence \\( f \\) and target sentence \\( e \\), the probability \\( P(e, a | f) \\) is calculated as:\n\n\\[\nP(e, a | f) = \\frac{1}{\\sum_{a'} P(a' | f)}} \\prod_{i=1}^{l_f} P(e_{a(i)} | f_i)\n\\]\n\nwhere \\( a \\) is the alignment function that pairs words between the source and target sentences, \\( P(e_{a(i)} | f_i) \\) is the lexical translation probability of the target word \\( e_{a(i)} \\) given the source word \\( f_i \\), and the normalization constant ensures that the total probability over all possible alignments sums to 1. The model incorporates lexical translation probabilities by looking up the probability of each word pair in a dictionary, and it uses the alignment function to determine which words are paired together. The normalization constant accounts for the additional possibility of a \"null\" alignment, where a target word may not align to any source word."
            ]
        },
        {
            "question": "Question: \"Explain the IBM Model 1 and its approach to machine translation.\"",
            "reference-answers": [
                "The IBM Model 1 is a generative model designed for the initial step of word-based translation, focusing exclusively on lexical translation. It operates by establishing an alignment between words in the source language and the target language, without considering word positions. Given a foreign sentence and its English counterpart, the model uses an alignment function to determine word correspondences. The probability of the English sentence, conditioned on the source, is computed as the product of the lexical probabilities for each aligned word pair, using a dictionary. Importantly, the model includes a null alignment option, which accounts for cases where a source word may not correspond to any target word, thereby normalizing the probabilities across all possible alignments. This approach allows the model to estimate the conditional probability of the target sentence and its alignment, given the source sentence, by considering all potential alignments and their respective probabilities."
            ]
        },
        {
            "question": "**What is the IBM Model 1 and how does it use lexical translation probabilities, the alignment function, and the null position for normalization in its operation?**",
            "reference-answers": [
                "The IBM Model 1 is a generative model used for word-based translation, focusing on lexical translation probabilities. It operates by aligning words between the source (foreign) sentence and the target English sentence using an alignment function. Each word pair has a probability defined in a dictionary, and the model calculates the overall translation probability by multiplying these lexical probabilities for the aligned words. \n\nThe model includes a normalization constant by considering an additional null position, which allows for the possibility that a source word does not align to any target word. This null position effectively adds one more alignment option (lf + 1) for each word, ensuring that all possible alignments, including the null, are accounted for in the calculation of the conditional probability of the target sentence and alignment given the source sentence."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen44-slide26/text.txt": [
        {
            "question": "How does IBM Model One calculate the probability of the most likely target sentence given a source sentence, using lexical probabilities and word alignments?",
            "reference-answers": [
                "IBM Model One calculates the probability of the most likely target sentence by multiplying the lexical probabilities of each word, considering direct alignments where each source word is matched to a target word monotonically. The probability is found by multiplying the highest probability lexical translation for each word and then applying a normalization factor. This approach ensures that the target sentence with the highest product of these probabilities is selected as the most likely translation."
            ]
        },
        {
            "question": "What does IBM Model One compute for a given source sentence using the lexical probabilities of each word and their alignment?",
            "reference-answers": [
                "IBM Model One computes the probability of the whole target sentence given the source sentence using the lexical probabilities of each word and their alignment."
            ]
        },
        {
            "question": "The source sentence with the highest probability in IBM Model One, given the target sentence \"the house east client,\" is \"does house east client.\" This is because each source word (\"does,\" \"house,\" \"east,\" \"client\") was aligned to the corresponding target word using the highest probability from the lexical translation table, maximizing the overall sentence probability.",
            "reference-answers": [
                "The source sentence \"does house east client\" has the highest probability in IBM Model One given the target sentence \"the house east client\" because each source word (\"does,\" \"house,\" \"east,\" \"client\") was aligned to the corresponding target word using the highest probability from the lexical translation table, thereby maximizing the overall sentence probability."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen45-slide43/text.txt": [
        {
            "question": "What is the main problem described in the text when estimating lexical probabilities from a parallel corpus?",
            "reference-answers": [
                "The main problem is the chicken and egg problem where estimating lexical probabilities from a parallel corpus depends on having alignments, and estimating alignments depends on having lexical probabilities, creating a circular dependency that is hard to break."
            ]
        },
        {
            "question": "What is the \"chicken neck problem\" and how does the text suggest we might resolve it?",
            "reference-answers": [
                "The \"chicken neck problem\" refers to the circular dependency where estimating lexical probabilities requires alignments, and estimating alignments requires lexical probabilities. The text suggests resolving this by breaking the loop, possibly through iterative methods starting with rough estimates."
            ]
        },
        {
            "question": "What issue arises when trying to estimate lexical probabilities from a parallel corpus, and why is it challenging?",
            "reference-answers": [
                "The issue is a circular dependency, referred to as the \"chicken neck problem.\" We need alignments to estimate lexical probabilities, but we also need those probabilities to determine the alignments, creating a loop that is challenging to resolve."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen46-slide44/text.txt": [
        {
            "question": "What is the main iterative process of the EM algorithm as described in the text?",
            "reference-answers": [
                "The main iterative process of the EM algorithm involves initializing the model parameters, using the current model to fill in the gaps in the data (impute missing data), estimating new model parameters from the completed data, and repeating this process until convergence, each time improving the model."
            ]
        },
        {
            "question": "What is the process of the EM algorithm when dealing with incomplete data, as described in the text?",
            "reference-answers": [
                "The EM algorithm works by initializing the model parameters, then using the current model to assign probabilities to the missing data, effectively creating a completed dataset. It then estimates new model parameters from this completed data. This process is repeated, with each iteration improving the model parameters, leading to a better version of the model."
            ]
        },
        {
            "question": "What is the process of initializing and iterating in the EM algorithm to estimate model parameters when there is missing data?",
            "reference-answers": [
                "The process begins by initializing the model parameters, typically done randomly or uniformly. Using these initial parameters, the algorithm assigns probabilities to the missing data, effectively creating an improved version of the data. This completed data is then used to estimate new model parameters in the maximization step. The process repeats, with each iteration improving the model by updating the parameters based on the more accurate completed data, continuing until the parameters converge."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen47-slide45/text.txt": [
        {
            "question": "What is the initial state of lexical probabilities in the described corpus, and how are words linked at the beginning?",
            "reference-answers": [
                "The initial state of lexical probabilities is uniform, meaning each word is equally likely to be linked with any other word. For example, \"can\" is equally likely to be linked with \"la\" or \"maison,\" and \"house\" is equally likely to be linked with \"la\" or \"maison.\" This uniform initialization reflects a lack of prior knowledge, where all possible word pairings are equally probable at the beginning."
            ]
        },
        {
            "question": "What is the translation probability of \"maison\" to \"house\" after processing the given sentences?",
            "reference-answers": [
                "The translation probability of \"maison\" to \"house\" after processing the given sentences is **1.0**."
            ]
        },
        {
            "question": "What were the initial lexical linking probabilities between the French and English words in the example?",
            "reference-answers": [
                "The initial lexical linking probabilities between the French and English words were uniform. Each French word (e.g., \"la\", \"maison\", \"fleur\") was equally likely to be linked with each English word (e.g., \"the\", \"house\", \"flower\"), with no initial bias or preference."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen48-slide46/text.txt": [
        {
            "question": "What does the model notice about the co-occurrence of \"the\" with \"la\" compared to \"meson\" after one iteration?",
            "reference-answers": [
                "The model notices that \"the\" co-occurs with \"la\" more often than it co-occurs with \"meson,\" leading to a stronger alignment link between \"the\" and \"la.\""
            ]
        },
        {
            "question": "Explain how the concept of alignment strength is demonstrated in the given text regarding the co-occurrence of words.",
            "reference-answers": [
                "The concept of alignment strength is demonstrated through the frequency of co-occurrence between words. In the given text, it shows that after one iteration, the model recognizes that certain words co-occur more frequently with \"la\" than with \"meson,\" leading to a stronger alignment link between those words. This illustrates how alignment strength is reinforced based on the repeated occurrence of specific word combinations."
            ]
        },
        {
            "question": "What happens to the alignment link between 'the' and 'la' after one iteration of the model processing the corpus?",
            "reference-answers": [
                "The alignment link between 'the' and 'la' gets stronger."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen49-slide28/text.txt": [
        {
            "question": "Why does the word \"flower\" eventually align with \"flare\" according to the pigeonhole principle?",
            "reference-answers": [
                "The word \"flower\" eventually aligns with \"flare\" because, according to the pigeonhole principle, when there are more words (pigeons) than available alignments (holes), some words must share an alignment. Since \"flower\" is not covered in other sentences, it has no other option but to align with \"flare,\" ensuring the probability mass is distributed, even if it means incorrect pairings."
            ]
        },
        {
            "question": "What principle ensures that words like \"flower\" and \"flare\" align when other words are already matched in the given context?",
            "reference-answers": [
                "The principle that ensures words like \"flower\" and \"flare\" align when other words are already matched is the pigeonhole principle."
            ]
        },
        {
            "question": "What principle explains why the word \"flower\" eventually aligns with \"flare\" according to the text?",
            "reference-answers": [
                "The pigeonhole principle explains why the word \"flower\" eventually aligns with \"flare.\" This principle states that when there are more words than possible alignments, some words must share an alignment, leading \"flower\" to align with \"flare\" due to the necessity of distributing the probability mass."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen50-slide26/text.txt": [
        {
            "question": "What does the convergence process lead to in terms of word alignment?",
            "reference-answers": [
                "The convergence process leads to the correct alignment of words efficiently."
            ]
        },
        {
            "question": "What does the method ensure when it converges?",
            "reference-answers": [
                "The method ensures a global optimum and quick convergence, allowing us to know which words align to which words."
            ]
        },
        {
            "question": "What is achieved when the process converges quickly to a global optimum?",
            "reference-answers": [
                "The alignment of words is achieved when the process converges quickly to a global optimum."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen51-slide26/text.txt": [
        {
            "question": "What method is used to estimate the conditional probability based on the observations in the corpus?",
            "reference-answers": [
                "The method used is the maximum likelihood estimate."
            ]
        },
        {
            "question": "What statistical approach is employed to estimate the conditional probability based on the described method?",
            "reference-answers": [
                "The statistical approach employed is **maximum likelihood estimation**."
            ]
        },
        {
            "question": "What method is used to calculate the conditional probability based on the observed frequencies in the text?",
            "reference-answers": [
                "Maximum likelihood estimate."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen52-slide26/text.txt": [
        {
            "question": "What are the two main steps of the expectation maximization algorithm in IBM Model 1, and what do they involve?",
            "reference-answers": [
                "The two main steps of the expectation maximization algorithm in IBM Model 1 are the **Expectation step** and the **Maximization step**. \n\nIn the **Expectation step**, you apply the model (the dictionary of word translations) to the data (the alignments). Using IBM Model 1's formula, you calculate the probabilities of the source sentence, target sentence, and alignment. This step involves refining the alignments by assigning probabilities to them.\n\nIn the **Maximization step**, you use the observed alignments from the data to update the model parameters. This involves refining the dictionary and the alignment probabilities to better fit the observed data, effectively maximizing the likelihood of the data given the model."
            ]
        },
        {
            "question": "What are the two steps of the Expectation Maximization algorithm, and what does each step involve in the context of the IBM Model 1?",
            "reference-answers": [
                "The Expectation Maximization (EM) algorithm consists of two steps in the context of the IBM Model 1. \n\n1. **Expectation Step (E-step):** In this step, the model (dictionary of word translations) is applied to the data (alignments). The model assigns probabilities to the alignments, effectively refining them based on the current dictionary.\n\n2. **Maximization Step (M-step):** In this step, the alignments obtained from the E-step are used to observe the data and update the model. The model parameters (dictionary) are adjusted to better fit the observed alignments, thereby improving the probabilities of the source-target sentence pairs."
            ]
        },
        {
            "question": "**Question:**  \nWhat are the two main steps of the Expectation Maximization (EM) algorithm, and what does each step involve?",
            "reference-answers": [
                "The two main steps of the Expectation Maximization (EM) algorithm are the Expectation step and the Maximization step. In the Expectation step, you apply the model (the dictionary of word translations) to the data (the alignments), using the model to assign probabilities to the alignments and refine them. In the Maximization step, you use the observed data and the current alignments to update the model parameters."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen53-slide26/text.txt": [
        {
            "question": "Based on the previous text, how detailed can we cover the formulas?",
            "reference-answers": [
                "The level of detail in which the formulas can be covered is not specified in the provided text."
            ]
        },
        {
            "question": "What is a key factor in achieving global energy transition?  \nA) International cooperation  \nB) Technological innovation  \nC) Public awareness campaigns  \nD) Private sector investments",
            "reference-answers": [
                "A) International cooperation"
            ]
        },
        {
            "question": "How does the level of detail achieved in the coverage depend on the factors discussed, and what role do the formulas play in determining this level of detail?",
            "reference-answers": [
                "The level of detail in coverage is influenced by factors such as the complexity of the formulas, the approach taken (theoretical or applied), the context in which the formulas are used, and the intended audience's knowledge level. More complex formulas require a higher level of detail to ensure understanding, while simpler ones may need less. The formulas themselves serve as the foundation, dictating what needs to be explained in detail based on their structure and application."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen54-slide26/text.txt": [
        {
            "question": "Explain the process of re-estimating lexical probabilities in the maximization step of IBM Model One using the given sentence pair and alignment possibilities.",
            "reference-answers": [
                "The process of re-estimating lexical probabilities in the maximization step of IBM Model One involves the following steps:\n\n1. **Expectation Step**: Calculate the probabilities of all possible alignments for the given sentence pair using the initial lexical probabilities. For example, for the sentence pair \"la mezon\" and \"the house,\" compute the probability of each alignment, such as aligning \"la\" with \"the\" and \"mezon\" with \"house.\"\n\n2. **Normalization**: Normalize these probabilities to sum to one, effectively determining the likelihood of each alignment given the source and target sentences. This step converts the IBM Model One probabilities into conditional probabilities of alignments.\n\n3. **Fractional Counts**: Use these normalized probabilities as weights to update the counts of word alignments. For instance, if aligning \"la\" with \"the\" has a higher probability, it will contribute more weight to this alignment.\n\n4. **Re-estimation**: Based on these weighted counts, re-estimate the lexical probabilities. This involves normalizing the counts to ensure that the probabilities sum appropriately. For example, if \"the\" can be aligned with multiple words, the updated probabilities will reflect the weighted counts from the previous step.\n\nThis iterative process uses the given sentence pair and alignment possibilities to refine the lexical probabilities, leading to a better model of word alignments."
            ]
        },
        {
            "question": "What is the most probable alignment for the sentence pair 'la mezon' and 'the house' using IBM Model One, and how is this alignment determined?",
            "reference-answers": [
                "The most probable alignment for the sentence pair 'la mezon' and 'the house' using IBM Model One is 'la' ↔ 'the' and 'mezón' ↔ 'house', with a probability of 0.82. This alignment is determined by calculating the probabilities of all possible alignments, considering the lexical probabilities of the words, and then normalizing these probabilities to find the most likely one. The alignment where 'la' is aligned with 'the' and 'mezón' is aligned with 'house' is the most probable, as it has the highest normalized probability of 0.82."
            ]
        },
        {
            "question": "What is the most probable alignment for the sentence pair \"la mezon the house\" based on the IBM Model One, and how is normalization used to determine this?",
            "reference-answers": [
                "The most probable alignment for the sentence pair \"la mezon the house\" based on the IBM Model One is where \"la\" aligns with \"the\" and \"mezon\" aligns with \"house.\" This alignment has a probability of 0.824 after normalization. Normalization is used to adjust the probabilities of all possible alignments so they sum to one, allowing us to determine the most likely alignment."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen55-slide26/text.txt": [
        {
            "question": "What is the process for computing the alignment probability in IBM Model One, and how does the chain rule apply to it?",
            "reference-answers": [
                "To compute the alignment probability in IBM Model One, we consider all possible alignments between the source and target sentences. The chain rule is applied to express this probability as a fraction. The numerator represents the joint probability of the target sentence and the alignment given the source, while the denominator is the marginal probability of the target sentence given the source, regardless of the alignment. This approach allows us to estimate the fractional counts necessary for the model."
            ]
        },
        {
            "question": "What is the key component used to compute the alignment probability in the IBM Model One?",
            "reference-answers": [
                "The key component used to compute the alignment probability in IBM Model One is the probability of the target sentence given the source sentence, which is computed using the chain rule by considering the product of probabilities for each target word given their alignment to source words."
            ]
        },
        {
            "question": "What are the two main components of the probability fraction used in IBM Model One to compute the alignment, and how do they relate to the alignment of source and target sentences?",
            "reference-answers": [
                "The two main components of the probability fraction used in IBM Model One to compute the alignment are:\n\n1. **The numerator**: This is the probability of the target sentence and the alignment given the source sentence. It reflects how likely the target sentence and the specific alignment are together, considering the source sentence.\n\n2. **The denominator**: This is the probability of the target sentence given the source sentence, regardless of the alignment. It represents the overall likelihood of the target sentence being generated from the source sentence, irrespective of how the alignment is achieved.\n\nThese two components are related because the numerator is divided by the denominator to compute the probability of the alignment given the source and target sentences. This fraction allows us to estimate the alignment by normalizing the joint probability of the target and alignment with respect to the marginal probability of the target."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen56-slide26/text.txt": [
        {
            "question": "What does it mean to sum over all alignments when calculating the probability of a target sentence given the source using IBM Model 1?",
            "reference-answers": [
                "To sum over all alignments when calculating the probability of a target sentence given the source using IBM Model 1 means considering all possible ways words in the target can align with the source. Each alignment is represented as a matrix where each cell indicates which source word corresponds to a target word. For each alignment, the probability is computed by multiplying the lexical probabilities of the words and normalizing based on the sentence lengths. The total probability is obtained by summing these probabilities over all possible alignments."
            ]
        },
        {
            "question": "What does the summation in the formula represent when calculating the probability of the target sentence given the source sentence?",
            "reference-answers": [
                "The summation in the formula represents the process of considering all possible ways the words in the target sentence can align with the words in the source sentence. For each possible alignment, which indicates how target words correspond to source words, we calculate the probability using the IBM Model 1. This involves multiplying the lexical probabilities of the words and then summing these probabilities across all possible alignments to get the overall probability of the target sentence given the source sentence."
            ]
        },
        {
            "question": "What does it mean to sum over all possible alignments when calculating the probability of the target sentence given the source using the IBM Model 1?",
            "reference-answers": [
                "Summing over all possible alignments means considering every possible way the source and target words can be paired. For each pairing, we calculate the probability using the IBM Model 1, which relies on lexical probabilities and normalization. This involves creating a matrix where each cell indicates an alignment between source and target words. Each column in this matrix represents a different alignment, and for each alignment, we compute the probability by multiplying the lexical probabilities of the aligned words and then normalizing based on sentence lengths. The total probability is the sum of these individual probabilities across all possible alignments."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen57-slide26/text.txt": [
        {
            "question": "What does the text imply about the subject it is discussing?",
            "reference-answers": [
                "The text is incomplete and does not provide enough information to determine the subject or its implications."
            ]
        },
        {
            "question": "What is the teacher about to discuss based on the given text?",
            "reference-answers": [
                "The text provided is incomplete, making it impossible to determine the specific topic the teacher is about to discuss."
            ]
        },
        {
            "question": "What is the speaker referring to with \"this\" in the text?",
            "reference-answers": [
                "There is not enough information in the provided text to determine what 'this' refers to."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen58-slide26/text.txt": [
        {
            "question": "What is the main topic discussed in this slide?",
            "reference-answers": [
                "The main topic discussed in this slide is unclear or not provided."
            ]
        },
        {
            "question": "What is the main topic or subject discussed in this slide?",
            "reference-answers": [
                "The main topic cannot be determined with the provided text."
            ]
        },
        {
            "question": "What is the speaker implying about the slide?",
            "reference-answers": [
                "The speaker is questioning the relevance or value of the slide."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen59-slide26/text.txt": [
        {
            "question": "What does IBM Model 1 use according to the chain rule as mentioned in the text?",
            "reference-answers": [
                "IBM Model 1 uses the lexical translation probabilities and the full probability of the target given the source across all alignments according to the chain rule."
            ]
        },
        {
            "question": "What is the probability of alignment between source and target sentences in IBM Model 1, and how does it use the chain rule to incorporate lexical translation probabilities and the full probability of the target given the source across all alignments?",
            "reference-answers": [
                "The probability of alignment between source and target sentences in IBM Model 1 is calculated using the chain rule, which allows us to factor the joint probability into a product of conditional probabilities. Specifically, the probability is the product of the lexical translation probabilities (the probability of each target word given its corresponding source word) and the full probability of the target sentence given the source sentence across all possible alignments. This approach ensures that the model accounts for all possible alignments by considering the alignment probability and the translation probabilities of individual word pairs."
            ]
        },
        {
            "question": "The probability of the alignment given the source and target sentences in the IBM Model 1 is derived using the chain rule, which factors the joint probability into a product of conditional probabilities. Specifically, the alignment probability is expressed as the product of translation probabilities for each target word given its aligned source word, considering all possible alignments. This can be represented as:\n\nP(alignment) = ∏_{j=1}^{n} P(y_j|x_i), \n\nwhere y_j is the j-th target word aligned to the i-th source word x_i. This formula encapsulates the chain rule's application in breaking down the alignment probability into a sequence of conditional probabilities, essential for calculating the full target probability given the source in IBM Model 1.",
            "reference-answers": [
                "The probability of the alignment given the source and target sentences in the IBM Model 1 is derived using the chain rule. This breaks down the joint probability into a product of conditional probabilities, specifically the translation probabilities for each target word given its aligned source word. The formula is:\n\nP(alignment) = ∏_{j=1}^{n} P(y_j|x_i),\n\nwhere y_j is the j-th target word aligned to the i-th source word x_i. This approach is essential for calculating the full probability of the target given the source in IBM Model 1."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen60-slide26/text.txt": [
        {
            "question": "Why is having a sum in the denominator problematic when considering all possible alignments?",
            "reference-answers": [
                "Having a sum in the denominator is problematic because it complicates the expression, making it difficult to work with due to the complexity of handling sums in the denominator."
            ]
        },
        {
            "question": "Why is summing over all possible alignments problematic when placed in the denominator?",
            "reference-answers": [
                "Summing over all possible alignments is problematic when placed in the denominator because it results in a complex expression that is difficult to work with mathematically."
            ]
        },
        {
            "question": "Why is summing over all possible alignments problematic?",
            "reference-answers": [
                "Summing over all possible alignments is problematic because it results in a sum in the denominator, which is difficult to work with."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen61-slide26/text.txt": [
        {
            "question": "What method is described for estimating the probability of the target sentence given the source?",
            "reference-answers": [
                "The method described involves swapping sums and products through algebraic manipulation, specifically by factoring out elements that appear multiple times in the calculation. This results in estimating the probability of the target sentence given the source by multiplying, across all target positions, the sum over all source words that the target can map to, considering their lexical probabilities."
            ]
        },
        {
            "question": "How does factoring out elements that appear multiple times help in swapping sums and products to estimate the probability of a target sentence given the source?",
            "reference-answers": [
                "Factoring out elements that appear multiple times allows us to swap sums and products by simplifying the algebraic expression. This process enables us to interleave the sums and products, making it easier to estimate the probability of the target sentence given the source by multiplying over all the target positions and summing over all the source words they can be mapped to."
            ]
        },
        {
            "question": "What is the method described for swapping sums and products to estimate the probability of the target sentence given the source?",
            "reference-answers": [
                "The method involves factoring out elements that appear multiple times in the calculation, allowing the sums and products to be swapped through algebraic manipulation. This results in estimating the probability of the target sentence given the source by multiplying, over all target positions, the summation over all source words that the target can be mapped to, considering their lexical probabilities."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen62-slide26/text.txt": [
        {
            "question": "What is the trick mentioned, and how does the formula demonstrate the swapping of the sum and the product?",
            "reference-answers": [
                "The trick is that the sum and the product are swapped in the formula, and by reviewing the formula, you can see how this exchange occurs."
            ]
        },
        {
            "question": "In the formula provided, where was the sum incorrectly swapped with the product, and how does this substitution affect the formula's outcome?",
            "reference-answers": [
                "The sum was incorrectly swapped with the product in the formula, and this substitution affects the formula's outcome by altering the result, as the sum and product were interchangeably used in a way that may lead to an incorrect or different result than intended."
            ]
        },
        {
            "question": "What is the trick mentioned in the text regarding the sum and product?",
            "reference-answers": [
                "The trick involves swapping the sum and the product in the formula."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen64-slide26/text.txt": [
        {
            "question": "What mathematical concept is described in the text where the sum of all pairs, considered as a full graph, can be regrouped into a product of summations?",
            "reference-answers": [
                "The mathematical concept described is the distributive property, where the sum of all pairs can be regrouped into a product of summations."
            ]
        },
        {
            "question": "What is the regrouped form of the sum of all pairs, where each pair is a multiplication of things?",
            "reference-answers": [
                "The regrouped form is a product of summations."
            ]
        },
        {
            "question": "What is the result of regrouping the sum of all pairs, each being a multiplication of elements, into a product of summations?",
            "reference-answers": [
                "The result is that the sum of all pairs, each being a multiplication of elements, can be regrouped into a product of summations."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen66-slide26/text.txt": [
        {
            "question": "**Factors influencing the trustworthiness of alignment points:**\n1. **Alignment Probabilities:** The likelihood of each alignment point is determined by the alignment matrix, which represents the probability of each word in the source sentence aligning with each word in the target sentence.\n2. **Lexical Translation Probabilities:** The trustworthiness is influenced by the lexical probabilities, which are derived from the alignment matrix and refined through iterative processes.\n3. **Chain Rule Application:** The chain rule is used to calculate the probability of the entire alignment by multiplying the individual alignment probabilities.\n4. **All Possible Alignments:** The method considers all possible alignments between the source and target sentences, ensuring a comprehensive evaluation.\n5. **Tractability Through Normalization:** The computation becomes manageable due to the cancellation of normalizations, allowing for efficient product and sum operations after division.\n\nThese factors collectively determine the trustworthiness of alignment points in the IBM Model 1.",
            "reference-answers": [
                "**Factors influencing the trustworthiness of alignment points:**\n\n1. **Alignment Probabilities:** The likelihood of each alignment point is determined by the alignment matrix, which represents the probability of each word in the source sentence aligning with each word in the target sentence.\n2. **Lexical Translation Probabilities:** The trustworthiness is influenced by the lexical probabilities, which are derived from the alignment matrix and refined through iterative processes.\n3. **Chain Rule Application:** The chain rule is used to calculate the probability of the entire alignment by multiplying the individual alignment probabilities.\n4. **All Possible Alignments:** The method considers all possible alignments between the source and target sentences, ensuring a comprehensive evaluation.\n5. **Tractability Through Normalization:** The computation becomes manageable due to the cancellation of normalizations, allowing for efficient product and sum operations after division.\n\nThese factors collectively determine the trustworthiness of alignment points in the IBM Model 1."
            ]
        },
        {
            "question": "How does the alignment probability between source and target sentences get computed using the chain rule and the IBM model 1, and why does swapping the product and sum help in making the computation tractable?",
            "reference-answers": [
                "The alignment probability between source and target sentences is computed using the chain rule, which breaks down the joint probability into a product of conditional probabilities as per the IBM Model 1. This model assumes that each source word independently aligns to a target word. Swapping the product and sum simplifies the computation by allowing the normalization terms to cancel out, making the process tractable. This swapping enables the product to be computed after division, ensuring efficient and manageable calculations."
            ]
        },
        {
            "question": "Explain how the alignment probabilities between source and target sentences are computed, focusing on the role of normalization and the simplification achieved by swapping product and sum in the IBM Model 1.",
            "reference-answers": [
                "The alignment probabilities between source and target sentences in the IBM Model 1 are computed using the chain rule, which simplifies the calculation by considering the alignment probabilities between individual words. The key simplification involves swapping the product and sum, allowing us to compute the product of probabilities across all possible alignments before applying the sum. This approach ensures that normalization factors cancel out, making the computation tractable. The result is a refined fractional count of alignment probabilities, which are derived from the lexical translation probabilities between source and target words in the given sentence pair."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen67-slide26/text.txt": [
        {
            "question": "What is the purpose of using the fractional counts derived from the estimated likelihood of word alignment between source and target sentences?",
            "reference-answers": [
                "The purpose is to use these fractional counts for creating the model or further processing."
            ]
        },
        {
            "question": "What is the purpose of using the fractional counts derived from the estimated alignment likelihood between source and target sentences?",
            "reference-answers": [
                "The purpose is to use these fractional counts to create probability estimates for the translation model, helping to predict target words based on source words."
            ]
        },
        {
            "question": "What is the purpose of creating fractional counts based on the alignment likelihood between words in source and target sentences?",
            "reference-answers": [
                "The purpose of creating fractional counts based on the alignment likelihood between words in source and target sentences is to represent the estimated likelihood of such alignments."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen68-slide26/text.txt": [
        {
            "question": "What is the method for determining the maximum likelihood estimate mentioned in the text?",
            "reference-answers": [
                "The maximum likelihood estimate is determined by dividing the fractional count of the target word by the sum of all fractional counts. \n\nAnswer: The maximum likelihood estimate is obtained by dividing the fractional count of the target word by the sum of all fractional counts."
            ]
        },
        {
            "question": "What are the two components needed to calculate the maximum likelihood estimate, and how are they used in the calculation?",
            "reference-answers": [
                "The two components needed to calculate the maximum likelihood estimate are the sum of fractional counts and the fractional count that falls to the particular target word. They are used by dividing the fractional count of the target word by the sum of all fractional counts."
            ]
        },
        {
            "question": "What is the process used to determine the maximum likelihood estimate of the probability of a target word based on its co-occurrence with other words?",
            "reference-answers": [
                "To determine the maximum likelihood estimate of the probability of a target word, we first consider all the words with which the target word has been observed. We then calculate the sum of all fractional counts of these co-occurrences. The fractional count specifically associated with the target word is then identified. Finally, we divide this target word's fractional count by the total sum of all fractional counts, yielding the maximum likelihood estimate."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen69-slide26/text.txt": [
        {
            "question": "What is the homework assignment that the teacher mentioned in the text?",
            "reference-answers": [
                "The homework assignment is to implement the provided pseudocode in your favorite programming language."
            ]
        },
        {
            "question": "What is the task that the homework assignment requires the student to perform?",
            "reference-answers": [
                "Implement the pseudocode in your favorite programming language."
            ]
        },
        {
            "question": "What is the task assigned as homework according to the provided text?",
            "reference-answers": [
                "The task is to implement the provided pseudocode in your favorite programming language."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen71-slide26/text.txt": [
        {
            "question": "What specific improvement occurs in the alignment of 'White' with 'Bili' after the first iteration of the EM algorithm as described in the text?",
            "reference-answers": [
                "After the first iteration, the alignment of 'White' with 'Bili' improves because the probability of this alignment increases compared to others. Specifically, 'White' is now more likely to align with 'Bili' than with other words, as it was observed in two out of the three sentence pairs."
            ]
        },
        {
            "question": "What does the model assume about word translations before analyzing the corpus?",
            "reference-answers": [
                "The model assumes that all possible word translations are equally likely at the beginning, with uniform probabilities."
            ]
        },
        {
            "question": "What does the text illustrate about the EM algorithm in the context of machine translation?",
            "reference-answers": [
                "The text illustrates how the EM algorithm iteratively improves the alignment of words in machine translation by using the corpus to refine probabilities. It starts with uniform probabilities, then updates these based on observed alignments, gradually increasing the likelihood of correct word correspondences. For example, after initial observations, \"White\" is more likely linked to \"Bili\" than to other words, and \"Cerny\" is ruled out as a translation for \"White.\" The algorithm refines these probabilities with each iteration, leading to a more accurate model."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen73-slide26/text.txt": [
        {
            "question": "What are the key features added by IBM Model 1, Model 2, and Model 3 respectively?",
            "reference-answers": [
                "The key features added by IBM Model 1, Model 2, and Model 3 are lexical translation, absolute reordering models, and the ability to translate a single word into multiple words respectively."
            ]
        },
        {
            "question": "What key feature does the IBM Model 3 introduce compared to Model 1 and Model 2?",
            "reference-answers": [
                "The key feature introduced by IBM Model 3 is the ability to translate a single word into multiple words, which is not present in Models 1 and 2."
            ]
        },
        {
            "question": "Question: What are the key features introduced in IBM Model 2 and Model 3 compared to Model 1?",
            "reference-answers": [
                "IBM Model 2 introduces absolute reordering models, considering word positions (beginning, end, middle), while Model 3 allows translating a single word into multiple words."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen74-slide26/text.txt": [
        {
            "question": "What is the primary concept behind IBM Model 4 as described in the text?",
            "reference-answers": [
                "The primary concept behind IBM Model 4 is that it uses a fertility model to determine how many target words each source word corresponds to, a lexical translation model to assign translation probabilities, and a distortion model to allow word reordering."
            ]
        },
        {
            "question": "What are the three main components of the IBM Model 4 as described in the text, and how do they contribute to the process of word alignment in translation?",
            "reference-answers": [
                "The three main components of the IBM Model 4, as described in the text, are:\n\n1. **Fertility Model**: This component determines how many target words correspond to each source word. For example, it might assign three target words to translate \"slap.\" This helps in preparing slots for the translation, which is crucial for aligning words correctly.\n\n2. **Lexical Translation Probabilities**: This part handles the translation of individual words. For instance, \"Mary\" is most likely translated to \"Maria.\" These probabilities are based on the same entry in the table, ensuring consistency in translation.\n\n3. **Distortion Model**: This allows for reordering of words based on context, ensuring that the translated words fit naturally in the target sentence. It helps in aligning words correctly by considering their order and context.\n\nThese components work together to enable word alignment in translation by preparing slots for target words, providing lexical translations, and allowing for reordering to fit the target language structure."
            ]
        },
        {
            "question": "What are the three main components of the IBM Model 4 and what do they do?",
            "reference-answers": [
                "The IBM Model 4 consists of three main components:\n\n1. **Fertility Model**: This model assigns probabilities to source words, determining how many target words each source word can correspond to. For example, it might assign a three-word translation probability to the word \"slap.\"\n\n2. **Lexical Translation Model**: This model provides probabilities for translating individual words from the source language to the target language. For instance, it might indicate that \"Mary\" is most likely translated to \"Maria.\"\n\n3. **Distortion Model**: This model allows for the reordering of words in the target sentence beyond the constraints of the fertility model. It considers the context and possible word arrangements, ensuring that the translated sentence is both syntactically and semantically correct."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen75-slide26/text.txt": [
        {
            "question": "What was mentioned alongside the IBM Model 4 in the text?",
            "reference-answers": [
                "the..."
            ]
        },
        {
            "question": "What is the IBM Model 4 mentioned in the text?",
            "reference-answers": [
                "The IBM Model 4 is mentioned, but no further details are provided."
            ]
        },
        {
            "question": "What is one of the items mentioned in the text?",
            "reference-answers": [
                "IBM Model 4"
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen76-slide26/text.txt": [
        {
            "question": "What is the purpose of using the intersection of alignments when extracting a parallel dictionary for human use?",
            "reference-answers": [
                "The purpose of using the intersection of alignments when extracting a parallel dictionary for human use is to rely on the reliable link, ensuring that the translations are consistent and accurate."
            ]
        },
        {
            "question": "What are the two main methods used to combine alignments (intersection and union) and what are their purposes in the context of creating parallel dictionaries and phrase-based machine translation systems? Additionally, explain the \"grow-divide\" approach and mention the optimization technique used in principled algorithms.",
            "reference-answers": [
                "The two main methods used to combine alignments are **intersection** and **union**. The **intersection** is used to create reliable parallel dictionaries by ensuring that both alignments (English to Spanish and Spanish to English) agree on the same points, making it more accurate for human use. On the other hand, the **union** is used for phrase-based machine translation systems, as it provides a broader coverage of possible alignments.\n\nThe **grow-divide** approach is an intermediate method that aims to enlarge the intersection without reaching the full union. It is a heuristic used to balance reliability and coverage in the creation of phrase-based translation systems. \n\nPrincipled algorithms use an optimization technique called **minimum edge cover** to improve the quality of alignments."
            ]
        },
        {
            "question": "What are the two methods discussed in the text for combining alignments when extracting parallel dictionaries or phrase translations?",
            "reference-answers": [
                "The two methods discussed are **intersection** and **a method between intersection and union** (referred to as \"grow dive final\")."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen77-slide26/text.txt": [
        {
            "question": "What is the main strategy described in the text for the Grodai final exam regarding the inclusion of points in the Symmetrisation heuristics?",
            "reference-answers": [
                "Include only points that are already in close vicinity from the intersection ones."
            ]
        },
        {
            "question": "What is the key characteristic of points included in the Symmetrisation heuristics for the Grodiad final?",
            "reference-answers": [
                "The key characteristic is that the points included are those already in close vicinity to the intersection points."
            ]
        },
        {
            "question": "The purpose of including only points that are already in close vicinity from the intersection ones in the Symmetrisation heuristics for the Grodiag final is to ensure a more accurate, efficient, and symmetric result by focusing on relevant nearby points.",
            "reference-answers": [
                "The purpose of including only points that are already in close vicinity from the intersection ones in the Symmetrisation heuristics for the Grodiag final is to ensure a more accurate, efficient, and symmetric result by focusing on relevant nearby points."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen78-slide26/text.txt": [
        {
            "question": "What type of words were the hardest for humans to align during the word alignment task, and why?",
            "reference-answers": [
                "The hardest words for humans to align during the word alignment task were auxiliary words, such as \"to, the, of, a\" in English and \"comma, sev, na\" in Czech. These words were difficult to align because they often did not have a direct counterpart in the target language."
            ]
        },
        {
            "question": "What type of words were the most challenging for humans to align in the comparison between English and Czech, as observed in the text?",
            "reference-answers": [
                "The most challenging words for humans to align were auxiliary words, such as 'to', 'the', 'of', 'a' in English and 'comma', 'sev', 'na' in Czech, which often lack direct counterparts in the target language."
            ]
        },
        {
            "question": "Question: According to the text, which type of words presented the greatest difficulty for humans when performing word alignment?",
            "reference-answers": [
                "Auxiliary words, such as \"to,\" \"the,\" \"of,\" \"a\" in English and \"comma,\" \"sev,\" \"na\" in Czech, which do not have direct counterparts in the target language, presented the greatest difficulty for humans during word alignment."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen79-slide26/text.txt": [
        {
            "question": "\"What is the main message about the importance of human agreement in the context of improving alignment algorithms based on the experiment described?\"",
            "reference-answers": [
                "The main message is that human agreement is crucial for improving alignment algorithms. If the task is ill-defined and humans do not agree on the correct alignments, any improvements in the algorithm will not yield measurable benefits. Therefore, before attempting to enhance alignment algorithms, it is essential to ensure the task is well-defined and that there is high inter-annotator agreement. Only in a well-defined task can algorithmic improvements lead to meaningful enhancements in performance."
            ]
        },
        {
            "question": "What is the main conclusion about the necessity of a well-defined task and inter-annotator agreement for improving automatic alignment algorithms, as discussed in the text?",
            "reference-answers": [
                "The main conclusion is that a well-defined task with high inter-annotator agreement is essential for improving automatic alignment algorithms. Without clear human agreement on the task, improvements in the algorithm will not yield measurable benefits."
            ]
        },
        {
            "question": "What is the main message about improving machine learning algorithms according to the teacher?",
            "reference-answers": [
                "The main message is that improving machine learning algorithms is only effective when the task is well-defined and there is agreement among annotators. Without a clear definition of the task and human consensus, improvements in algorithms will not lead to measurable benefits. Therefore, it is crucial to focus on defining the task clearly and ensuring inter-annotator agreement before attempting to enhance the algorithms."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen80-slide26/text.txt": [
        {
            "question": "What are the two main types of alignment problems observed between English and Chinese, and what percentages do they account for?",
            "reference-answers": [
                "The two main types of alignment problems observed between English and Chinese are:  \n1. **No counterpart in Chinese for the English article**, which accounts for **96%** of alignment points.  \n2. **Role equivalent pairs that are not lexical equivalents**, such as \"was\" aligning with a passive marker in Chinese, which accounts for the remaining **4%**."
            ]
        },
        {
            "question": "What are the two main types of alignment issues discussed in the text when aligning English and Chinese, and how are they characterized?",
            "reference-answers": [
                "The two main types of alignment issues discussed are:\n1. **Missing counterparts**: When an English word or phrase has no direct equivalent in Chinese, making alignment impossible or meaningless.\n2. **Role-equivalent pairs**: When words align based on their grammatical or functional roles rather than being direct lexical equivalents, such as \"was\" in English aligning with a passive marker in Chinese, which is not a direct translation."
            ]
        },
        {
            "question": "The two types of alignment problems mentioned in the text are:\n1. **Missing Counterparts**: 96% of alignment points in English definitions have no counterpart in Chinese, making such alignments a bad question.\n2. **Non-Lexical Equivalents**: Role-equivalent pairs that are not direct lexical translations, such as \"was\" in English not being a direct translation of the Chinese passive marker, leading to alignment issues despite superficial similarity.",
            "reference-answers": [
                "The two types of alignment problems mentioned in the text are:\n1. **Missing Counterparts**: 96% of alignment points in English definitions have no counterpart in Chinese, making such alignments a bad question.\n2. **Non-Lexical Equivalents**: Role-equivalent pairs that are not direct lexical translations, such as \"was\" in English not being a direct translation of the Chinese passive marker, leading to alignment issues despite superficial similarity."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen81-slide26/text.txt": [
        {
            "question": "What was the significant observation made when two groups independently annotated the same test set?",
            "reference-answers": [
                "The significant observation was that two groups, working independently without knowing each other, both defined the same rules for annotators, resulting in a three-way annotation of the same dataset. This demonstrated that when a shared theory or linguistic approach exists within a community, independent groups can perform the same task consistently."
            ]
        },
        {
            "question": "What observation did the researchers make after two groups independently annotated the same test set?",
            "reference-answers": [
                "The researchers observed that two independent groups, without knowing each other, applied the same annotation rules to the same test set, resulting in a three-way annotation. This showed that the linguistic approaches and shared theory within their community led to consistent annotation practices."
            ]
        },
        {
            "question": "What is the translation of the Czech word \"nemyslím\" into English, and how is it analyzed in terms of its components?",
            "reference-answers": [
                "The Czech word \"nemyslím\" translates to \"I do not think\" in English. It is analyzed as being composed of three components: \"I,\" \"do not,\" and \"think.\""
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen82-slide26/text.txt": [
        {
            "question": "What is the purpose of the tectogrammatic layer in the context of syntactic representation?",
            "reference-answers": [
                "The purpose of the tectogrammatic layer is to hide auxiliary words as node attributes in the deep syntactic representation, making it easier to align the nodes in the trees by focusing on content-bearing words and their corresponding structures."
            ]
        },
        {
            "question": "What theory involves marking content-bearing words and hiding auxiliary words as node attributes in syntactic trees to facilitate alignment of nodes?",
            "reference-answers": [
                "The theory is the tectogrammatic layer. In this theory, content-bearing words are marked, and auxiliary words are hidden as node attributes in syntactic trees, facilitating the alignment of nodes."
            ]
        },
        {
            "question": "What is the role of auxiliaries in the tectogrammatic layer as described in the text?",
            "reference-answers": [
                "In the tectogrammatic layer, auxiliaries are auxiliary words that are hidden as node attributes within the deep syntactic representation. They are not prominently displayed in the trees but are part of the structure that helps align the nodes."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen83-slide26/text.txt": [
        {
            "question": "What was the main disadvantage of redefining the task by aligning texogrammatic nodes instead of words?",
            "reference-answers": [
                "The main disadvantage was that the task became language-dependent, requiring the definition of a texogrammatic layer for all languages and reliance on complex tools for annotation."
            ]
        },
        {
            "question": "What was the main strategy used by the colleague to improve machine translation, and what were the key outcomes?",
            "reference-answers": [
                "The main strategy was aligning \"texogrammatic nodes\" instead of words, which improved alignment accuracy and human agreement, facilitated the extraction of a dictionary for deep syntactic machine translation, and allowed for better-defined tasks. However, it introduced language dependency and required complex annotation tools."
            ]
        },
        {
            "question": "What was the main downside of redefining the task to align texogrammatic nodes instead of words?",
            "reference-answers": [
                "The main downside was that it became language-dependent, requiring the definition of a texogrammatic layer for each language and reliance on complex tools for annotation."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen84-slide26/text.txt": [
        {
            "question": "What is the primary mechanism by which LEAF links words across different languages?",
            "reference-answers": [
                "The primary mechanism by which LEAF links words across different languages is by aligning headwords (content-bearing words) across languages, while auxiliaries (non-headwords) are kept within the given language."
            ]
        },
        {
            "question": "What is the main assumption behind the LEAF approach in distinguishing between words?",
            "reference-answers": [
                "The main assumption is that headwords can be linked across languages, while auxiliaries are specific to a given language and are attached within that language."
            ]
        },
        {
            "question": "What is the distinguishing feature of the LEAF approach and how does it handle the alignment of words across languages?",
            "reference-answers": [
                "The distinguishing feature of the LEAF approach is its ability to explicitly differentiate between content-bearing words (headwords) and auxiliaries (non-headwords) in a given text. It handles the alignment of words across languages by linking only the headwords, which are content-carrying words, across different languages. Meanwhile, auxiliaries, which are not headwords and specific to a given language, are always attached within the same language. This approach allows for the deletion or insertion of headwords, making it linguistically more adequate."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen85-slide26/text.txt": [
        {
            "question": "What was the main use of word alignment before the advent of Neural Machine Translation, and why were they problematic from both linguistic and statistical perspectives?",
            "reference-answers": [
                "The main use of word alignment before Neural Machine Translation was in phrase-based machine translation. However, they were problematic linguistically because word alignments do not account for phrases, which are more natural units of meaning, leading to inaccuracies. Statistically, word alignments were problematic because they were not in alignment with the phrases actually used, resulting in ill-defined probabilities."
            ]
        },
        {
            "question": "**Question:** What were the main issues with word alignments in machine translation before the advent of NeuralMT, and why did this lead to their eventual abandonment?",
            "reference-answers": [
                "The main issues with word alignments in machine translation before the advent of NeuralMT were their problematic nature from both linguistic and statistical perspectives. Linguistically, word alignments failed to account for how words combine into phrases, which is crucial for accurate translation. Statistically, these alignments led to ill-defined probabilities because they were not directly linked to the phrases used in translation. Despite attempts to transition from word alignments to phrase-based alignment using a phrase-based model, the resulting improvements in translation quality were insufficient, leading to their eventual abandonment."
            ]
        },
        {
            "question": "Why did phrase-based models move away from using word alignment despite having a technique to convert word alignments to phrase alignments?",
            "reference-answers": [
                "Phrase-based models moved away from using word alignment despite having a technique to convert word alignments to phrase alignments because the technique did not bring enough improvement in translation quality."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen86-slide26/text.txt": [
        {
            "question": "How does a better translation of a text affect its alignment, as illustrated by the example given in the text?",
            "reference-answers": [
                "A better translation of a text can make its alignment less reachable because, while it may be more accurate, there is no clear word-level correspondence between the original and the translated text. For example, the phrase \"I would just like to warn that if you have a better translation of a text, then the alignment can even become less reachable\" translates into Czech as \"Aby vztoupila do devadesátých let v co nejlepší formě,\" and this translation lacks direct word-to-word correspondence, making alignment harder to achieve."
            ]
        },
        {
            "question": "How does a better translation affect the alignment, and provide an example from the text to illustrate this point?",
            "reference-answers": [
                "A better translation can make alignment less reachable because it may not correspond word-for-word, as seen in the example: \"get in shape for the 90s\" translates to \"Aby vztoupila do devadesátých let v co nejlepší formě,\" where there is no clear word level correspondence."
            ]
        },
        {
            "question": "What does the text suggest about the correspondence between words in the original and translated texts?",
            "reference-answers": [
                "The text suggests that there is no clear word-level correspondence between the original and translated texts when a better translation is used, as demonstrated by the example where the English and Czech phrases do not align one-to-one."
            ]
        }
    ],
    "nmt-class/lecture04-alignment/screen87-slide32/text.txt": [
        {
            "question": "How do the Czech and English sentences structure the placement of time expressions, as illustrated in the example?",
            "reference-answers": [
                "The Czech sentence places the time expression \"až v lednu\" (only in January) after the verb phrase \"začnou funguat\" (will start functioning), while the English sentence places the time expression \"until January\" at the end of the sentence, after the verb phrase \"aren't expected to begin.\""
            ]
        },
        {
            "question": "What adverb does Czech use to indicate that something will start functioning only in a specific month, as demonstrated in the example?",
            "reference-answers": [
                "The adverb used in Czech is \"až.\""
            ]
        },
        {
            "question": "What is the structural difference in how the time expressions are used in the English and Czech sentences to convey the timing of the event?",
            "reference-answers": [
                "In English, the time expression \"until January\" is introduced by the preposition \"until\" and placed after the verb phrase \"aren't expected to begin,\" whereas in Czech, the time expression \"až v lednu\" (only in January) is introduced by the adverb \"až\" and placed before the verb phrase \"začnou funguat\" (they will start functioning)."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen01-slide02/text.txt": [
        {
            "question": "What is this lecture devoted to?",
            "reference-answers": [
                "This lecture is devoted to phrase-based machine translation."
            ]
        },
        {
            "question": "What is the primary method discussed in this lecture for machine translation?",
            "reference-answers": [
                "Phrase based machine translation"
            ]
        },
        {
            "question": "What topic is the fifth lecture on machine translation focused on?",
            "reference-answers": [
                "Phrase based machine translation."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen02-slide03/text.txt": [
        {
            "question": "What is decoding, and how is it used in the context of phrase-based machine translation?",
            "reference-answers": [
                "Decoding is the process of translating a given input sentence using the prepared translation tables and automatically extracted dictionaries. It involves applying the log-linear model and the features used to score phrases to generate the target language output."
            ]
        },
        {
            "question": "What is the mathematical basis of the phrase-based machine translation system discussed in the lecture?",
            "reference-answers": [
                "The mathematical basis of the phrase-based machine translation system discussed in the lecture is the log-linear model."
            ]
        },
        {
            "question": "What are the key components of the lecture on phrase-based machine translation, including its mathematical basis, decoding process, and training method?",
            "reference-answers": [
                "The key components of the lecture on phrase-based machine translation are as follows:\n\n1. **Phrase-based Machine Translation (MT)**: The lecture begins with a summary of phrase-based MT, providing an overview of the approach.\n\n2. **Log-linear Model**: The mathematical basis of the system is the log-linear model, which forms the statistical foundation of the phrase-based MT.\n\n3. **Phrase-based Machine Translation Model**: The model is described, focusing on the features used in the log-linear model, including how these features are obtained and utilized to score phrases.\n\n4. **Decoding Process**: A significant portion of the lecture is dedicated to the decoding process, which involves translating a given input sentence using the prepared translation tables and automatically extracted translation dictionaries.\n\n5. **Training Method**: The final part of the lecture covers the training method, specifically weight optimization and minimum error rate training, which are essential for optimizing the model's performance."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen03-slide04/text.txt": [
        {
            "question": "What is the primary linguistic resource that Phrasebase MT creates from a parallel corpus?",
            "reference-answers": [
                "The primary linguistic resource that Phrasebase MT creates from a parallel corpus is a collection of short phrases, each consisting of a source phrase and its corresponding target phrase, which are extracted and used as building blocks for translation."
            ]
        },
        {
            "question": "How does Phrasebase MT use a parallel corpus to create translation dictionaries and translate new sentences?",
            "reference-answers": [
                "Phrase-based MT uses a parallel corpus to create translation dictionaries by extracting correspondences between individual words, including multi-word expressions and fully inflected forms, resulting in a dictionary of exact word forms. To translate new sentences, it segments the input into short phrases, selects the best matching phrases from the dictionary, and combines them to form a fluent output."
            ]
        },
        {
            "question": "What are the key steps in Phrase-Based Machine Translation, and how does it utilize a parallel corpus and a translation dictionary to translate new sentences?",
            "reference-answers": [
                "The key steps in Phrase-Based Machine Translation involve building a translation dictionary from a parallel corpus and using it to translate new sentences. First, a parallel corpus is used to extract correspondences between words and phrases in both languages, creating a dictionary that includes multi-word expressions and fully inflected forms. This dictionary is then used to segment input sentences into short phrases, which are replaced with their corresponding translations from the dictionary. The system selects the best combination of these phrases to ensure the output sentence is fluent and contextually appropriate. This approach allows Phrase-Based MT to effectively translate new sentences by leveraging the encoded linguistic knowledge from the parallel corpus and translation dictionary."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen04-slide05/text.txt": [
        {
            "question": "What is the primary focus of the log linear model when used for estimating the conditional probability of a target sentence given a source sentence in machine translation?",
            "reference-answers": [
                "The primary focus of the log linear model is to maximize the score of the target sentence by considering multiple feature functions without normalizing the probabilities, aiming to select the sentence with the highest score."
            ]
        },
        {
            "question": "What is the primary goal of the log linear model when evaluating translation candidates?",
            "reference-answers": [
                "The primary goal of the log linear model when evaluating translation candidates is to determine which candidate has the highest score based on the weighted sum of feature functions, thus selecting the best translation without normalizing across all possible sentences."
            ]
        },
        {
            "question": "What is the main purpose of using the log linear model when evaluating translation candidates?",
            "reference-answers": [
                "The main purpose of using the log linear model when evaluating translation candidates is to score each candidate based on a weighted sum of feature functions and select the candidate with the highest score, without needing to normalize the probabilities. This allows us to determine the best sentence in the competition by focusing on the highest scorer."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen05-slide06/text.txt": [
        {
            "question": "What is the key assumption of phrase-based MTE and how does the model handle the segmentation and reordering of phrases during translation?",
            "reference-answers": [
                "The key assumption of phrase-based MTE is that sentences can be translated by translating short sequences of words (phrases) independently of each other. The model handles segmentation by considering different ways to split the input sentence into phrases, with segmentation treated as a hidden variable in the scoring process. Reordering is also incorporated into the scoring, allowing for flexibility in phrase order during translation, though word order within individual phrases can change independently."
            ]
        },
        {
            "question": "What is the key assumption of phrase-based machine translation (MTE) as described in the text?",
            "reference-answers": [
                "The key assumption of phrase-based machine translation (MTE) is that sentences can be translated by translating short sequences of words (phrases) independently of each other."
            ]
        },
        {
            "question": "**Question:**  \nWhat is the role of segmentation in phrase-based machine translation (MTE) and how does it influence the translation process?",
            "reference-answers": [
                "Segmentation plays a crucial role in phrase-based machine translation (MTE) by determining how the input sentence is divided into phrases. These phrases are translated independently, and the segmentation can vary depending on the match with the training data. The model considers different possible segmentations of the input sentence, which influences the scoring process. Each segmentation is a hidden variable that affects the feature functions in the model, as they depend on the input, output, and segmentation. Essentially, the model searches for the best derivation of the output sentence, which includes the optimal segmentation and translation of the input. This means that segmentation is not just about splitting the sentence into chunks but also about finding the most effective way to translate the input by considering various ways to break it down into phrases. This approach allows for flexibility in translation, especially when dealing with languages that require specific word order changes within phrases."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen06-slide07/text.txt": [
        {
            "question": "What is the role of the phrase count or phrase penalty feature in phrase-based MT, and how does it influence the model's preference for phrase length?",
            "reference-answers": [
                "The phrase count or phrase penalty feature in phrase-based MT plays a crucial role by influencing the model's preference for phrase length. This feature controls whether the system will prefer to use longer phrases or shorter phrases. It allows the model to adjust its behavior based on the match between the training data and the test data. If the training data closely matches the test data, the system can use longer phrases, which reduces the risk of errors. However, if the test data is different from the training data, the system may resort to using shorter phrases to minimize errors, even though this increases the risk of introducing errors at phrase boundaries. This feature thus serves as an important parameter that enables the model to balance between accuracy and robustness."
            ]
        },
        {
            "question": "What is the purpose of the phrase translation probability feature in phrase-based MT, and how do the word count and phrase count/penalty features influence the translation process?",
            "reference-answers": [
                "The purpose of the phrase translation probability feature in phrase-based MT is to score phrases independently based on their translation quality, using probabilities estimated from a parallel corpus. The word count feature influences the translation process by controlling the overall length of the target sentence, encouraging either shortening or lengthening. The phrase count/penalty feature affects whether the system prefers longer or shorter phrases, balancing between accuracy and robustness depending on the match between training and test data."
            ]
        },
        {
            "question": "What are the four main features discussed in the text for phrase-based Machine Translation?",
            "reference-answers": [
                "The four main features discussed in the text for phrase-based Machine Translation are:\n\n1. **Phrase Translation Probability**: Scores phrases independently based on their translation probability from a parallel corpus.\n2. **Word Count**: Controls the model's tendency to shorten or lengthen the output by considering the number of words in the target sentence.\n3. **Phrase Count or Phrase Penalty**: Influences whether the system prefers longer or shorter phrases by considering the segmentation of the sentence.\n4. **Language Model Probability**: Uses an N-gram language model to assess the probability of each word in the target sentence based on its preceding context."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen07-slide08/text.txt": [
        {
            "question": "What are the steps involved in the pipeline for building a phrase-based machine translation system?",
            "reference-answers": [
                "The pipeline for building a phrase-based machine translation system involves the following steps: first, finding parallel texts; second, aligning them at the document and sentence levels; third, extracting translation units with their scores; fourth, training the model by estimating the importance of parameters; and finally, performing inference using the phrase table."
            ]
        },
        {
            "question": "What are the key steps involved in training a phrase-based machine translation system?",
            "reference-answers": [
                "The key steps involved in training a phrase-based machine translation system are:\n\n1. **Finding parallel texts**: This involves identifying bilingual texts that can be aligned for translation.\n2. **Aligning the texts**: The parallel texts are aligned at the document and sentence levels to establish correspondence between the source and target languages.\n3. **Extracting translation units**: From the aligned texts, translation units along with their scores are extracted. These units form the basis of the phrase tables or probability tables.\n4. **Training the model**: The system estimates the importance of different parameters based on the data from the extracted translation units."
            ]
        },
        {
            "question": "What are the two main components of a phrase-based machine translation system as described in the text?",
            "reference-answers": [
                "The two main components of a phrase-based machine translation system as described in the text are the phrase tables (translation units) and the model (training and inference process)."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen08-slide09/text.txt": [
        {
            "question": "Question: Based on the text, where do the counts used to estimate the probabilities of the most important feature in phrase-based machine translation come from?",
            "reference-answers": [
                "The counts come from the training data, specifically the co-occurrences of the source and target phrases in that data."
            ]
        },
        {
            "question": "How are the probabilities of the most important feature of phrase-based machine translation estimated?",
            "reference-answers": [
                "The probabilities are estimated using maximum likelihood estimation, where the count of co-occurrences of the source and target phrases is normalized by the count of the antecedent. Specifically, for the probability of the target given the source, the count of their co-occurrences is divided by the count of the source phrase. Similarly, for the probability of the source given the target, the count is divided by the count of the target phrase."
            ]
        },
        {
            "question": "What is the method for estimating the probabilities in phrase-based machine translation and how are the counts used?",
            "reference-answers": [
                "The method for estimating probabilities in phrase-based machine translation is maximum likelihood estimation. This involves counting the co-occurrences of source and target phrases and normalizing these counts by the count of the source phrase for one direction and the count of the target phrase for the other direction. The counts are derived from the observed occurrences of the phrases in the training data."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen09-slide10/text.txt": [
        {
            "question": "What is the key requirement for extracting consistent phrase pairs in a parallel corpus, and why is it important?",
            "reference-answers": [
                "The key requirement for extracting consistent phrase pairs in a parallel corpus is consistency with the word alignment. This is important because it ensures that no word is forgotten and that the phrase pairs accurately reflect the meaning of the source and target phrases, allowing for reliable translations."
            ]
        },
        {
            "question": "What is the typical maximum length of phrases extracted from the parallel corpus in this method?",
            "reference-answers": [
                "The typical maximum length of phrases extracted from the parallel corpus in this method is ten words."
            ]
        },
        {
            "question": "How does the phrase-based system ensure consistency in the extraction of phrase pairs based on word alignments?",
            "reference-answers": [
                "The phrase-based system ensures consistency by extracting phrase pairs that align completely with word alignments. This means that any phrase extracted must include all words in both the source and target languages that correspond to each other in the alignment. For example, if a source phrase is aligned with multiple target words, the extracted phrase pair must cover all these aligned words. This consistency ensures that no word is left out and that the phrase pairs accurately represent the aligned segments. Additionally, the system extracts phrases of varying lengths and overlaps, which contributes to the robustness of the phrase table by allowing for flexibility in translation reconstruction."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen10-slide09/text.txt": [
        {
            "question": "What factors are used to determine the translation probabilities in the example provided?",
            "reference-answers": [
                "The translation probabilities are determined using two main factors: co-occurrence counts divided by either the count of the source phrase or the count of the target phrase (in two directions), and lexically weighted scores of the phrases."
            ]
        },
        {
            "question": "How are the translation probabilities calculated?",
            "reference-answers": [
                "The translation probabilities are calculated by dividing the co-occurrence counts of phrases by either the count of the source phrase or the count of the target phrase, representing the two directions."
            ]
        },
        {
            "question": "Question: What are the three main components of the scores used in the translation model described in the text?",
            "reference-answers": [
                "The three main components of the scores used in the translation model are phrase translation probabilities, co-occurrence counts divided by either the count of the source phrase or the count of the target phrase, and lexically weighted scores."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen11-slide10/text.txt": [
        {
            "question": "What is the purpose of using internal dots in phrases when scoring phrase pairs for lexical weighting?",
            "reference-answers": [
                "The purpose of using internal dots in phrases when scoring phrase pairs for lexical weighting is to assess how often the individual components of a phrase are linked together through word alignment. By examining how frequently each word within a phrase is connected to others, the system can determine the phrase's lexical weight. If the components are common and frequently paired, the phrase's translation score is enhanced, as seen in examples like \"faster and full stop,\" where frequent pairing increases its lexical weighting."
            ]
        },
        {
            "question": "What are the two main factors considered when scoring the phrase pair \"this\" and \"niñi\"?",
            "reference-answers": [
                "The two main factors considered when scoring the phrase pair \"this\" and \"nini\" are:\n\n1. The division of counts of the phrases as a whole.\n2. How often \"nini\" was linked with specific words through word alignment, such as \"time,\" \"around,\" \"richly,\" and \"faster.\""
            ]
        },
        {
            "question": "What is the impact of considering two-word phrases and lexical weighting on the scoring of phrase pairs like 'this time around' and 'nini'?",
            "reference-answers": [
                "Considering two-word phrases and lexical weighting impacts the scoring of phrase pairs by focusing on the frequency of individual words within the phrases. If the words are commonly linked through word alignment, the lexically weighted phrase translation probability increases. This method can lead to higher scores for phrase pairs where the components are frequent and often associated, even if the entire phrase isn't as common on its own."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen12-slide11/text.txt": [
        {
            "question": "What is the value of the phrase penalty that was always set to e to the power of one in the Moses decoder?",
            "reference-answers": [
                "The phrase penalty was always set to e to the power of one, which is approximately 2.71. \n\nAnswer: e to the power of one (approximately 2.71)."
            ]
        },
        {
            "question": "Why is there a high risk of imprecision when using maximum likelihood estimates for long phrases?",
            "reference-answers": [
                "When using maximum likelihood estimates for long phrases, there is a high risk of imprecision because the number of observations for these phrases can be very low. This leads to situations where probabilities are calculated based on small numbers, such as dividing two by one, resulting in exaggerated estimates like 0.5. These imprecise probabilities are unreliable due to the scarcity of data for long phrases."
            ]
        },
        {
            "question": "What was the phrase penalty value in the Moses decoder and why was it removed in later versions?",
            "reference-answers": [
                "The phrase penalty value in the Moses decoder was set to e to the power of one (approximately 2.71). It was removed in later versions of the decoder, though the specific reason for its removal is not detailed in the text."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen13-slide12/text.txt": [
        {
            "question": "What are the two phases of translation in a phrase-based system, and what does each phase involve?",
            "reference-answers": [
                "The two phases of translation in a phrase-based system are as follows:  \n1. **First Phase**: This involves considering the phrase table and searching for applicable phrases that could cover the input sentence. It is akin to selecting puzzle pieces from a bag that could potentially fit together to form the sentence.  \n2. **Second Phase**: This is the decoding phase, where the system plays around with the combinations of the selected phrases, sub-selecting the final set of phrases and arranging them in the correct order to form the translated sentence."
            ]
        },
        {
            "question": "What are the two phases of translation in a phrase-based system, and how are phrases used in each phase?",
            "reference-answers": [
                "The translation in a phrase-based system has two phases. In the first phase, the system considers and selects phrases from a phrase table that are applicable to the input sentence. In the second phase, the system combines these selected phrases into the correct order to produce the final translated sentence."
            ]
        },
        {
            "question": "What are the two phases of translation described, and how is the process compared to a puzzle?",
            "reference-answers": [
                "The translation process has two phases. In the first phase, you search your phrase table to identify which phrases are applicable for the input sentence. This is like selecting puzzle pieces that could cover the sentence. In the second phase, you combine these selected phrases into a coherent translation, similar to arranging the puzzle pieces in the correct order to form the complete picture."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen14-slide13/text.txt": [
        {
            "question": "What is the best translation selected by the system to translate the whole sentence \"Er get Janik na Hause\"?",
            "reference-answers": [
                "he does not go home"
            ]
        },
        {
            "question": "What is the best translation selected by the system for the sentence \"Er get Janik na Hause,\" and how was this selection determined based on the phrase table and possible translations?",
            "reference-answers": [
                "The best translation selected by the system for the sentence \"Er get Janik na Hause\" is \"he does not go home.\" This selection was determined by considering all possible translations of shorter and longer spans from the sentence, including individual words and combinations like \"Er get.\" The system identified the best choice based on these translations, even though it involved some reordering that the system had yet to fully process."
            ]
        },
        {
            "question": "What is the best translation of the sentence \"Er get Janik na Hause\" and what reordering is involved in achieving it?",
            "reference-answers": [
                "The best translation of the sentence \"Er get Janik na Hause\" is \"he does not go home.\" The reordering involved changes the structure to fit English syntax, resulting in the translation."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen15-slide14/text.txt": [
        {
            "question": "What is the second stage of translation called, as mentioned in the text?",
            "reference-answers": [
                "The second stage of translation is called beam search.\n\nANSWER: beam search"
            ]
        },
        {
            "question": "What is mentioned as the second stage of translation?",
            "reference-answers": [
                "The second stage of translation mentioned is **beam search**."
            ]
        },
        {
            "question": "What is the name of the second stage of translation mentioned here?",
            "reference-answers": [
                "beam search"
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen16-slide15/text.txt": [
        {
            "question": "What does the coverage vector indicate when the hypothesis is empty?",
            "reference-answers": [
                "The coverage vector is set to zero, indicating that no input words have been translated."
            ]
        },
        {
            "question": "What does the coverage vector indicate when the hypothesis is empty?",
            "reference-answers": [
                "The coverage vector indicates that no input words have been translated when the hypothesis is empty."
            ]
        },
        {
            "question": "What does the coverage vector indicate when the hypothesis is empty?",
            "reference-answers": [
                "The coverage vector indicates that no input words have been translated when the hypothesis is empty."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen17-slide16/text.txt": [
        {
            "question": "What term is used to describe the process of expanding the initial hypothesis by translating the second word in the input?",
            "reference-answers": [
                "hypothesis expansion"
            ]
        },
        {
            "question": "What English word was translated as 'r' in the example?",
            "reference-answers": [
                "get"
            ]
        },
        {
            "question": "What is the process called when the teacher starts by translating the second word 'get' as 'r' in the example?",
            "reference-answers": [
                "The process is called hypothesis expansion."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen18-slide17/text.txt": [
        {
            "question": "How many different translation hypotheses are created by translating the first word as \"he\" and \"it\" and the second word as \"it\"?",
            "reference-answers": [
                "There are two different translation hypotheses created by translating the first word as \"he\" and \"it\" and the second word as \"it\"."
            ]
        },
        {
            "question": "How many hypotheses are mentioned in the text, and what do they cover?",
            "reference-answers": [
                "There are three hypotheses mentioned. They cover different source words and produce different translations."
            ]
        },
        {
            "question": "How many hypotheses are mentioned in the text as a result of translating the first two words differently?",
            "reference-answers": [
                "Three."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen19-slide18/text.txt": [
        {
            "question": "What is the final outcome after considering all possible orderings of phrases in the process described?",
            "reference-answers": [
                "The final outcome is the arrival at a hypothesis that covers the whole input and output after considering all possible orderings of phrases."
            ]
        },
        {
            "question": "What is the purpose of expanding the hypothesis, and which specific words were used as examples in the text to illustrate this process?",
            "reference-answers": [
                "The purpose of expanding the hypothesis is to refine and extend it by considering various translations and ensuring it covers the entire input and output. Specific words used as examples include \"he,\" \"ja\" (translated as \"yes\" or \"does not\"), \"go,\" and \"nach Hause\" (translated as \"home\")."
            ]
        },
        {
            "question": "What is the significance of expanding the hypothesis by considering the translation of the third word \"ja\" as \"yes\" and \"does not\" in the context of the described translation process?",
            "reference-answers": [
                "The significance of expanding the hypothesis by considering the translation of the third word \"ja\" as both \"yes\" and \"does not\" is to evaluate the potential paths for a complete and coherent translation. Translating \"ja\" as \"yes\" led to an incomplete sequence, while translating it as \"does not\" provided a more promising path, allowing further expansion and eventual completion of the translation. This process highlights the importance of exploring multiple translation possibilities to ensure the hypothesis covers the entire input and output effectively."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen20-slide19/text.txt": [
        {
            "question": "What is the result of the path through the search graph back to the initial hypothesis?",
            "reference-answers": [
                "The result of the path through the search graph back to the initial hypothesis is the final full output text of the target sentence, the best scoring sentence."
            ]
        },
        {
            "question": "What does the best scoring sentence say he does not do?",
            "reference-answers": [
                "He does not go home."
            ]
        },
        {
            "question": "What is the best scoring sentence mentioned in the text?",
            "reference-answers": [
                "the best scoring sentence"
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen21-slide18/text.txt": [
        {
            "question": "What is important to realize according to the text?",
            "reference-answers": [
                "that it's important to realize that"
            ]
        },
        {
            "question": "The question cannot be answered as is because the provided text is incomplete. To form a meaningful question, the full statement after \"realize that\" is needed. Please provide the complete text so an appropriate question can be generated.",
            "reference-answers": [
                "The question cannot be answered as is because the provided text is incomplete. To form a meaningful question, the full statement after \"realize that\" is needed."
            ]
        },
        {
            "question": "What is important to realize next?",
            "reference-answers": [
                "It's important to realize that [additional context needed to provide a specific answer]."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen22-slide19/text.txt": [
        {
            "question": "What factors contribute to the large search space in the described scenario?",
            "reference-answers": [
                "The large search space is contributed by three main factors:\n1. A vast number of translation options available.\n2. Each particle span within the text also offers numerous translation options.\n3. The presence of many spans in the text. These elements collectively increase the search space by exponentially expanding the number of possible partial hypotheses."
            ]
        },
        {
            "question": "Why is the search space considered pretty big in the context of hypothesis generation?",
            "reference-answers": [
                "The search space is considered pretty big because there are many translation options, and each particle span also has many translation options. Additionally, there are a large number of spans to consider, which together make the number of hypotheses (partial hypotheses) quite extensive."
            ]
        },
        {
            "question": "What is the reason mentioned for the large search space in machine translation?",
            "reference-answers": [
                "The large search space in machine translation is due to the many translation options available for each particle span and the numerous spans themselves, leading to a significant number of partial hypotheses to consider."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen23-slide20/text.txt": [
        {
            "question": "What does the text prove about machine translation in terms of computational complexity, and how is this demonstrated using the Hamiltonian circuit problem?",
            "reference-answers": [
                "The text proves that machine translation is NP-hard by demonstrating a reduction from the Hamiltonian circuit problem, a well-known NP-complete problem, to the machine translation task. This is shown by using a black box machine translation system, which solves the problem of finding the best word order based on a bigram language model in polynomial time. By converting the Hamiltonian circuit problem into a machine translation problem, where nodes represent words and edges represent bigrams, the text establishes that solving the machine translation problem efficiently would also solve the Hamiltonian circuit problem efficiently. Since the Hamiltonian circuit problem is NP-hard, this implies that machine translation is also NP-hard."
            ]
        },
        {
            "question": "Question: Explain how the reduction from the Hamiltonian circuit problem was used to prove that machine translation is NP-hard, as described in the text.",
            "reference-answers": [
                "To prove that machine translation is NP-hard, the reduction from the Hamiltonian circuit problem was used as follows: Each node in the graph is labeled with a word, and each edge represents a permissible bigram. The task of finding a Hamiltonian circuit corresponds to finding the most likely word order using the bigram language model. If a machine translation system could solve this in polynomial time, it would imply that the Hamiltonian circuit problem can also be solved in polynomial time. Since the Hamiltonian circuit problem is NP-complete, this reduction demonstrates that machine translation is NP-hard."
            ]
        },
        {
            "question": "Question: Explain how the reduction of the Hamiltonian circuit problem to a machine translation problem using a bigram language model demonstrates that machine translation is NP-hard.",
            "reference-answers": [
                "The reduction demonstrates that solving the Hamiltonian circuit problem can be translated into a machine translation task using a bigram language model. By encoding the graph's nodes as words and edges as allowed bigrams, the machine translation system's ability to find the most probable word sequence (equivalent to the Hamiltonian circuit) implies that if machine translation were solvable in polynomial time, so would the Hamiltonian circuit problem. Since the Hamiltonian circuit problem is NP-hard, this reduction shows that machine translation inherits this NP-hardness, proving it is also NP-hard."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen24-slide21/text.txt": [
        {
            "question": "What conclusion can be drawn about the relationship between machine translation and the minimum set cover problem based on the example provided?",
            "reference-answers": [
                "The conclusion is that machine translation tasks involving the minimum set cover problem cannot be solved in polynomial time if P does not equal NP, implying that such tasks are NP-hard."
            ]
        },
        {
            "question": "How is the minimum set cover problem related to the machine translation task described in the text?",
            "reference-answers": [
                "The minimum set cover problem is related to the machine translation task by mapping the task of selecting multiword expressions to cover all source words into a set cover problem. Each multiword expression represents a set of source words, and the goal is to find the minimum number of such expressions (sets) needed to cover all the words (elements) in the source sentence. This makes the machine translation task equivalent to solving the minimum set cover problem, which is NP-hard."
            ]
        },
        {
            "question": "What NP-complete problem is used to model the selection of multiword expressions in machine translation as described in the text?",
            "reference-answers": [
                "The NP-complete problem used is the **minimum set cover problem**."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen25-slide22/text.txt": [
        {
            "question": "What three techniques are discussed by Barry Heddo in his slides to fight complexity?",
            "reference-answers": [
                "The three techniques discussed by Barry Heddo are hypothesis recombination, stack-based pruning, and future cost estimation."
            ]
        },
        {
            "question": "What three methods are discussed in Barry Heddo's slides to address complexity?",
            "reference-answers": [
                "The three methods discussed in Barry Heddo's slides to address complexity are:  \n1. Hypothesis recombination  \n2. Stack-based pruning  \n3. Future cost estimation"
            ]
        },
        {
            "question": "What three topics will be discussed in the slides by Barry Heddo to address the complexity?",
            "reference-answers": [
                "The three topics discussed in the slides by Barry Heddo to address complexity are hypothesis recombination, stack-based pruning, and future cost estimation."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen26-slide23/text.txt": [
        {
            "question": "What process does the system use to reduce the search space by combining hypotheses that share the same input and output coverage?",
            "reference-answers": [
                "The system uses a process called \"recombining\" hypotheses. When a new partial hypothesis is created, the system checks if there is already a hypothesis that covers the same set of input words and produces the same output. If such a hypothesis exists, the system combines them, effectively reducing the search space by avoiding redundant continuations. This process ensures that only one path is expanded for identical future steps, thereby efficiently managing the search space."
            ]
        },
        {
            "question": "What is the technique described in the text where partial hypotheses are combined if they cover the same input and output, and what is its purpose?",
            "reference-answers": [
                "The technique described is the combination of partial hypotheses if they cover the same input and output. Its purpose is to reduce the search space and make the translation process more efficient by recombining hypotheses that would otherwise be ignored in the end."
            ]
        },
        {
            "question": "What is the method described for reducing the search space in machine translation by recombining partial hypotheses?",
            "reference-answers": [
                "The method described for reducing the search space in machine translation is the recombination of partial hypotheses. This involves checking if a new partial hypothesis already exists that covers the same input words and produces the same output. If such a hypothesis exists, the two are merged, and only one is expanded further, thus reducing the search space by avoiding redundant paths."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen27-slide24/text.txt": [
        {
            "question": "**Question:**  \nUnder what condition can two different translation hypotheses be safely recombined according to the text?",
            "reference-answers": [
                "Two different translation hypotheses can be safely recombined if they have the same coverage vector and the same last two words in their history, ensuring that future scores remain unaffected."
            ]
        },
        {
            "question": "Under what conditions can two hypotheses be safely recombined when using a trigram language model?",
            "reference-answers": [
                "Two hypotheses can be safely recombined when using a trigram language model if they have the same coverage vector and the same last two words in their history. The difference in the beginning of the hypotheses does not affect the scores because the trigram model only considers the last two words when scoring future paths."
            ]
        },
        {
            "question": "Under what condition can two hypotheses be safely recombined when using a trigram language model, according to the text?",
            "reference-answers": [
                "Two hypotheses can be safely recombined when using a trigram language model if they have the same last two words. This is because the trigram model only considers the last two words to predict the next one, making the history beyond those two words irrelevant for future scoring."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen28-slide25/text.txt": [
        {
            "question": "What condition is imposed on the reordering model and where does it come from?",
            "reference-answers": [
                "The condition imposed on the reordering model comes from the language model, and the translation model does not impose any restrictions on it."
            ]
        },
        {
            "question": "Question: Does the translation model impose any restrictions on the reordering model, and if so, what is the condition?",
            "reference-answers": [
                "The translation model does not impose any restrictions on the reordering model. The condition comes from the language model, not the translation model."
            ]
        },
        {
            "question": "Does the translation model impose any restrictions on the reordering model?",
            "reference-answers": [
                "No, the translation model does not impose any restrictions on the reordering model."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen29-slide26/text.txt": [
        {
            "question": "**Question:**  \nWhy is early pruning necessary in the context of the search algorithm described in the text?",
            "reference-answers": [
                "Early pruning is necessary because the search space is still not sufficient, and it allows us to drop less likely partial hypotheses before completing the search, thereby making the process more efficient."
            ]
        },
        {
            "question": "What must be done to address the insufficient search space?",
            "reference-answers": [
                "To address the insufficient search space, early pruning must be performed. This involves dropping less likely partial hypotheses before completion."
            ]
        },
        {
            "question": "What must be done when the search space is still not sufficient?",
            "reference-answers": [
                "Early pruning must be done, and less likely partial hypotheses should be dropped before finishing the search."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen30-slide25/text.txt": [
        {
            "question": "What is the role of stacks in stack-based beam search, and how do they help in controlling the search space during translation?",
            "reference-answers": [
                "In stack-based beam search, stacks organize partial hypotheses such that each stack contains hypotheses covering the same number of source words. This ensures the search proceeds linearly, exploring only a limited number of options at each step. By imposing a stack limit (e.g., 20, 200, 1000), the search space is controlled, preventing the exponential growth of possibilities. During expansion, hypotheses are moved to higher stacks based on the number of words covered, and lower-scoring hypotheses are discarded if the stack limit is reached. This approach efficiently manages the search by focusing on the most promising translations and linearly progressing through the sentence."
            ]
        },
        {
            "question": "What is the purpose of the stack limit in stack-based beam search as described in the text?",
            "reference-answers": [
                "The purpose of the stack limit in stack-based beam search is to ensure that only a limited number of candidate translations or partial hypotheses are stored within each stack. This limitation helps to manage the computational resources and prevent the search from becoming too extensive, thus avoiding the exponential explosion of possibilities and keeping the search process efficient."
            ]
        },
        {
            "question": "How does stack-based beam search manage the exploration of translation hypotheses to prevent an exponential increase in the search space?",
            "reference-answers": [
                "Stack-based beam search manages the exploration of translation hypotheses by organizing them into a sequence of stacks, where each stack contains hypotheses that cover the same number of source words. This approach ensures that the search progresses linearly through the sentence, moving from one stack to the next. Each hypothesis is expanded by adding new words, which moves it to a higher stack. A stack limit is applied, capping the number of candidate translations stored in each stack. If too many expansions are generated, the lower-scoring ones are discarded. This linear progression and pruning limit the search space, preventing exponential growth and ensuring efficient exploration."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen31-slide25/text.txt": [
        {
            "question": "What is the order in which the stacks are processed when traveling across all stacks in the described algorithm?",
            "reference-answers": [
                "The stacks are processed in a linear order, starting from the first stack and moving sequentially to the next stacks."
            ]
        },
        {
            "question": "What does the final stack contain after the described process is completed?",
            "reference-answers": [
                "The final stack contains only hypotheses that cover all the words and are at the top of the stack."
            ]
        },
        {
            "question": "What is contained in the final stack after processing all the hypotheses?",
            "reference-answers": [
                "The final stack contains only hypotheses that cover all the words, with these hypotheses at the top of the stack."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen32-slide25/text.txt": [
        {
            "question": "What are the pruning strategies mentioned, and how do they affect the complexity based on the number of translation options?",
            "reference-answers": [
                "Pruning strategies mentioned include keeping at most K hypotheses in a stack, keeping the best score, and hypotheses that are not worse than 10 times or one tenth of the best score. These strategies affect the complexity by introducing quadratic complexity based on the number of translation options."
            ]
        },
        {
            "question": "What type of complexity is mentioned when considering the number of translation options in pruning strategies?",
            "reference-answers": [
                "quadratic complexity"
            ]
        },
        {
            "question": "What type of complexity is mentioned in the text when discussing the number of translation options?",
            "reference-answers": [
                "quadratic complexity"
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen33-slide25/text.txt": [
        {
            "question": "What trade-off is described in the text regarding the complexity of translation when reordering is limited?",
            "reference-answers": [
                "The trade-off described is between the complexity of translation and the accuracy when reordering is limited. By restricting the expansion of translation options close to the currently translated word, the search complexity is reduced from quadratic to linear. However, this limitation may result in less accurate translations as fewer translation options are considered."
            ]
        },
        {
            "question": "What happens to the search complexity when the reordering is limited to expansions only within a constant range of the currently translated word?",
            "reference-answers": [
                "The search complexity reduces to linear complexity when the reordering is limited to expansions only within a constant range of the currently translated word."
            ]
        },
        {
            "question": "What is the reason for quadratic complexity in the context of translation, as mentioned in the text?",
            "reference-answers": [
                "The quadratic complexity arises from the reordering of words during translation, as it involves considering a large number of possible reorderings, which increases with the sentence length."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen34-slide25/text.txt": [
        {
            "question": "**Question:**  \nWhy is a future cost estimate necessary when comparing hypotheses that cover the same number of source words in the allocation process?",
            "reference-answers": [
                "A future cost estimate is necessary because it ensures a fair comparison between hypotheses that cover the same number of source words but approach the translation differently. Without considering future costs, a hypothesis that translates the harder part of the sentence first might be unfairly pruned because its initial score seems lower. However, this hypothesis could later recover its score by translating the easier part of the sentence. The future cost estimate accounts for the potential additional effort required to complete the translation, ensuring that hypotheses are not prematurely discarded based solely on their current score. This approach is similar to the heuristic used in the A* search algorithm, which estimates the cost to reach the final point, thus providing a more accurate and fair comparison."
            ]
        },
        {
            "question": "What does the text introduce to make the comparison of hypotheses fair when they cover the same number of source words?",
            "reference-answers": [
                "The text introduces a future cost estimate heuristic, similar to the A* search algorithm, to ensure fair comparison between hypotheses. This heuristic accounts for the remaining translation difficulty, making the comparison fair by considering both past and future costs."
            ]
        },
        {
            "question": "Question: Why are future cost estimates important in the allocation of translation hypotheses?",
            "reference-answers": [
                "Future cost estimates are important because they provide a fair way to compare hypotheses by considering the estimated cost to complete the translation. Without them, the system might prune a hypothesis that initially translates harder words, which could later recover its score by translating easier words. By including future cost estimates, the system ensures a more accurate and fair allocation of translation hypotheses, preventing premature pruning and allowing for better recovery of potential scores."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen35-slide25/text.txt": [
        {
            "question": "Why is the future cost estimate considered optimistic in this context?",
            "reference-answers": [
                "The future cost estimate is considered optimistic because it ignores the reordering cost and the n-grams at the boundaries of phrases, resulting in a lower estimate than the actual cost."
            ]
        },
        {
            "question": "What is the future cost considered an optimistic estimate of, and why is this approach taken in the context of A* search?",
            "reference-answers": [
                "The future cost is considered an optimistic estimate of how much it will cost to proceed with the translation, particularly for each individual feature function. This approach is taken because ignoring certain costs, such as reordering costs and n-grams at phrase boundaries, makes the estimate lower and more efficient in the context of A* search."
            ]
        },
        {
            "question": "What components are ignored in the future cost estimation to make it optimistic?",
            "reference-answers": [
                "The components ignored in the future cost estimation to make it optimistic are the reordering cost and the n-grams at the boundaries of phrases."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen36-slide25/text.txt": [
        {
            "question": "What approach is used to estimate the future cost of translating a sentence, and what components are considered in this approach?",
            "reference-answers": [
                "The approach used to estimate the future cost of translating a sentence is a dynamic programming style algorithm. The components considered in this approach include the cheapest costs for translating individual words, word pairs, triples of words, and so on. The algorithm fills up a diagonal or triangular table that provides the minimum cost for translating each span of the sentence."
            ]
        },
        {
            "question": "What method is used to estimate the future cost of translating a sentence, and what are the components involved in this process?",
            "reference-answers": [
                "The method used to estimate the future cost of translating a sentence is a dynamic programming style algorithm. The components involved include the translation options for individual words, word pairs, triples of words, and so on. These components are used to fill up a table that determines the cheapest cost for translating each span of the sentence."
            ]
        },
        {
            "question": "How does the dynamic programming algorithm use a triangular table to estimate the minimum cost for translating different spans of a sentence based on the translation options available?",
            "reference-answers": [
                "The dynamic programming algorithm uses a triangular table to estimate the minimum cost for translating different spans of a sentence by filling it up in a diagonal or triangular manner. For each span in the sentence, the table stores the minimum cost required to translate that span. This is done by considering all possible ways to split the span into smaller parts and selecting the cheapest combination of translation costs for those parts. Starting from individual words, the algorithm builds up to larger spans, ensuring that the minimum cost for each span is computed based on the cheapest translation options available for its subparts. This systematic approach allows the algorithm to efficiently compute the minimum cost for translating any part of the sentence."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen37-slide25/text.txt": [
        {
            "question": "What is the reason for the difference in translation cost between function words and content words, as illustrated in the tourism initiative example?",
            "reference-answers": [
                "The difference in translation cost between function words and content words is due to the fact that function words, such as common phrases, are cheaper to translate than content words, which are harder to translate. For example, in the tourism initiative example, \"the first time\" is cheaper to translate than unusual phrases like \"tourism initiative addresses.\""
            ]
        },
        {
            "question": "What does the text state about the cost comparison between translating function words and content words?",
            "reference-answers": [
                "The text states that function words are cheaper to translate than content words."
            ]
        },
        {
            "question": "Question: Why are function words generally cheaper to translate than content words according to the text?",
            "reference-answers": [
                "Function words are generally cheaper to translate than content words because common phrases, which function words often form, are easier to translate than unusual or specific content words."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen38-slide25/text.txt": [
        {
            "question": "What is the total estimated cost for each hypothesis, and why is one considered better than the other?",
            "reference-answers": [
                "The total estimated cost for the hypothesis that translates the easy words first is **13.41**, while the total estimated cost for the hypothesis that translates the hard word first is **11.98**. The latter is considered better because the future cost estimate is lower, resulting in a smoother and more efficient translation process. By addressing the harder words first, the system avoids potential dead-end roads and ensures a more optimal path overall."
            ]
        },
        {
            "question": "Which hypothesis results in a lower total cost when considering both the current score and the future cost estimate, and why?",
            "reference-answers": [
                "The hypothesis that translates the hard word first results in a lower total cost. Its total score is 11.98, which is lower than the 13.41 total cost of the hypothesis that translates the easy words first. This is because the future cost estimate for the hard word hypothesis is lower, leading to a smoother and more efficient translation path."
            ]
        },
        {
            "question": "Which hypothesis results in a better total score and why?",
            "reference-answers": [
                "The hypothesis that translates the hard word first results in a better total score because it leads to a lower overall cost. The total score for this approach is 11.98, which is better than the 13.41 score of the other hypothesis. This is due to the lower future cost estimate when translating the hard word first, which avoids dead-end roads and ensures a smoother translation path."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen39-slide25/text.txt": [
        {
            "question": "What other decoding algorithms were mentioned in the text, and what additional method was referenced?",
            "reference-answers": [
                "The other decoding algorithms mentioned are A* and greedy hill climbing. Additionally, finite state transducers were referenced as another possible method."
            ]
        },
        {
            "question": "What other decoding algorithms were mentioned in the text?",
            "reference-answers": [
                "The other decoding algorithms mentioned are A* algorithm and greedy hill climbing."
            ]
        },
        {
            "question": "What decoding algorithms were mentioned as alternatives?",
            "reference-answers": [
                "The decoding algorithms mentioned as alternatives are A* and greedy hill climbing."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen40-slide25/text.txt": [
        {
            "question": "What is required for the A* Search to guarantee finding the best solution?",
            "reference-answers": [
                "An admissible heuristic is required for the A* Search to guarantee finding the best solution. An admissible heuristic is optimistic, meaning it never overestimates the future cost."
            ]
        },
        {
            "question": "What does the A-Star Search guarantee when an admissible heuristic is used?",
            "reference-answers": [
                "The A-Star Search guarantees that it will find the best (optimal) solution when an admissible heuristic is used."
            ]
        },
        {
            "question": "What is the term for a heuristic that never overestimates the future cost in the context of the A* Search algorithm?",
            "reference-answers": [
                "The term for a heuristic that never overestimates the future cost in the context of the A* Search algorithm is an **admissible heuristic**."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen41-slide25/text.txt": [
        {
            "question": "Question: What is the speaker affirming the ability to do in the text?",
            "reference-answers": [
                "do something"
            ]
        },
        {
            "question": "What is the speaker agreeing to in the text?",
            "reference-answers": [
                "proceed with the plan"
            ]
        },
        {
            "question": "What is the speaker indicating they can do?",
            "reference-answers": [
                "The speaker is confirming their ability to do something, but the specific action isn't provided in the text."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen42-slide25/text.txt": [
        {
            "question": "What is the main difference between local and non-local feature functions in phrase-based machine translation?",
            "reference-answers": [
                "The main difference between local and non-local feature functions in phrase-based machine translation is that local feature functions operate on individual phrases, such as phrase translation probability, phrase penalty, and word penalty, without considering context beyond the phrase itself. Non-local feature functions, like the language model, consider aspects that span across phrase boundaries, such as bigrams, requiring context from surrounding phrases to evaluate."
            ]
        },
        {
            "question": "What is the distinction between local and non-local feature functions in phrase-based machine translation, and provide an example from the text to illustrate this difference?",
            "reference-answers": [
                "In phrase-based machine translation, local feature functions are those that can be calculated for a given phrase independently of its context, such as the phrase translation probability, phrase penalty, and word penalty. These are based on the properties of individual phrases, like \"Peter\" or \"left for,\" without considering how they fit into the broader sentence structure.\n\nNon-local feature functions, on the other hand, consider properties that span across the boundaries of individual phrases. An example from the text is the language model, which evaluates bigrams like \"odeshel domu.\" This bigram spans the boundary between two phrases (\"left for\" and \"home\"), and its evaluation depends on the context of surrounding phrases, making it a non-local feature function."
            ]
        },
        {
            "question": "What is the distinction between local and non-local feature functions in phrase-based machine translation, and how does the language model exemplify a non-local feature?",
            "reference-answers": [
                "In phrase-based machine translation, local feature functions are those that can be calculated independently for each individual phrase without considering the broader context of the sentence. Examples include the phrase translation probability, phrase penalty, and word penalty. These features are associated with specific segments or \"puzzle pieces\" of the input sentence and do not require information about neighboring phrases.\n\nNon-local feature functions, on the other hand, consider the context or relationships between phrases. The language model exemplifies a non-local feature because it evaluates the overall fluency and grammatical correctness of the translated sentence, often based on n-grams that may span across phrase boundaries. For instance, the language model may assess the probability of a bigram like \"odeshel domu\" only after combining two separate translation phrases, which requires considering the context provided by the surrounding phrases. This dependency on the broader sentence structure makes the language model a non-local feature function."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen43-slide25/text.txt": [
        {
            "question": "What is the purpose of the external score in the minimum error rate training process described in the text, and how is it used to modify the model's behavior?",
            "reference-answers": [
                "The purpose of the external score in the minimum error rate training process is to evaluate the quality of the translations produced by the model and ensure that they align with human preferences. It is based on reference translations provided by an oracle (or a held-out test set) and assesses the translation quality after the entire process is completed. The external score is used to modify the model's behavior by adjusting the weights assigned to different components (e.g., word translation probabilities and language model scores). Instead of changing the internal probabilities of the model components directly, which would be too complex, the weights are tuned to influence the scoring of hypotheses during the beam search. By iteratively adjusting the weights and evaluating the resulting hypotheses with the external score, the model's internal scoring system is aligned to better match human judgment of translation quality. This process continues until the weight settings stabilize, ensuring that the model produces translations that are both internally consistent and externally validated as correct."
            ]
        },
        {
            "question": "What is the purpose of weight optimization in the context of machine learning training described in the text, and how is it achieved?",
            "reference-answers": [
                "The purpose of weight optimization in the context of machine learning training is to find the best balance of weights that influence which translation hypotheses survive during the beam search. This ensures that the model's internal scoring aligns with external quality assessments, such as human evaluations. It is achieved through an iterative process using minimum error rate training, where weights are adjusted to make the internal scores reflect the external quality of the translations. This process involves generating hypotheses, evaluating them with external scores, and updating the weights until they stabilize, ensuring the model produces higher-quality translations."
            ]
        },
        {
            "question": "What is the purpose of weight optimization in the described machine translation system, and how is it achieved through the iterative adjustment of weights based on internal and external scores?",
            "reference-answers": [
                "The purpose of weight optimization in the described machine translation system is to find the best balance of weights that influence the scoring of candidate translations. This ensures that the system prioritizes hypotheses that align with both internal model scores (based on word and phrase probabilities) and external scores (based on translation quality assessed against reference translations). \n\nWeight optimization is achieved through an iterative process where the system adjusts the weights to maximize the correlation between internal and external scores. Initially, weights are set, and hypotheses are scored using internal scores. These scores are then evaluated against external scores. If the internal scores do not match the external scores well, the weights are adjusted, and the process repeats. This continues until the weights are optimized, ensuring that the highest internal score corresponds to the best external score. The iterative adjustment of weights helps the system learn which hypotheses are most likely to produce high-quality translations according to user preferences."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen44-slide25/text.txt": [
        {
            "question": "What is the effect of a high phrase penalty on the system's segmentation of the input sentence?",
            "reference-answers": [
                "A high phrase penalty causes the system to prefer many segments when segmenting the input sentence, translating each phrase independently of the context. This results in the input sentence being split into more segments, such as six or seven instead of fewer, leading to a more piecewise translation."
            ]
        },
        {
            "question": "What are the two most important weights in a phrase-based machine translation model, and how does the phrase penalty influence the segmentation of the input sentence?",
            "reference-answers": [
                "The two most important weights in a phrase-based machine translation model are the phrase translation probability and the language model score. The phrase penalty influences the segmentation of the input sentence by controlling whether the system prefers many segments or few. A higher phrase penalty encourages more segmentation, translating each phrase independently, while a lower phrase penalty favors fewer, longer segments."
            ]
        },
        {
            "question": "What is the effect of a high phrase penalty on the segmentation of the input sentence during translation?",
            "reference-answers": [
                "A high phrase penalty causes the system to prefer many segments when segmenting the input sentence, resulting in each phrase being translated independently of the context."
            ]
        }
    ],
    "nmt-class/lecture05-pbmt/screen45-slide25/text.txt": [
        {
            "question": "What are some tasks for which the Moses decoder is still used despite the prevalence of neural machine translation?",
            "reference-answers": [
                "The Moses decoder is still used for tasks such as porting treebank annotations across languages, where a more literal translation is needed, and for languages with extremely small datasets. Additionally, its phrase independence assumption can be valid for certain tasks beyond machine translation."
            ]
        },
        {
            "question": "What is the current primary use of Moses decoder according to the text?",
            "reference-answers": [
                "The current primary use of Moses decoder is for linguistic analysis, particularly in tasks that require a more literal translation, such as porting treebank annotation across languages, especially when dealing with languages that have small datasets or specific tasks where the assumption of phrase independence is valid."
            ]
        },
        {
            "question": "Moses decoder.",
            "reference-answers": [
                "Moses decoder is the best implementation for phrase-based machine translation (MT). Although neural MT is more commonly used today, Moses is still valuable for tasks like linguistic analysis where a literal translation is needed. It is effective for mapping sequences, such as transferring annotations across languages. However, it is not widely used for large datasets due to its assumption of phrase independence, which may not hold for all natural languages but can be suitable for other tasks."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen01-slide01/text.txt": [
        {
            "question": "What is the main focus of the lecture on machine translation discussed in the TEXT?",
            "reference-answers": [
                "The main focus of the lecture is morphology in machine translation."
            ]
        },
        {
            "question": "What is the main topic of today's lecture?",
            "reference-answers": [
                "The main topic of today's lecture is morphology in machine translation."
            ]
        },
        {
            "question": "What is the next topic to be discussed in the lecture on machine translation?",
            "reference-answers": [
                "Morphology in machine translation."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen02-slide02/text.txt": [
        {
            "question": "What two problems or approaches related to rich morphology in Czech machine translation are discussed in the lecture, and how does each handle the complexity?",
            "reference-answers": [
                "The lecture discusses two main approaches related to rich morphology in Czech machine translation:  \n1. **Combinatorial Explosion in Phrase-Based MT**: The complexity of Czech morphology leads to a combinatorial explosion in phrase-based machine translation, where the number of possible word forms and combinations becomes overwhelming.  \n2. **Neural MT and Rich Morphology**: Neural machine translation (Neural MT) handles rich morphology more effectively by generalizing patterns through its data-driven approach, reducing the need to explicitly manage the combinatorial complexity."
            ]
        },
        {
            "question": "What are the main ways Neural Machine Translation handles the challenges posed by Czech's rich morphology, and how do these approaches differ from those used in phrase-based machine translation?",
            "reference-answers": [
                "Neural Machine Translation (NMT) handles the challenges posed by Czech's rich morphology through its end-to-end training approach and the use of attention mechanisms, which allow it to better capture context and handle complex morphological phenomena. In contrast, phrase-based machine translation deals with morphology by relying on pre-defined phrase tables and combinatorial rules, which can lead to a \"combinatorial explosion\" due to the high complexity of Czech morphology. NMT's ability to model relationships between words and phrases more holistically helps mitigate these challenges more effectively than phrase-based systems."
            ]
        },
        {
            "question": "What is discussed about phrase-based machine translation in the context of Czech?",
            "reference-answers": [
                "The lecture discusses how phrase-based machine translation deals with the combinatorial explosion caused by the rich morphology of the Czech language."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen03-slide03/text.txt": [
        {
            "question": "Why does English have fewer morphological tags compared to Czech, and how does this difference affect the sentence structure in each language?",
            "reference-answers": [
                "English has fewer morphological tags compared to Czech because its morphological system is much simpler and less varied. In Czech, the morphological system is highly complex, allowing for over 4,000 possible tags, which reflect the wide variety of word forms and grammatical information encoded in the language. English, on the other hand, has a much simpler morphological system, with only around 50 tags, because its word forms vary much less and do not encode as much grammatical information.\n\nThis difference affects sentence structure in each language. In Czech, the rich morphology allows for more flexibility in word order because the grammatical relationships between words are indicated by their forms (e.g., case, number, gender). This means that the syntax of a sentence is less dependent on the order of words and more on the morphological features of the words themselves. In English, however, the morphological system is much simpler, so the sentence structure relies more heavily on word order to convey meaning. English speakers must follow strict word order to make the sentence grammatically correct, as the language does not encode as much grammatical information in its word forms."
            ]
        },
        {
            "question": "Why does English have fewer morphological tags compared to Czech and how does this affect the structure of sentences in each language?",
            "reference-answers": [
                "English has fewer morphological tags compared to Czech because its word form variety is considered less complex, leading linguists to use a smaller number of tags to describe them. This difference affects sentence structure: in English, the lack of morphological detail means sentence meaning relies heavily on word order. In contrast, Czech's extensive morphological system allows for more flexibility in word order, as the forms themselves encode grammatical relationships, making the syntax less dependent on sequence."
            ]
        },
        {
            "question": "How does the number of morphological tags in Czech compare to that in English, and what implications does this have for the structure and word order in each language?",
            "reference-answers": [
                "Czech has over 4,000 morphological tags, while English has only 50. This large number of tags in Czech indicates a highly complex morphological system, allowing speakers to express meaning through word forms, which often enables greater flexibility in word order. In contrast, English's limited number of tags suggests that word forms are less differentiated, requiring more reliance on word order and sentence structure to convey meaning."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen04-slide04/text.txt": [
        {
            "question": "The main challenge in translating a sentence into Czech, according to the text, is the complexity of the language's grammatical structures, including seven cases, four genders, three numbers for nouns and adjectives, and similarly intricate verb forms. This complexity leads to a combinatorial explosion, requiring careful selection of correct endings and forms.",
            "reference-answers": [
                "The main challenge in translating a sentence into Czech is the complexity of its grammatical structures, which include seven cases, four genders, and three numbers for nouns and adjectives. Additionally, verbs have similarly intricate forms. This complexity leads to a combinatorial explosion, requiring careful selection of correct endings and forms to ensure accuracy."
            ]
        },
        {
            "question": "What challenges do translators face when dealing with the combinatorial explosion of word endings in Czech, and why is this understanding crucial for accurate translation?",
            "reference-answers": [
                "Translators face the challenge of the combinatorial explosion due to Czech's numerous word endings, influenced by seven cases, four genders, three numbers, and complex verb forms. This complexity requires careful selection of the correct endings to ensure accurate meaning. Understanding these intricacies is crucial because it allows translators to choose the appropriate word forms, maintaining the intended meaning and clarity in the translation. Without this understanding, the translation risks losing the correct sense or becoming unclear."
            ]
        },
        {
            "question": "What is the main challenge when translating a sentence into Czech?",
            "reference-answers": [
                "The main challenge when translating a sentence into Czech is the complexity of its grammar, which includes seven cases, four genders, and three numbers for nouns and adjectives, as well as a similarly complex system for verbs. This requires choosing not only the correct meaning of words but also their appropriate grammatical endings, leading to a combinatorial explosion where only a few combinations are correct, demanding a thorough understanding of the language's structure."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen05-slide05/text.txt": [
        {
            "question": "What is agglutination and how does it differ from compounding according to the text?",
            "reference-answers": [
                "Agglutination is a method of word formation where prefixes, suffixes, infixes, and other modifications are added to a root word to express variations in meaning. It differs from compounding, which involves combining whole words into a single compound word. In agglutination, the focus is on modifying the word form itself, rather than joining separate words. For example, the word \"istua\" in Finnish/Hungarian is formed by adding such elements, whereas in compounding, German words like \"RindfleischetikettierungundÜberwachungaufstrahberundübertreibunggesetz\" are created by combining separate words."
            ]
        },
        {
            "question": "What is the difference between compounding and agglutination as phenomena in word formation, and how do they manifest in the examples provided in the text?",
            "reference-answers": [
                "Compounding involves combining whole words or word parts to form a new word, as seen in the German example where the long word is created by joining several existing words without spaces. Agglutination, on the other hand, involves adding prefixes, suffixes, or other affixes to a root word to create variations in meaning, as illustrated by the Finnish example \"istua,\" which can be modified with affixes to express different nuances. While compounding joins entire words, agglutination modifies a single root word with added elements."
            ]
        },
        {
            "question": "**Question:**  \nWhich morphological phenomenon is described in the text, where words are formed by using suffixes, prefixes, and other modifications to express variations in meaning?",
            "reference-answers": [
                "The morphological phenomenon described is agglutination."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen06-slide06/text.txt": [
        {
            "question": "What is the maximum BLEU score improvement possible if a machine translation system produces perfect word forms in the phrase-based machine translation era?",
            "reference-answers": [
                "The maximum BLEU score improvement possible is eight points."
            ]
        },
        {
            "question": "What is the maximum increase in BLEU score achievable by ensuring perfect word forms in machine translation systems?",
            "reference-answers": [
                "The maximum increase in BLEU score achievable by ensuring perfect word forms in machine translation systems is eight points."
            ]
        },
        {
            "question": "What is the potential improvement in BLEU scores if morphology problems are ignored in phrase-based machine translation systems?",
            "reference-answers": [
                "The potential improvement in BLEU scores could increase by up to eight points."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen07-slide07/text.txt": [
        {
            "question": "What is the approach to handling morphology in phrase-based machine translation?",
            "reference-answers": [
                "In phrase-based machine translation, morphology is typically handled by segmenting words into their morphological components, such as roots, prefixes, and suffixes, so that each part can be translated individually or combined in the target language."
            ]
        },
        {
            "question": "What is the approach to handling morphology in phrase-based machine translation?",
            "reference-answers": [
                "The text does not provide an answer to the question. It only poses the question about handling morphology in phrase-based machine translation."
            ]
        },
        {
            "question": "What is the main topic addressed in the provided text?",
            "reference-answers": [
                "The main topic addressed in the provided text is how to handle morphology in phrase-based machine translation."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen08-slide08/text.txt": [
        {
            "question": "What are the two major components of phrase-based systems, and why is the global search considered more complicated?",
            "reference-answers": [
                "The two major components of phrase-based systems are the translation model, which is based on parallel data, and the language model, which is based on modeling data. The global search is considered more complicated because it involves many more options to choose from, leading to a larger search space."
            ]
        },
        {
            "question": "What causes the complexity in the global search process in phrase-based systems?",
            "reference-answers": [
                "The complexity in the global search process in phrase-based systems is caused by the many options available to choose from, leading to a larger search space."
            ]
        },
        {
            "question": "What are the two major components of phrase-based systems, and how does the global search process become more complex?",
            "reference-answers": [
                "The two major components of phrase-based systems are the translation model, which is based on parallel data, and the language model, which is based on modeling data. The global search process becomes more complex because you have many more options to choose from, leading to a larger search space."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen09-slide09/text.txt": [
        {
            "question": "Why is the second candidate \"dva zelené pruhovány kočky\" problematic despite the first three words being correct?",
            "reference-answers": [
                "The second candidate \"dva zelené pruhovány kočky\" is problematic because while the first three words (\"dva zelené pruhovány\") fit together perfectly, the last word \"kočky\" does not agree with the preceding words grammatically. This creates a conflict in the sentence structure, making it incorrect as a whole."
            ]
        },
        {
            "question": "What issue does the triagram language model have in ensuring grammatical agreement in the translation of the sentence \"I saw two green striped cats\" into Czech?",
            "reference-answers": [
                "The triagram language model has difficulty ensuring grammatical agreement in the translation of the sentence because it only considers sequences of three words (triagrams), which makes it insufficient to capture the full grammatical structure and agreement of the entire sentence. This limitation causes issues where individual triagrams may appear correct, but the overall sentence lacks coherence and proper agreement."
            ]
        },
        {
            "question": "What is the main issue with using a triagram language model for translating the sentence \"I saw two green striped cats\" into Czech, and what is required to overcome this issue?",
            "reference-answers": [
                "The main issue with using a triagram language model for translating the sentence \"I saw two green striped cats\" into Czech is that it cannot ensure full grammatical agreement across all word forms in the sentence. A triagram model only considers sequences of three words at a time, which is insufficient to capture the entire sentence's agreement. To overcome this issue, a larger corpus is required that includes all possible combinations of colors, patterns, and animals in various contexts. This corpus would allow the language model to learn and select the correct word forms that agree with each other across the entire sentence."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen10-slide10/text.txt": [
        {
            "question": "What is the morphological ambiguity in the phrase \"dvě zelené pruchované kočky\" according to the text?",
            "reference-answers": [
                "The morphological ambiguity in the phrase \"dvě zelené pruchované kočky\" is that the forms can be interpreted as either nominative or accusative case, making it unclear which case is intended."
            ]
        },
        {
            "question": "What is the morphological ambiguity present in the phrase \"dvě zelené pruchované kočky\"?",
            "reference-answers": [
                "The morphological ambiguity in the phrase \"dvě zelené pruchované kočky\" lies in whether it is in the nominative or accusative case."
            ]
        },
        {
            "question": "What is the morphological ambiguity present in the noun phrase \"dvě zelené pruchované kočky\" as discussed in the text?",
            "reference-answers": [
                "The morphological ambiguity in the noun phrase \"dvě zelené pruchované kočky\" lies in the fact that the forms are ambiguous regarding whether they are in the nominative or accusative case."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen11-slide11/text.txt": [
        {
            "question": "What are the advantages of modifying language models to use morphological text instead of word forms, and can you provide an example from the text where this approach does not help?",
            "reference-answers": [
                "Modifying language models to use morphological text instead of word forms offers several advantages. First, it allows the model to focus on the combinations of adjectives and nouns, ensuring grammatical agreement by observing more instances of any noun and adjective combination, leading to more reliable probability estimates. Additionally, the vocabulary size is significantly reduced, from millions of word forms to just thousands of morphological forms, enabling the use of longer engrams and enhancing the system's grammatical coherence.\n\nHowever, this approach does not help in cases where the question revolves around the probability of different word forms, such as comparing \"dvě zelené\" (feminine nominative) to \"dva zelené\" (masculine nominative). In such instances, the morphological text approach cannot effectively resolve the issue, as it lacks the context to determine the appropriate grammatical form."
            ]
        },
        {
            "question": "What is one advantage of modifying language models to use morphological text instead of word forms?",
            "reference-answers": [
                "One advantage is that by using morphological text, the vocabulary size is significantly reduced, allowing the model to use longer sequences and improve grammatical coherence, as it can reliably estimate probabilities based on a smaller set of morphological forms rather than millions of word forms."
            ]
        },
        {
            "question": "What is a significant advantage of modifying language models to analyze morphological text instead of word forms in machine translation systems?",
            "reference-answers": [
                "A significant advantage is that analyzing morphological text reduces the vocabulary size, allowing the use of longer engrams, which enhances the system's ability to ensure grammatical coherence in sentences."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen12-slide12/text.txt": [
        {
            "question": "Why does the translation of the English word phrase \"kneecaps\" into Czech require a corpus of 50 million sentence pairs?",
            "reference-answers": [
                "The translation of the English word phrase \"kneecaps\" into Czech requires a corpus of 50 million sentence pairs because the Czech language has a rich morphology, meaning words can have many forms depending on grammatical cases. For the word \"Češky\" (the translation of \"kneecaps\"), there are seven cases and numbers, with some forms appearing identically across multiple cases. To accurately determine the correct case and form in context, the model needs to observe numerous examples of how these forms are used in actual sentences. Specifically, the instrumental case of \"Češky\" (\"Češkami\") may only appear in a corpus of this size, ensuring the model can reliably propose the correct translation."
            ]
        },
        {
            "question": "Why does translating the English word \"kneecaps\" into Czech require a corpus of 50 million sentence pairs, and what does this indicate about the challenges of rich morphology in translation models?",
            "reference-answers": [
                "Translating the English word \"kneecaps\" into Czech requires a corpus of 50 million sentence pairs because Czech morphology is highly complex, with words having many forms depending on grammatical cases and numbers. For example, \"kneecaps\" translates to \"Češky,\" which can appear in multiple cases (nominative, accusative, vocative) with the same surface form. This ambiguity means the translation model must consider all possible cases and their contexts to determine the correct form. The need for extensive data highlights the challenge of handling rich morphology in translation models, as it demands a large dataset to capture all necessary word forms and their usage contexts."
            ]
        },
        {
            "question": "The translation model requires a corpus of 50 million sentence pairs to accurately produce the instrumental case of \"Češky\" in Czech, as this extensive dataset allows the model to learn the various forms and contexts needed for correct translation.",
            "reference-answers": [
                "The translation model requires a corpus of 50 million sentence pairs to accurately produce the instrumental case of \"Češky\" in Czech, as this extensive dataset allows the model to learn the various forms and contexts needed for correct translation."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen13-slide13/text.txt": [
        {
            "question": "What is a key benefit of the factored phrase-based model's approach to morphological translation using monolingual corpora?",
            "reference-answers": [
                "A key benefit is that the generation step can be trained independently using monolingual corpora, allowing it to produce correct forms even when parallel data is insufficient."
            ]
        },
        {
            "question": "What is the primary goal of separating the translation of a word's lemma from its morphology in the factored phrase-based model?",
            "reference-answers": [
                "The primary goal is to enable the system to train the generation step independently using monolingual data, allowing it to produce correct word forms, such as \"instrumental for kneecaps,\" which might not be available in limited parallel data."
            ]
        },
        {
            "question": "What is the main benefit of the factored lemma and morphological tag approach in machine translation?",
            "reference-answers": [
                "The main benefit is that the generation step can be trained independently using monolingual data, allowing it to handle cases not covered in parallel data, such as producing \"instrumental for kneecaps.\""
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen14-slide14/text.txt": [
        {
            "question": "What are the two types of coherence ensured in the described machine translation model, and how are they achieved?",
            "reference-answers": [
                "The two types of coherence ensured in the described machine translation model are vertical and horizontal. Vertical coherence is achieved through the generation steps, which ensure consistency between the source and target factors, as well as between the word forms and their corresponding lemmas and tags. Horizontal coherence is ensured by the target side language model, which guarantees that the sequence of words in the target language forms a coherent and grammatically correct structure."
            ]
        },
        {
            "question": "What are the two main components ensuring coherence in the generative model, and how do they function?",
            "reference-answers": [
                "The two main components ensuring coherence in the generative model are the vertical coherence and the horizontal coherence. Vertical coherence is ensured by the generation steps, which map source factors to target factors, ensuring consistency between the form, tag, and lemma created during the mapping steps. Horizontal coherence is ensured by the target side language model, which ensures that the sequence of words is coherent and makes sense in the target language."
            ]
        },
        {
            "question": "What are the key components of the factor phrase-based machine translation model discussed in the text, and how do they contribute to the translation process?",
            "reference-answers": [
                "The key components of the factor phrase-based machine translation model discussed in the text are:\n\n1. **Mapping Steps**: These translate phrases of source factors to target factors, such as mapping word forms directly (e.g., \"two green\" to \"dvia zelené\"). This ensures a direct correspondence between source and target word forms.\n\n2. **Generation Steps**: These produce new target factors from already known target side factors. For example, \"dvia\" can be generated as a feminine noun and \"dva\" as a masculine nominative. These steps involve tagging on the fly, ensuring that the model knows the possible tags for a given word form.\n\n3. **Vertical Coherence**: This is maintained by the generative model, ensuring consistency between the word form, its tag, and the lemma created from the source. This coherence ensures that the translation aligns correctly across different levels of linguistic structure.\n\n4. **Horizontal Coherence**: This is ensured by the target side language model, which maintains the consistency and flow of words in the sequence. It ensures that the translated words form a coherent and meaningful sequence.\n\n5. **Language Models for Morphological Tags**: The model can also apply a language model to the morphological tags, not just the word forms. This allows for the consideration of tag sequences and their coherence, contributing to the overall translation quality.\n\nEach of these components works together to ensure that the translation is both consistent and coherent, both vertically and horizontally, resulting in a more accurate and natural translation."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen15-slide15/text.txt": [
        {
            "question": "What is the rationale behind the order of translation and generation steps in the described process?",
            "reference-answers": [
                "The rationale behind the order of translation and generation steps is to manage the number of candidates efficiently. By first translating the lemma, the process narrows down the possibilities, after which generation expands the options based on morphological tags from the target language corpus. This sequence helps prevent an explosion of candidates by scoring and prioritizing them, ensuring the system operates within memory constraints while maintaining the ability to produce forms not seen in the parallel data."
            ]
        },
        {
            "question": "What steps does the system take to translate \"kneecaps\" into Czech, and how does it ensure the correct word form is generated?",
            "reference-answers": [
                "The system translates \"kneecaps\" into Czech through a predefined sequence of steps: first translating, then generating, translating again, and generating a final result. Initially, \"kneecaps\" is analyzed into lemmas \"knee\" and \"cap,\" with the second word identified as plural. The first translation step converts \"kneecaps\" to its base form \"Češka.\" The first generation step explores possible morphological tags for \"Češka\" using a monolingual corpus, considering singular and plural forms across different cases. The second translation step aligns the plural tag from the source with potential plural tags in the target. The second generation step then produces the correct word form based on the monolingual data, ensuring the accurate translation of \"kneecaps\" into Czech. This process ensures the correct word form is generated by leveraging monolingual corpora and predefined processes."
            ]
        },
        {
            "question": "What does the system described in the text use to manage the number of translation candidates it generates?",
            "reference-answers": [
                "The system described in the text uses an optimized order of joining tables (generation and mapping steps) to manage the number of translation candidates it generates, thereby reducing the memory complexity of the joint operation."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen16-slide16/text.txt": [
        {
            "question": "What are the components that contribute to the translation score in the factored phrase-based empty modal model?",
            "reference-answers": [
                "The components that contribute to the translation score in the factored phrase-based empty modal model include: (1) engrams of the language model, (2) engrams of morphological tags, (3) engrams of lemmas, and (4) phrase translations based on lemma sequences and morphological tags of the source and target languages."
            ]
        },
        {
            "question": "What is the key aspect of the factored phrase-based empty modal and how does it extend the phrase-based modal?",
            "reference-answers": [
                "The key aspect of the factored phrase-based empty modal is that it extends the phrase-based modal by incorporating additional features into the scoring process. It does this by considering not only word forms but also morphological tags, lemmas, and phrase translations. This extension fits well within the log-linear structure of the phrase-based model, allowing for a more comprehensive scoring of translations."
            ]
        },
        {
            "question": "What additional components are considered in the scoring process for translation using the factored phrase-based empty modal model?",
            "reference-answers": [
                "The additional components considered in the scoring process include engrams of morphological tags, engrams of lemmas, and the translation quality based on lemma sequences and morphological tags in both source and target languages."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen17-slide17/text.txt": [
        {
            "question": "What is the basis for extracting phrases in phrase-based machine translation?",
            "reference-answers": [
                "The basis for extracting phrases in phrase-based machine translation is the word alignments, from which phrases consistent with these alignments are extracted."
            ]
        },
        {
            "question": "How is phrase extraction accomplished in phrase-based MT, based on the given text?",
            "reference-answers": [
                "Phrase extraction in phrase-based MT is accomplished by extracting phrases that are consistent with the word alignments, which is a straightforward process."
            ]
        },
        {
            "question": "What is the basis for extracting phrases in phrase-based machine translation?",
            "reference-answers": [
                "The basis for extracting phrases in phrase-based machine translation is the word alignments, from which phrases consistent with these alignments are extracted."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen18-slide18/text.txt": [
        {
            "question": "How would you extend the translation of \"naturally John can\" into another language by incorporating morphological or part of speech information?",
            "reference-answers": [
                "To extend the translation of \"naturally John can,\" you would incorporate the morphological or part of speech information by translating \"can\" into its past participle form in the target language. For example, in another language, \"can\" might be translated as \"could\" or its past participle equivalent, depending on the verb's form and usage."
            ]
        },
        {
            "question": "What would you need to do to extend the translation from \"naturally John can be translated as naturally John has\" by using morphological and part-of-speech analysis?",
            "reference-answers": [
                "To extend the translation from \"naturally John can be translated as naturally John has\" by using morphological and part-of-speech analysis, you would need to analyze the grammatical structure and word forms of the sentence pair. Specifically, you would identify the parts of speech (e.g., verb forms) and morphological components (e.g., tense, aspect) in both the original and translated sentences to systematically apply similar transformations to other sentence pairs."
            ]
        },
        {
            "question": "What does the text suggest about extending the translation using morphological or part of speech text?",
            "reference-answers": [
                "The text suggests that by using morphological or part of speech information, we can extend the translation beyond just the words themselves, as demonstrated by translating \"John can be translated as naturally John has.\""
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen19-slide19/text.txt": [
        {
            "question": "What is one way that syntactic patterns differ between languages when considering part of speech, as demonstrated in the example of adverbs?",
            "reference-answers": [
                "One way syntactic patterns differ between languages when considering part of speech is that if a sentence starts with an adverb, in German the verb goes second and then comes the subject, while in English the subject is still first and the verb is second."
            ]
        },
        {
            "question": "What are the differences in sentence structure between German and English when a sentence starts with an adverb?",
            "reference-answers": [
                "When a sentence starts with an adverb, the structure differs between German and English. In German, the verb comes second, followed by the subject, whereas in English, the subject is first, followed by the verb. \n\n**Answer:** In German, the sentence structure is adverb → verb → subject, while in English, it is adverb → subject → verb."
            ]
        },
        {
            "question": "What structural differences are there between German and English when a sentence starts with an adverb?",
            "reference-answers": [
                "When a sentence starts with an adverb, German places the verb second and the subject after the verb, while English keeps the subject first and the verb second."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen20-slide20/text.txt": [
        {
            "question": "What grammatical cases are used in the two sentence pairs mentioned in the text?",
            "reference-answers": [
                "The grammatical cases used are the accusative case for the old man and the dative case for the black dog."
            ]
        },
        {
            "question": "What is the benefit of using factored models when working with sentence pairs that involve different grammatical cases in a corpus?",
            "reference-answers": [
                "The benefit of using factored models is that they allow the model to handle different grammatical cases more effectively by separating them, which can lead to more accurate processing of the sentence pairs."
            ]
        },
        {
            "question": "What is the benefit of using factored models as demonstrated with the two sentence pairs involving an old man in the accusative case and a black dog in the dative case?",
            "reference-answers": [
                "The benefit of using factored models is that they can handle different grammatical cases, such as accusative and dative, in separate components, allowing for more accurate processing of sentence pairs like \"an old man in the accusative case\" and \"a black dog in the dative case.\""
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen21-slide21/text.txt": [
        {
            "question": "What process is described for working with fully inflected word forms, and what is its purpose?",
            "reference-answers": [
                "The process described involves extracting a phrase table from a database that contains fully inflected word forms, such as translating phrases like \"starého pána\" (old man) or \"černému psobe\" (black dog). The purpose is to work with the inflected forms of words for accurate translation."
            ]
        },
        {
            "question": "What happens if you break the system that extracts phrase tables from fully inflected word forms for translation?",
            "reference-answers": [
                "If you break the system that extracts phrase tables from fully inflected word forms for translation, the translations would be incorrect because the system would not be able to recognize or translate the fully inflected word forms properly. For example, it would fail to translate phrases like \"starého pána\" (old man) or \"černému psobe\" (black dog) accurately, leading to loss of meaning and accuracy in the translations."
            ]
        },
        {
            "question": "What happens if you break it when working with the phrase table extracted from the database containing fully inflected word forms?",
            "reference-answers": [
                "If you break the phrase table extracted from the database containing fully inflected word forms, you would not be able to translate correctly. For example, you might not know how to translate \"starého pána\" (old man) or \"černému psobe\" (black dog) properly."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen22-slide22/text.txt": [
        {
            "question": "What two items were you instructed to extract separately from the text?",
            "reference-answers": [
                "The two items to extract separately are:\n1. Separate phrases (lemmas)\n2. Separate tables (translations)"
            ]
        },
        {
            "question": "What is the suggested method for preparing for an exam according to the text?",
            "reference-answers": [
                "The suggested method is to break down study material into separate phrases and tables. One table is for lemmas, which helps in translating terms like \"starý pán\" as old man and \"černý pás\" as black dog. This approach allows for organized translation and presumably effective exam preparation."
            ]
        },
        {
            "question": "What are the two steps suggested for translating phrases into English according to the text?",
            "reference-answers": [
                "The two steps suggested are:  \n1. Extract separate phrases (e.g., translating \"starý pán\" as old man and \"černý pás\" as black dog).  \n2. Separate tables, specifically for the lemmas."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen23-slide23/text.txt": [
        {
            "question": "What is the benefit of translating either accusative or dative to the sequence of an adjective and noun in English?",
            "reference-answers": [
                "The benefit is that you will learn how to translate either accusative or dative cases into the correct sequence of an adjective and noun in English."
            ]
        },
        {
            "question": "What can you translate either accusative or dative to in English?",
            "reference-answers": [
                "the sequence of an adjective and noun in English"
            ]
        },
        {
            "question": "What is the sequence of an adjective and noun in English when translating either the accusative or dative case?",
            "reference-answers": [
                "adjective and noun"
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen24-slide24/text.txt": [
        {
            "question": "What method is used to gain increased robustness by combining lemmas and morphological text from parallel data in a way that hasn't been seen together before?",
            "reference-answers": [
                "The method used is a combination of lemmas and morphological text from parallel data, factorized to integrate knowledge from different phrases, enhancing robustness."
            ]
        },
        {
            "question": "What allows the model to combine lemmas and morphological text from different sources to enhance robustness?",
            "reference-answers": [
                "The model's ability to factorize allows it to combine lemmas and morphological text from different sources, enhancing robustness."
            ]
        },
        {
            "question": "How does the factorization method improve robustness in the system?",
            "reference-answers": [
                "The factorization method improves robustness by enabling the system to combine lemmas and morphological text from different sources, even if they haven't been seen together before. This allows the system to use knowledge from separate phrases, such as the accusative knowledge from \"the old man\" and the lexical knowledge from \"the black dog,\" leading to increased flexibility and reliability."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen25-slide25/text.txt": [
        {
            "question": "What is the origin of the slides presented in the text?",
            "reference-answers": [
                "The slides originated from a presentation by Philip Kahn at MT Marathon more than 10 years ago."
            ]
        },
        {
            "question": "Question: Who presented the slides that are the basis for this exam?",
            "reference-answers": [
                "Philip Kahn"
            ]
        },
        {
            "question": "What is the source of the slides presented in the exam?",
            "reference-answers": [
                "The slides presented in the exam are based on what Philip Kahn has presented at MT Marathon more than 10 years ago and include information about decoding how to translate with the model and some experiments with that."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen26-slide26/text.txt": [
        {
            "question": "What is the first step in phrase-based machine translation according to the text?",
            "reference-answers": [
                "The first step is consulting the phrase tables to find possible translations of the input phrases."
            ]
        },
        {
            "question": "Which example sentence is used in the text to illustrate the process of phrase-based machine translation?",
            "reference-answers": [
                "Er geht Janich nach Hause."
            ]
        },
        {
            "question": "What does a phrase-based machine translation system do with the input sequence of words?",
            "reference-answers": [
                "A phrase-based machine translation system takes the input sequence of words, consults its phrase tables to find all possible translations of these phrases, and considers both the longer and shorter ones to determine the best translation."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen27-slide27/text.txt": [
        {
            "question": "What is used to find the best selections?",
            "reference-answers": [
                "The best selections are used to find the best selections in the search."
            ]
        },
        {
            "question": "What is used to search for the best selections?",
            "reference-answers": [
                "The best selections are used to search for the best selections."
            ]
        },
        {
            "question": "What is used to identify the best options when searching?",
            "reference-answers": [
                "The selection of those which are the best."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen29-slide30/text.txt": [
        {
            "question": "What does the text describe as the starting point for the process of generating questions?",
            "reference-answers": [
                "The starting point for the process of generating questions is the empty hypothesis."
            ]
        },
        {
            "question": "What is the approach when starting with an empty hypothesis and going further?",
            "reference-answers": [
                "I'm unable to answer the question as the provided text is empty. Please provide the necessary information so I can assist you."
            ]
        },
        {
            "question": "What is the starting point of the process described in the text?",
            "reference-answers": [
                "The starting point is the empty hypothesis."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen30-slide31/text.txt": [
        {
            "question": "What action is the teacher performing while preparing for the exam?",
            "reference-answers": [
                "The teacher is adding words from the source to the current hypothesis while preparing for the exam."
            ]
        },
        {
            "question": "What process involves adding words from the source and appending them to the current hypothesis?",
            "reference-answers": [
                "The process is called \"adding.\" \n\nANSWER: adding"
            ]
        },
        {
            "question": "What is the process described in the text for enhancing the current hypothesis using words from the target?",
            "reference-answers": [
                "The process described involves adding words from the target and appending the current hypothesis with the words in the target."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen31-slide33/text.txt": [
        {
            "question": "What criteria is used to decide the final choice?",
            "reference-answers": [
                "The criteria used to decide the final choice is whether it covers everything and has the highest overall probability."
            ]
        },
        {
            "question": "What is the basis for making the final choice?",
            "reference-answers": [
                "The basis for making the final choice is the path which covers everything and has the overall highest probability."
            ]
        },
        {
            "question": "What is the main consideration when making the final choice according to the text?",
            "reference-answers": [
                "The main consideration is choosing the path that covers everything and has the overall highest probability."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen32-slide34/text.txt": [
        {
            "question": "What is the more complex aspect of creating translation options with factored decoding, and how is it constructed?",
            "reference-answers": [
                "The more complex aspect of creating translation options with factored decoding is the construction of these options through a sequence of mapping steps. Instead of a single lookup in a simple table, the process involves translating lemmas to lemmas, morphology to morphology, and then generating the surface form of the target word. For example, the source word \"house\" can be translated into \"houses,\" \"homes,\" \"buildings,\" or \"shelves,\" with each translation built using the \"join operations\" of SQL tables, similar to how data is combined in a structured manner."
            ]
        },
        {
            "question": "How are translation options created in factored decoding, and what analogy is used to explain their construction?",
            "reference-answers": [
                "Translation options in factored decoding are created through a sequence of mapping steps, where lemmas are translated to lemmas, morphology to morphology, and then the surface form is generated. The construction of these options is likened to the join operations of SQL tables, where different parts are combined to form the correct word form. For example, the source word \"house\" can be translated into \"houses,\" \"homes,\" \"buildings,\" or \"shelves,\" with each translation built using this mapping process."
            ]
        },
        {
            "question": "How are translation options constructed in factored decoding, and what is the example used to explain this process?",
            "reference-answers": [
                "Translation options in factored decoding are constructed through a sequence of mapping steps, which is more complex than a single lookup in a simple table. The process involves mapping lemma to lemma, morphology to morphology, and then generating the surface form of the target word. An example provided is the source word \"house,\" which can be translated into \"houses,\" \"homes,\" \"buildings,\" or \"shelves.\" These translation options are built using the join operations of the SQL tables, similar to how data is combined from different tables."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen33-slide35/text.txt": [
        {
            "question": "What is different about the translation options now compared to before?",
            "reference-answers": [
                "The translation options are now constructed on the fly and there are many more of them compared to before."
            ]
        },
        {
            "question": "What can be pre-computed to enable the creation of more verbose translation options during the search process?",
            "reference-answers": [
                "The join of tables can be pre-computed to enable the creation of more verbose translation options during the search process."
            ]
        },
        {
            "question": "What is the purpose of pre-computing the processing steps, and how does it affect the construction of translation options during the search?",
            "reference-answers": [
                "The purpose of pre-computing the processing steps is to make the process more efficient by completing these steps once during the initial setup of the translation options table. This allows the translation options to be more numerous and detailed, as they are constructed dynamically during the search without altering the search process itself. The search remains unchanged, but the translation options are generated on the fly, making the process more flexible and efficient."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen34-slide36/text.txt": [
        {
            "question": "What happens if the pruning limits are exceeded during the preparation of translation options?",
            "reference-answers": [
                "If the pruning limits are exceeded during the preparation of translation options, the system will drop some necessary candidates, thereby limiting its capability and potentially leading to incorrect wording of the sentence."
            ]
        },
        {
            "question": "**Question:** Why is pruning necessary when preparing translation options?",
            "reference-answers": [
                "Pruning is necessary when preparing translation options because it helps manage the exponential growth of possible word forms, which can easily escalate into thousands of options. Without pruning, the system could become overwhelmed, leading to increased memory complexity and potential limitations in the system's capability. Pruning ensures that only the best candidates are selected, preventing the loss of necessary translation options that are crucial for the correct wording of the sentence."
            ]
        },
        {
            "question": "What is a potential consequence of excessive pruning when dealing with a large number of translation options due to morphological richness?",
            "reference-answers": [
                "A potential consequence is that the system might drop necessary translation options, limiting its capability and potentially leading to incorrect wording of the sentence."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen35-slide38/text.txt": [
        {
            "question": "What was the purpose of using a 7-gram language model on part of speech tags in the described experiment?",
            "reference-answers": [
                "The purpose was to impose some syntactic structure over the sentence by leveraging a 7-gram language model on part of speech tags, serving as a partial solution to the problem of phrase-based empty structures."
            ]
        },
        {
            "question": "What does the described setup allow you to do with a 7-gram language model?",
            "reference-answers": [
                "The described setup allows you to use a 7-gram language model to analyze part of speech tags and impose some syntactic structure over the sentence, thereby providing a partial remedy to understanding the overall sentence structure."
            ]
        },
        {
            "question": "What approach was used to impose syntactic structure on the sentence?",
            "reference-answers": [
                "Part of speech tagging was used in conjunction with a 7-gram language model to impose syntactic structure on the sentence."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen36-slide39/text.txt": [
        {
            "question": "Why does the BLEU score show only a slight increase when tested on English-German or German-English, and how does this compare to human judgments?",
            "reference-answers": [
                "The BLEU score shows only a slight increase when tested on English-German or German-English because the overall fluency of the sentence is better, but the score is limited to 4 grams, making it less sensitive to improvements. In comparison, human judgments would likely show a better increase than the BLEU score, which is limited to 4 grams."
            ]
        },
        {
            "question": "What is the reason mentioned for the limited improvement in the BLEU score despite better sentence fluency in English-German or German-English translations?",
            "reference-answers": [
                "The BLEU score is limited to 4 grams, making it less sensitive to improvements in sentence fluency."
            ]
        },
        {
            "question": "What does the text suggest about the sensitivity of the BLEU score to improvements in sentence fluency?",
            "reference-answers": [
                "The text suggests that the BLEU score is not very sensitive to improvements in sentence fluency because it is limited to considering only up to 4 grams."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen37-slide40/text.txt": [
        {
            "question": "What is the importance of morphological agreement between determiners and nouns in the language model described in the text?",
            "reference-answers": [
                "The importance lies in ensuring grammatical accuracy by correctly handling the agreement between determiners and nouns based on their morphological forms, such as singular or plural, thereby allowing the model to avoid incorrect combinations like determiner in singular followed by a noun in plural."
            ]
        },
        {
            "question": "What improvement can be made to the language model to ensure grammatical correctness in noun and determiner agreement?",
            "reference-answers": [
                "The improvement involves considering the full morphological information, such as the singular or plural form, to ensure correct agreement between determiners and nouns. The language model should avoid incorrect sequences, like a singular determiner followed by a plural noun, by being trained on all possible determiners and nouns, eliminating the need to relearn agreement rules for each combination."
            ]
        },
        {
            "question": "What grammatically incorrect combination should the language model avoid based on the morphological information discussed?",
            "reference-answers": [
                "The language model should avoid the combination of a determiner in singular followed by a noun in plural."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen38-slide41/text.txt": [
        {
            "question": "What method is described for generating the final word form based on the translation of lemmas into Česka and part of speech technology?",
            "reference-answers": [
                "The method described involves translating lemmas into Česka and using part of speech technology to generate the final word form based on that translation."
            ]
        },
        {
            "question": "What example is used to illustrate the process of translating lemmas into Česka by breaking down part of speech technology into morphology and part of speech tech, and then generating the final word form?",
            "reference-answers": [
                "The example used is translating the lemma \"kneecap\" into \"Česka\" by breaking down part of speech technology into morphology and part of speech tech, and then generating the final word form."
            ]
        },
        {
            "question": "What example was used to illustrate the translation of lemmas into lemmas in the text?",
            "reference-answers": [
                "The example used to illustrate the translation of lemmas into lemmas in the text is translating \"kneecap\" in any form into Czech."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen39-slide42/text.txt": [
        {
            "question": "Question: Why did the BLEU score drop when moving from the part-of-speech model to the linguistically adequate morphological generation model?",
            "reference-answers": [
                "The BLEU score dropped when moving to the morphological generation model because unnecessary independence assumptions were introduced, leading to information loss. The model decomposed words into lemmas and tags, which isn't always beneficial, as many words can be directly copied without additional processing. This overcomplication reduced translation quality."
            ]
        },
        {
            "question": "The BLEU score dropped by 5 points when moving to the morphological generation model because unnecessary independence assumptions were introduced by decomposing words into lemmas and tags, causing information loss and reducing translation quality.",
            "reference-answers": [
                "The BLEU score dropped by 5 points when moving to the morphological generation model because unnecessary independence assumptions were introduced by decomposing words into lemmas and tags, causing information loss and reducing translation quality."
            ]
        },
        {
            "question": "Why did the BLEU score drop by 5 points when transitioning to the linguistically adequate morphological generation model?",
            "reference-answers": [
                "The BLEU score dropped by 5 points because the linguistically adequate morphological generation model introduced unnecessary independence assumptions. These assumptions caused an information loss by decomposing words into lemmas and tags where direct copying of correct word forms would have been more accurate. This degradation in translation quality led to the score decline."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen40-slide43/text.txt": [
        {
            "question": "What are the two main components used by the system in alternative decoding paths?",
            "reference-answers": [
                "The two main components used by the system in alternative decoding paths are the normal word-for-word translation and the lemma and part of speech translation."
            ]
        },
        {
            "question": "How does the system determine the balance between direct translation and lemma-based separation during training?",
            "reference-answers": [
                "The system determines the balance between direct translation and lemma-based separation during training by using minimum error rate training on a held-out dataset. This process identifies how often, on average, the system should trust the direct translation path and how often it should resort to the lemma-based separation, thereby establishing the optimal balance."
            ]
        },
        {
            "question": "How does the system use alternative decoding paths to handle cases where the direct translation path may not be sufficient?",
            "reference-answers": [
                "The system uses alternative decoding paths by combining two models to operate simultaneously. It performs normal word-for-word translation and independently translates lemmas and parts of speech, generating inflected forms from these. During training on a held-out dataset, the system learns how often to trust the direct translation path versus resorting to the lemma and part-of-speech separation. This flexibility allows it to back off to the alternative method when needed, while retaining the full power of direct translation from data."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen41-slide44/text.txt": [
        {
            "question": "What happens when both paths are taken?",
            "reference-answers": [
                "When both paths are taken, you achieve the desired improvement and surpass the baseline significantly."
            ]
        },
        {
            "question": "What is the outcome of taking both paths?",
            "reference-answers": [
                "The outcome of taking both paths is significant improvement over the baseline."
            ]
        },
        {
            "question": "What is the outcome of taking both paths mentioned?",
            "reference-answers": [
                "The outcome is improvement over the baseline by far."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen42-slide45/text.txt": [
        {
            "question": "The translator needs to extract additional information from the source text in the following situations:\n\n1. When translating from English into German or Czech, identifying the case for noun phrases.\n2. When translating from Chinese into English, guessing the number for nouns.\n3. When translating pronouns, determining what they refer to.",
            "reference-answers": [
                "The translator needs to extract additional information from the source text in the following situations:\n\n1. When translating from English into German or Czech, identifying the case for noun phrases.\n2. When translating from Chinese into English, guessing the number for nouns.\n3. When translating pronouns, determining what they refer to."
            ]
        },
        {
            "question": "What are the examples of language pairs mentioned in the text where additional grammatical information must be extracted from the source side during translation?",
            "reference-answers": [
                "- English into German  \n- English into Czech  \n- Chinese into English"
            ]
        },
        {
            "question": "How can additional grammatical information be extracted when it's missing in the source language?",
            "reference-answers": [
                "Additional grammatical information can be extracted from the source side alone when it is missing in the source language. For example, when translating from English to German or English to Czech, the case for noun phrases can be identified on the source side. Similarly, when translating from Chinese to English, the number for nouns can be guessed from the source text. Pronouns also require figuring out what they refer to, which can be done by analyzing the source side. This method allows for the extraction of necessary grammatical details directly from the source language."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen43-slide46/text.txt": [
        {
            "question": "What information is used to determine whether a noun should be translated as nominative or accusative in the target language?",
            "reference-answers": [
                "The information used is whether the noun serves as a subject or an object in the source sentence. This determines if it should be translated as nominative or accusative in the target language, based on the mapping in the translation table."
            ]
        },
        {
            "question": "What is analyzed in the source sentence to decide if a noun should be translated as nominative or accusative in the target language?",
            "reference-answers": [
                "The role of the noun in the source sentence, specifically whether it serves as a subject or an object, is analyzed to decide if it should be translated as nominative or accusative in the target language."
            ]
        },
        {
            "question": "What is the primary function of the translation table in the method described for translating English to Greek?",
            "reference-answers": [
                "The primary function of the translation table is to map the grammatical role or function of words in the source language (English) to the appropriate case in the target language (Greek). It uses the role of the word (e.g., subject or object) to determine whether it should be translated as nominative or accusative, based on the statistics of the data."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen44-slide47/text.txt": [
        {
            "question": "What is the purpose of determining the position of a word in the tree when parsing a sentence?",
            "reference-answers": [
                "The purpose is to determine whether a word sits in the subject or object position, which implies the case of the word."
            ]
        },
        {
            "question": "How does the position of a word in a tree help in determining its grammatical role, such as subject or object?",
            "reference-answers": [
                "The position of a word in the tree helps determine its grammatical role by indicating whether it is located in the subject or object position within the tree structure. This positional information implies the case of the word, as its placement in the tree reflects its grammatical function in the sentence."
            ]
        },
        {
            "question": "What does the teacher use the process of parsing a sentence and traversing the tree to determine?",
            "reference-answers": [
                "The teacher uses the process to determine the grammatical case of words based on their position in the sentence, specifically whether they are in the subject or object position."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen45-slide48/text.txt": [
        {
            "question": "What does the evaluation say about using explicit linguistic information on the source side?",
            "reference-answers": [
                "The evaluation highlights that using explicit linguistic information on the source side helps improve the prediction, which is a good thing."
            ]
        },
        {
            "question": "What does explicitly available linguistic information on the source side allow for?",
            "reference-answers": [
                "improve the prediction"
            ]
        },
        {
            "question": "What helps improve prediction according to the text?",
            "reference-answers": [
                "Using explicitly available linguistic information on the source side helps improve prediction."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen46-slide49/text.txt": [
        {
            "question": "What is the maximum number of language models applied in the described setups?",
            "reference-answers": [
                "The maximum number of language models applied in the described setups is three."
            ]
        },
        {
            "question": "How many language models can be applied in the most advanced setup described in the text to check the coherence of different linguistic levels?",
            "reference-answers": [
                "Three language models can be applied in the most advanced setup to check the coherence of different linguistic levels: word forms, lemmas, and morphological tags."
            ]
        },
        {
            "question": "The text describes three language models used in a translation setup for Czech. Each model checks the coherence of lemmas, morphological tags, and word forms, respectively.",
            "reference-answers": [
                "The text describes three language models used in a translation setup for Czech, each checking the coherence of lemmas, morphological tags, and word forms, respectively."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen47-slide50/text.txt": [
        {
            "question": "What is the trade-off when introducing explicit morphological tags in the translation process?",
            "reference-answers": [
                "The trade-off is that introducing explicit morphological tags increases the complexity of the search by expanding the number of translation options, as words can have multiple forms and meanings. However, this explicit information allows the system to better utilize the language model's detailed statistics, enabling it to select better combinations. If the balance between complexity and benefit isn't achieved, the increased complexity might negatively impact performance."
            ]
        },
        {
            "question": "Explain the trade-off when using morphological tags during translation and discuss why a balanced approach is necessary.",
            "reference-answers": [
                "Using morphological tags during translation introduces a trade-off between increased complexity and enhanced accuracy. While these tags provide explicit information about word forms, such as grammatical roles, they expand the search space by considering multiple possible meanings for each word. This can lead to more accurate translations by allowing the system to select the best combinations based on context. However, this added complexity can strain the system's efficiency, especially if the language model doesn't effectively utilize the morphological data. Therefore, a balanced approach is crucial to ensure that the benefits of using morphological tags outweigh the increased computational demands, maintaining both efficiency and translation quality."
            ]
        },
        {
            "question": "What is the trade-off when introducing explicit morphological tags into the translation process, according to the text?",
            "reference-answers": [
                "The trade-off is that while introducing explicit morphological tags increases the complexity of the search by requiring the system to consider multiple possible interpretations of word forms, it also provides the benefit of allowing the system to utilize more detailed statistics about the morphological tags, which can lead to better selection of translation options."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen48-slide51/text.txt": [
        {
            "question": "Why did the \"translate and check\" setup underperform in 2009 compared to previous years?",
            "reference-answers": [
                "In 2009, the \"translate and check\" setup underperformed compared to previous years because its added complexity in the search space wasn't offset by sufficient gains from the language model. The language model on word forms was reliable enough in 2009 due to larger data and faster machines, but in earlier years, the unreliably estimated language model on smaller data made it worth the effort. Additionally, the complexity of the setup with larger data led to an explosion of possible word and morphological tag combinations, making it difficult to handle the 2.2 million sentence pairs effectively at runtime, thus underperforming compared to the simpler phrase-based system."
            ]
        },
        {
            "question": "What was the main reason that the \"translate and check\" setup underperformed compared to the vanilla phrase-based system in 2009?",
            "reference-answers": [
                "The \"translate and check\" setup underperformed in 2009 because its added search space complexity wasn't compensated by sufficient gains from the language model, especially with larger data, leading to worse performance than the simpler phrase-based system."
            ]
        },
        {
            "question": "What was the reason that the \"translate and check\" setup was less effective in 2009 compared to previous years?",
            "reference-answers": [
                "In 2009, the \"translate and check\" setup was less effective because its added complexity overwhelmed the benefits provided by the language model. With larger data and faster computers, the system struggled with the increased search space and the unreliability of the language model on word forms due to smaller data in previous years. Additionally, the morphological decomposition approach became computationally infeasible with large datasets, leading to worse performance than the simpler phrase-based system."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen49-slide52/text.txt": [
        {
            "question": "What is the main issue with synchronous factored models in machine translation and how was it addressed in an alternative approach?",
            "reference-answers": [
                "The main issue with synchronous factored models in machine translation is that they require all translation options to be fully specified before the main search, which can lead to early pruning of less likely options and the removal of viable combinations. This is addressed in an alternative approach by using delayed factors, where some factors are left unspecified during the initial translation phase and only filled in after the main search, allowing the language model to consider a broader range of possibilities without early constraints."
            ]
        },
        {
            "question": "What is the key reason synchronous factored models failed and what alternative approaches were explored to address the issue?",
            "reference-answers": [
                "The key reason synchronous factored models failed is that they prune translation options too early, before the full linear context is available, thus removing combinations that the language model could have selected. Alternative approaches explored include asynchronous factors, which were too difficult to structure for search, and delayed factors, where some morphological information is left unspecified during the initial search and filled in later using neighboring words."
            ]
        },
        {
            "question": "What was the primary reason for the failure of the factored models, and how did the synchronous processing impact the translation options?",
            "reference-answers": [
                "The primary reason for the failure of the factored models was that synchronous processing required all translation options to be fully specified, including all morphological tags, before the main search could begin. This meant that any under-specified translation options were not allowed, and if the memory limits were too strict, less likely individual forms (like noun or adjective variations) were pruned. This pruning removed combinations that the language model would later need to evaluate, leading to the failure. The synchronous approach made it impossible to consider the necessary context from neighboring words until after the main search, which was too late to correct the under-specified options."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen50-slide53/text.txt": [
        {
            "question": "What components did the speaker use in their best phrase-based MT system, and what were the sizes of the n-grams for each?",
            "reference-answers": [
                "The speaker used four language models in their best phrase-based MT system:\n\n1. **Long**: Based on word forms with 7-grams, trained on 6-700 million tokens.\n2. **Big**: Based on word forms with 4-grams, trained on almost 4 gigawords of Czech texts.\n3. **Morphological tags**: Trained with 10-grams and 15-grams (with the 15-grams being sparser and edited later).\n\nEach model provided a different perspective on the output sentence, and their combination yielded the best results."
            ]
        },
        {
            "question": "What setup did the author use to improve translation quality in phrase-based MT, and how did combining multiple language models with different n-grams affect the BLEU score?",
            "reference-answers": [
                "The author improved translation quality in phrase-based MT by using four language models simultaneously, each specialized for different parts of the sentence. These models included a seven-gram word form model, a four-gram word form model, and morphological models with up to 15 grams. Combining these models, particularly using the 15-gram morphological model, significantly enhanced the BLEU score, as each model provided complementary information and improved the overall translation quality."
            ]
        },
        {
            "question": "What language models did the teacher use in their phrase-based MT system, and what were the specific configurations of each model?",
            "reference-answers": [
                "The teacher used four language models in their phrase-based MT system:\n\n1. **Long Model**: Based on seven-grams of word forms, trained on 6-7 hundred million tokens.\n2. **Big Model**: Based on four-grams, trained on almost four gigawords of data.\n3. **Morphological Tags (10-grams)**: Trained on 10-grams of morphological tags.\n4. **Morphological Tags (15-grams)**: Trained on 15-grams of morphological tags, used for additional improvement."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen51-slide54/text.txt": [
        {
            "question": "What are the two techniques mentioned in the text that aim to handle sparseness without causing an explosion?",
            "reference-answers": [
                "The two techniques mentioned are two-step translation and reverse self-training."
            ]
        },
        {
            "question": "Which two techniques are mentioned to handle sparseness while avoiding explosion?",
            "reference-answers": [
                "Two-step translation and reverse self-training."
            ]
        },
        {
            "question": "What are the two techniques mentioned to handle sparseness without explosion in translation?",
            "reference-answers": [
                "The two techniques mentioned are two-step translation and reverse self-training."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen52-slide55/text.txt": [
        {
            "question": "The translation system is composed of two main components. The first component is a phrase-based system that translates English into lemmatized Czech, handling structural changes like word reordering and count without dealing with case. The second component converts this simplified lemmatized Czech into fully inflected Czech using a larger dataset, focusing on word-level translations without reordering, thus preserving morphological details.",
            "reference-answers": [
                "The translation system is composed of two main components. The first component is a phrase-based system that translates English into lemmatized Czech, handling structural changes like word reordering and count without dealing with case. The second component converts this simplified lemmatized Czech into fully inflected Czech using a larger dataset, focusing on word-level translations without reordering, thus preserving morphological details."
            ]
        },
        {
            "question": "What are the two main steps in the translation process described in the text, and what is the purpose of each step?",
            "reference-answers": [
                "The two main steps in the translation process are:\n\n1. **Phrase-based translation from English to lemmatized Czech**: This step handles the reordering of words and changes in word count for various phrases. It simplifies the translation process by focusing on structural changes without dealing with detailed morphological decisions like case.\n\n2. **Conversion from lemmatized Czech to fully inflected Czech**: This step converts the simplified Czech into the complex, inflected form of the language. It uses a large corpus of data to train on and focuses on word-for-word translation, handling the rich morphology of Czech, including cases and affixes, to produce the final, accurate translation."
            ]
        },
        {
            "question": "What is the purpose of the two-step translation process described in the text?",
            "reference-answers": [
                "The purpose of the two-step translation process is to manage the complexity of translating into Czech, which has a rich morphology. The first step translates from English to lemmatized Czech, handling general translation and word reorderings without dealing with case decisions. The second step converts this simplified Czech into fully inflected Czech, trained on a large corpus, focusing on word-for-word translation and extracting necessary grammatical details to handle the morphological richness of Czech efficiently."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen53-slide56/text.txt": [
        {
            "question": "What improvements were observed when applying the two-step setup compared to the vanilla method in terms of BLEUCore and Semantic POS metrics?",
            "reference-answers": [
                "When the two-step setup was applied, improvements were observed in both BLEUCore and Semantic POS metrics compared to the vanilla method."
            ]
        },
        {
            "question": "What improvements were observed when applying the two-step setup on a small parallel and small monolingual corpus?",
            "reference-answers": [
                "When applying the two-step setup on a small parallel and small monolingual corpus, improvements were observed in BLEUCore and Semantic POS metrics. Both metrics showed an improvement from the vanilla setup to the two-step setup."
            ]
        },
        {
            "question": "What improvement does the described setup lead to when applied on a small parallel and small monolingual corpus?",
            "reference-answers": [
                "The described setup leads to an improvement in both BLEUCore and Semantic POS metrics when applied on a small parallel and small monolingual corpus."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen54-slide57/text.txt": [
        {
            "question": "What does the text suggest about the impact of using a small parallel corpus and a large monolingual corpus on the BLEU score?",
            "reference-answers": [
                "The text suggests that using a small parallel corpus and a large monolingual corpus results in a decrease in the BLEU score, indicating that this setup may not be optimal. It implies that the vanilla setup, without modifications, performs better, and while semantic word counting might still improve, the BLEU score is less sensitive to certain linguistic aspects, such as dropped words."
            ]
        },
        {
            "question": "Question:  \nWhy was the vanilla setup preferred over the other approaches when using a small parallel corpus and a large monolingual corpus?",
            "reference-answers": [
                "The vanilla setup was preferred because, despite using a small parallel corpus and a large monolingual corpus, it resulted in a higher BLEU score compared to other approaches. The BLEU score decreased when other methods were applied, indicating that the vanilla setup was more effective in this context. Additionally, the vanilla setup was found to be less sensitive to dropped words, which might contribute to its better performance."
            ]
        },
        {
            "question": "Question: Why does the BLEU score decrease in this scenario?",
            "reference-answers": [
                "The BLEU score decreases because the combination of a small parallel corpus and a large monolingual corpus does not provide enough benefit, making the vanilla setup preferable. BLEU is sensitive to n-grams like four grams in Czech but is less sensitive to dropped words."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen55-slide58/text.txt": [
        {
            "question": "What metaphor is used in the text to describe the consequences of making these assumptions, and what does it imply about the situation?",
            "reference-answers": [
                "The metaphor used is \"shooting yourself in the foot,\" which implies that making these assumptions is counterproductive and self-harming, highlighting the negative consequences of such actions."
            ]
        },
        {
            "question": "What does the speaker suggest happens when someone makes assumptions?",
            "reference-answers": [
                "The speaker suggests that making assumptions leads to shooting oneself in the foot, meaning causing self-harm or negative consequences."
            ]
        },
        {
            "question": "What does the speaker warn against in the given text?",
            "reference-answers": [
                "The speaker warns against making assumptions, as they can lead to self-harm or negative consequences."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen56-slide59/text.txt": [
        {
            "question": "What conclusion was reached regarding the two-step setup during the manual evaluation?",
            "reference-answers": [
                "The conclusion was that each annotator slightly preferred the two-step setup, but they didn't agree on which sentences it was better for, making it unclear whether the two-step setup is worthwhile."
            ]
        },
        {
            "question": "What conclusion did the annotators reach about the two-step setup?",
            "reference-answers": [
                "The annotators each slightly preferred the two-step setup, but they did not agree on which sentences it was better for, making the conclusion unclear about whether the two-step setup is worthwhile."
            ]
        },
        {
            "question": "What conclusion was reached regarding the two-step setup?",
            "reference-answers": [
                "The conclusion was that both annotators slightly preferred the two-step setup, but they did not agree on which sentences it was better for, making it unclear whether the two-step setup is worthwhile."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen57-slide60/text.txt": [
        {
            "question": "What is the purpose of reverse self-training in the described machine translation system, and how does it assist in handling word forms like \"cat\" in Czech?",
            "reference-answers": [
                "The purpose of reverse self-training in the described machine translation system is to enhance the translation model's ability to propose the correct word forms, particularly when dealing with different grammatical roles or word forms in the target language, such as \"cat\" in Czech (e.g., kočka and kočku). It achieves this by utilizing monolingual data to improve the translation model, ensuring that it can select the appropriate word forms even when the parallel data does not provide sufficient examples. This method is similar to back translation but tailored for phrase-based machine translation, allowing the system to better handle and propose the necessary word forms for accurate translations."
            ]
        },
        {
            "question": "What is the purpose of reverse self-training in the context of machine translation discussed in the text?",
            "reference-answers": [
                "The purpose of reverse self-training is to enhance the translation model's ability to propose the correct word forms, particularly when the parallel data is insufficient or lacks certain forms. It uses monolingual data to improve the translation model, ensuring it can generate accurate translations by filling gaps in the parallel data. This technique is similar to back translation but tailored for phrase-based machine translation to address word form variations, such as different Czech word forms for \"cat\" (kočka and kočku)."
            ]
        },
        {
            "question": "What is the main issue with the translation model when using parallel data, and how does reverse self-training help address this problem?",
            "reference-answers": [
                "The main issue with the translation model when using parallel data is that it may not propose all the necessary word forms needed for correct translation, especially when dealing with different grammatical roles or contexts. Reverse self-training addresses this by using monolingual data to expand the translation model's knowledge of word forms, allowing it to propose more diverse and appropriate forms that it couldn't produce before."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen58-slide61/text.txt": [
        {
            "question": "What is the issue with directly translating the phrase \"I read about kočce\" when creating source English data?",
            "reference-answers": [
                "The issue with directly translating the phrase \"I read about kočce\" is that kočce would be an unknown word based on your small parallel data, making the translation unhelpful and not useful for learning."
            ]
        },
        {
            "question": "How can we create the source English for the phrase and incorporate it into our parallel data collection, considering the issue with direct translation leading to unknown terms like 'kočce'?",
            "reference-answers": [
                "To create the source English for the phrase and incorporate it into the parallel data collection, we can use the source English we already have. By adding this source English directly to our parallel data collection, we can avoid the issue of direct translation leading to unknown terms like 'kočce'. This allows us to synthesize the source side effectively and include the monolingual data in our parallel dataset."
            ]
        },
        {
            "question": "What is a potential issue when translating the word \"kočce\" directly into English using your small parallel data?",
            "reference-answers": [
                "The potential issue is that translating \"kočce\" directly would result in an unknown word in English, rendering the translation ineffective for learning purposes."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen59-slide63/text.txt": [
        {
            "question": "What Czech word cannot be translated by the reverse system because of its case?",
            "reference-answers": [
                "The Czech word that cannot be translated by the reverse system because of its case is \"kočce.\""
            ]
        },
        {
            "question": "How does the system handle translation when it cannot translate a specific form like \"kočce\" but knows the lemma \"kočka\"?",
            "reference-answers": [
                "The system uses the lemma \"kočka\" when it cannot translate a specific form like \"kočce\". Since it knows the lemma \"kočka\" translates to \"cat\" in English, it relies on this lemma for the translation."
            ]
        },
        {
            "question": "What is a limitation of the reverse translation system when handling Czech words, and why does this happen?",
            "reference-answers": [
                "A limitation of the reverse translation system is that it cannot translate Czech words with different inflected forms, such as \"kočce,\" because it relies on lemmas and does not account for case distinctions in English, which lacks such grammatical cases."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen60-slide64/text.txt": [
        {
            "question": "What sentence about a cat is mentioned and included in the parallel data?",
            "reference-answers": [
                "Chetlsem okočce"
            ]
        },
        {
            "question": "What sentence is mentioned as being about a cat?",
            "reference-answers": [
                "Chetlsem okočce"
            ]
        },
        {
            "question": "What sentence is mentioned as being about a cat?",
            "reference-answers": [
                "The sentence mentioned as being about a cat is \"Chetlsem okočce.\""
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen61-slide65/text.txt": [
        {
            "question": "What is the benefit of learning a new form of a noun word, such as \"okotse,\" and how does it relate to extending the translation model?",
            "reference-answers": [
                "The benefit of learning a new form of a noun word, such as \"okotse,\" is that you are extending the translation model to be able to offer the forms that the language model will appreciate."
            ]
        },
        {
            "question": "What does the user gain by learning a new noun form about a cat called okotse?",
            "reference-answers": [
                "The user gains the ability to extend the translation model by learning a new noun form about a cat called okotse, enhancing the model's capability to offer such forms."
            ]
        },
        {
            "question": "What is the main benefit of learning the new noun form \"okotse\" according to the text?",
            "reference-answers": [
                "The main benefit is expanding the translation model's ability to offer noun forms."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen62-slide66/text.txt": [
        {
            "question": "What alternative approach was suggested for translating Czech words into English, and how does it differ from direct word-to-word translation?",
            "reference-answers": [
                "The alternative approach suggested for translating Czech words into English involves using the Czech lemma (e.g., \"kočka\") instead of translating directly from the inflected form (e.g., \"kočce\"). This method differs from direct word-to-word translation by creating synthetic parallel data, where the English word form is guessed based on the lemma. This approach leverages linguistic expertise to adapt translations, especially considering English's simpler morphology, rather than translating directly from form to form."
            ]
        },
        {
            "question": "What method is suggested for handling languages with complex morphologies in machine translation, and how does it involve creating synthetic parallel data?",
            "reference-answers": [
                "The method suggested involves using alternative decoding paths and translating Czech lemmas (like \"kočka\") into English forms to create synthetic parallel data. This approach requires guessing the English word form, leveraging the target language's morphological simplicity, and adapting linguistic expertise for different languages to build appropriate synthetic datasets. For example, German compounds are split and Turkish morphology is simplified to suit the target language's structure."
            ]
        },
        {
            "question": "What approach was described for improving translation from Czech to English, involving the use of lemmas and synthetic parallel data?",
            "reference-answers": [
                "The approach described for improving translation from Czech to English involved using Czech lemmas instead of direct word-to-word translation. By leveraging the lemma \"kočka,\" the method created synthetic parallel data by guessing the form of the corresponding English word. This process utilized alternative decoding paths in reverse translation, taking advantage of English's simpler morphology to generate the necessary data for training. This linguistic strategy aimed to enhance the translation process by aligning the synthetic data more effectively with the target language's structure."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen63-slide67/text.txt": [
        {
            "question": "What happens to the BLEU scores when the size of monolingual data increases?",
            "reference-answers": [
                "When the size of monolingual data increases, the BLEU scores also increase because the language model can estimate n-grams more reliably."
            ]
        },
        {
            "question": "What is the implication of having a large language model when the translation model is limited by small fixed parallel data?",
            "reference-answers": [
                "The implication is that even though a large language model can help, the translation model's small fixed parallel data limits its ability to provide the word forms that the language model would appreciate, thus restricting the translation quality."
            ]
        },
        {
            "question": "What is the impact of the size of monolingual data on the performance of a translation model according to the text?",
            "reference-answers": [
                "The size of monolingual data impacts the translation model's performance by improving it as the data grows, leading to higher BLEU scores due to more reliable estimation of n-grams."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen64-slide68/text.txt": [
        {
            "question": "What is the main benefit of including all the word forms that appear in the monolingual data in your translation model along with the reverse translation trick?",
            "reference-answers": [
                "You get double the increase."
            ]
        },
        {
            "question": "What is the result of including all word forms that appear in the monolingual data in your translation model using the reverse translation trick?",
            "reference-answers": [
                "The result is a double increase in the benefits of the translation model."
            ]
        },
        {
            "question": "What is the clear benefit of using reverse translation with monolingual data and including all word forms in the translation model?",
            "reference-answers": [
                "The clear benefit is that you get double the increase in the translation model's effectiveness by using reverse translation with monolingual data and including all word forms."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen65-slide69/text.txt": [
        {
            "question": "What is the approximate number of parallel sentences beyond which there is no benefit of using reverse translation?",
            "reference-answers": [
                "The approximate number of parallel sentences beyond which there is no benefit of using reverse translation is two million."
            ]
        },
        {
            "question": "What is the approximate number of parallel sentences after which the reverse translation trick is no longer beneficial?",
            "reference-answers": [
                "The reverse translation trick is no longer beneficial after approximately two million parallel sentences."
            ]
        },
        {
            "question": "What is the maximum number of parallel sentences that still provide benefit from the reverse translation trick?",
            "reference-answers": [
                "The maximum number of parallel sentences that still provide benefit from the reverse translation trick is one million."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen66-slide70/text.txt": [
        {
            "question": "What is the main role of morphology in neural machine translation systems?",
            "reference-answers": [
                "The main role of morphology in neural machine translation systems is to handle the structural aspects of words, such as roots, stems, and suffixes, to ensure accurate translation by managing variations like different tenses or pluralization."
            ]
        },
        {
            "question": "What is the focus of the discussion regarding neural machine translation?",
            "reference-answers": [
                "The focus of the discussion regarding neural machine translation is on how neural networks are used to model the translation process, integrating linguistic knowledge and processing large amounts of data to improve translation accuracy and fluency."
            ]
        },
        {
            "question": "What aspect of neural machine translation is examined in the text?",
            "reference-answers": [
                "The aspect of neural machine translation examined in the text is **morphology**."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen67-slide71/text.txt": [
        {
            "question": "Why is word segmentation necessary in neural machine translation and what methods are used for this purpose?",
            "reference-answers": [
                "Word segmentation is necessary in neural machine translation due to the limitations of embedding tables, which can only handle up to 80,000 word forms. This constraint arises when dealing with languages that have a large number of word forms, such as compound words or languages with extensive morphological variations. To address this, words are segmented into smaller units, such as syllables or morphemes. Additionally, bipair encoding is used, which creates a dictionary based on compression algorithms, prioritizing the most frequent words while handling less frequent ones."
            ]
        },
        {
            "question": "What limitation do neural machine translation models face in handling productive morphology, and what segmentation methods are suggested to address this?",
            "reference-answers": [
                "Neural machine translation models face the limitation of handling productive morphology due to the size constraint of their embedding tables, which can store at most 30 to 80 thousand word forms. To address this, the suggested segmentation methods include breaking words into syllables and morphemes, as well as using bipair encoding, a compression-based approach that creates a dictionary prioritizing frequent word forms."
            ]
        },
        {
            "question": "Question: What limitations do neural machine translation systems face with their embedding tables regarding word forms, and how is this issue addressed?",
            "reference-answers": [
                "Neural machine translation systems face a limitation with their embedding tables, which can store at most 30 to 80 thousand different word forms, making it difficult to handle languages with a large number of word forms due to productive morphology. To address this, words are segmented into smaller units such as syllables or morphemes. Additionally, bipair encoding, a compression-based method, is used to create a dictionary prioritizing the most frequent word forms."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen68-slide72/text.txt": [
        {
            "question": "**Question:**  \nWhat is the primary goal of the method described in the TEXT for constructing a fixed-size dictionary?",
            "reference-answers": [
                "The primary goal is to create a fixed-size dictionary that efficiently represents words by combining frequent parts, allowing longer or unknown words to be split into smaller, more frequent units. This method repeatedly identifies the most frequent character pairs and replaces them with new units to achieve this."
            ]
        },
        {
            "question": "What is the common name of the algorithm that constructs a fixed-size dictionary by repeatedly identifying and replacing the most frequent character pairs with new units?",
            "reference-answers": [
                "The common name of the algorithm is **Huffman coding**."
            ]
        },
        {
            "question": "What is the primary method used to construct the dictionary described in the text?",
            "reference-answers": [
                "The primary method used to construct the dictionary is based on the frequency of character pairs. The process involves repeatedly identifying the most frequent character pairs and replacing them with new units."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen69-slide73/text.txt": [
        {
            "question": "What does the text say about the frequent character pairs?",
            "reference-answers": [
                "The text says that the frequent character pairs are identified by how often the words were seen in the corpus. In this case, \"low\" was seen twice."
            ]
        },
        {
            "question": "How many times was the character pair 'l' and 'o' seen together in the corpus?",
            "reference-answers": [
                "The character pair 'l' and 'o' was seen together twice in the corpus."
            ]
        },
        {
            "question": "Which character pair is identified as frequent in the described vocabulary?",
            "reference-answers": [
                "The frequent character pairs are \"lo\" and \"ow.\""
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen70-slide74/text.txt": [
        {
            "question": "What did the teacher decide to do with the most frequent character pair in the example?",
            "reference-answers": [
                "The teacher decided to merge the two characters of the most frequent pair into a new unit."
            ]
        },
        {
            "question": "What did the teacher introduce to the vocabulary after noticing the frequent 'we' pair?",
            "reference-answers": [
                "The teacher introduced a new unit with the shape 'we' to the vocabulary."
            ]
        },
        {
            "question": "What is the most frequent character pair in the given example, and what new unit was introduced as a result?",
            "reference-answers": [
                "The most frequent character pair is \"we,\" and the new unit introduced is the merged shape of \"we.\""
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen71-slide75/text.txt": [
        {
            "question": "How many characters are in the word after you replace the most frequent pair of characters with a single new letter?",
            "reference-answers": [
                "3"
            ]
        },
        {
            "question": "What is the result of replacing the most frequent pair of characters in a word with a single new letter, as demonstrated in the example?",
            "reference-answers": [
                "The word becomes shorter by one character each time a pair is replaced by a single new letter."
            ]
        },
        {
            "question": "What word is transformed into \"LOVR\" by replacing a pair of its characters with a single new letter?",
            "reference-answers": [
                "LOVR is the transformed version of the word \"LOVER\" after replacing the pair \"ER\" with \"R\"."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen72-slide76/text.txt": [
        {
            "question": "What is the purpose of the new merge operation introduced in the setup?",
            "reference-answers": [
                "The purpose of the new merge operation is to treat V, E, R as a single unit to aid in the compression of the text being trained on."
            ]
        },
        {
            "question": "What is the reason for treating V, E, R as a single unit in the compression of text?",
            "reference-answers": [
                "The reason for treating V, E, R as a single unit in the compression of text is that it is useful for the compression of the text that you are training on to treat this V, E, R as a single unit because it is the most frequent pair in the text being trained on."
            ]
        },
        {
            "question": "What is the reason for treating V, E, R as a single unit in the text?",
            "reference-answers": [
                "It is useful for the compression of the text."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen73-slide77/text.txt": [
        {
            "question": "What is the next step in the process after the next merge is performed?",
            "reference-answers": [
                "The next step isn't specified in the text provided."
            ]
        },
        {
            "question": "What does the speaker predict about the next merge?",
            "reference-answers": [
                "The speaker predicts that the next merge will probably happen."
            ]
        },
        {
            "question": "What is the next merge mentioned?",
            "reference-answers": [
                "The next merge is mentioned, but no specific details are provided about it."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen74-slide78/text.txt": [
        {
            "question": "What does this ST go into?",
            "reference-answers": [
                "This ST goes into a single character ST."
            ]
        },
        {
            "question": "What happens when ST goes into this single character ST?",
            "reference-answers": [
                "ST goes into this single character ST."
            ]
        },
        {
            "question": "What action is taken by the original ST according to the text?",
            "reference-answers": [
                "The original ST is placed into this single character ST."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen75-slide79/text.txt": [
        {
            "question": "What makes neural machine translation conceptually simpler than phrase-based machine translation, according to the text, and how does byte pair encoding (BPE) handle subwords?",
            "reference-answers": [
                "Neural machine translation is conceptually simpler than phrase-based machine translation because it eliminates the need to manually handle morphological richness and complex phrase structures. Instead, neural models automatically learn the appropriate combinations of subword units through observation. Byte pair encoding (BPE) handles subwords by merging characters into larger units based on their frequency in the text, creating a vocabulary that includes both individual characters and merged subwords. This allows for efficient processing of subword units and enables the model to naturally handle morphological variations."
            ]
        },
        {
            "question": "What is the key idea behind Byte Pair Encoding (BPE) when applied to machine translation?",
            "reference-answers": [
                "The key idea behind Byte Pair Encoding (BPE) in machine translation is that it breaks down words into subwords by merging pairs of symbols (bytes or characters) in a sequence, allowing the model to learn the correct combinations of these subwords. This approach enables the neural machine translation system to handle the morphological richness of languages by automatically learning which parts of words (subwords) are frequent or meaningful, without requiring explicit morphological processing."
            ]
        },
        {
            "question": "What is the vocabulary size dependent on in the context of byte pair encoding?",
            "reference-answers": [
                "The vocabulary size is dependent on the size of your alphabet and the number of merge operations."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen76-slide80/text.txt": [
        {
            "question": "What is a key difference in how Sub-Bord Text Encoder (STE) and Byte-Pair Encoding (BPE) handle words with empty suffixes in the context of the provided examples?",
            "reference-answers": [
                "STE appends an underscore to the end of words, ensuring that words with empty suffixes are represented consistently as the same token (e.g., \"tramvaj_\"), which allows the model to generalize better. In contrast, BPE uses double apostrophes to indicate unfinished words and may split such cases into different tokens, preventing the model from recognizing the connection between related words."
            ]
        },
        {
            "question": "How did the handling of empty suffixes in Czech differ between SentencePiece and BPE, and what was the outcome in terms of tokenization?",
            "reference-answers": [
                "SentencePiece (STE) handles empty suffixes in Czech by appending an underscore at the end of words, which allows consistent tokenization. For example, the word \"tramvaj\" was recognized as the same token in both sentences processed with STE. In contrast, BPE by default does not handle empty suffixes well, often splitting such words into parts, leading to inconsistent tokenization. For instance, \"tramvaj\" was treated differently in each sentence when processed with BPE, whereas adding underscores before BPE processing could mitigate this issue."
            ]
        },
        {
            "question": "What are the key differences between how Subword Text Encoder (STE) and Byte-Pair Encoding (BPE) handle word tokenization, particularly in languages like Czech with complex morphology, and how does this affect the model's ability to generalize word meanings?",
            "reference-answers": [
                "The key differences between Subword Text Encoder (STE) and Byte-Pair Encoding (BPE) in handling word tokenization, particularly for languages like Czech with complex morphology, lie in how they process and tokenize words, especially with empty suffixes. STE automatically appends an underscore at the end of words, which allows it to separate endings, even if they are empty. This ensures that words like \"tramvaj\" are consistently treated as a single token across different contexts, enabling the model to generalize the word's meaning effectively. \n\nIn contrast, BPE, when used in its default variation, does not handle empty suffixes well. It uses double ampersands to indicate unfinished words, which can lead to inconsistent tokenization. For example, in the given Czech sentences, \"tramvaj\" might be split into different components, preventing the model from recognizing the relationship between the tokens. However, if BPE is preprocessed with underscores before tokenization, it can handle such cases correctly, though this is not the default behavior.\n\nThe difference in how these tokenization methods handle empty suffixes significantly impacts the model's ability to generalize word meanings. STE's consistent handling ensures that the model can recognize and generalize the behavior of words across different contexts, whereas BPE's default approach may fail to do so, unless manually adjusted."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen77-slide81/text.txt": [
        {
            "question": "What technique involving the addition of an underscore was found to significantly improve BPE performance in the German-to-Czech translation experiment?",
            "reference-answers": [
                "The technique that significantly improved BPE performance was adding an underscore after every word form, except for the final full stop."
            ]
        },
        {
            "question": "What method showed improved BLEU scores when an underscore was added after each token except the last one, and how did it compare to the sub-vortex encoder?",
            "reference-answers": [
                "The method that showed improved BLEU scores when an underscore was added after each token except the last one was BPE. With this modification, BPE achieved performance that was almost comparable to the sub-vortex encoder, which was the best-performing method in the experiments."
            ]
        },
        {
            "question": "What was the impact of adding an underscore after every word form, except for the final full stop, on the translation performance in the German-to-Czech experiments?",
            "reference-answers": [
                "Adding an underscore after every word form, except for the final full stop, significantly improved the translation performance in the German-to-Czech experiments. This method led to a much better increase in the BLEU score compared to simply adding an underscore to every word. It also allowed BPE to nearly match the performance of the sub-vortex encoder, indicating that this simple modification was highly effective."
            ]
        }
    ],
    "nmt-class/lecture06-morphology/screen78-slide82/text.txt": [
        {
            "question": "What approaches were used for linguistic segmentation in the Turkish, Finnish, English-German, and German-Czech language pairs, and which method was found to be most effective for each?",
            "reference-answers": [
                "For Turkish and Finnish, Morphosaurus was used and was found to be more effective than BPE. For English-German, Matthias Hook's rule-based compound splitting, suffix, and prefix splitting were employed. For German-Czech, the sub-tex encoder proved to be the most effective method after testing unsupervised segmentation and a derivation dictionary approach."
            ]
        },
        {
            "question": "Which encoder worked best in the German to Czech translation setup based on the described experiments?",
            "reference-answers": [
                "The sub-tex encoder by Catherine Cristo worked best."
            ]
        },
        {
            "question": "What method demonstrated superior performance in word segmentation for Turkish and Finnish over BPE?",
            "reference-answers": [
                "Morphosaurus"
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen01-slide01/text.txt": [
        {
            "question": "What is the focus of today's lecture on pre-neural approaches to machine translation?",
            "reference-answers": [
                "The focus of today's lecture is on discussing syntax in pre-neural machine translation, excluding phrase-based approaches."
            ]
        },
        {
            "question": "What is the focus of today's lecture on statistical machine translation?",
            "reference-answers": [
                "The focus of today's lecture on statistical machine translation is on pre-neural approaches, specifically discussing syntax in these methods rather than phrase-based ones."
            ]
        },
        {
            "question": "What is the focus of today's lecture on statistical machine translation?",
            "reference-answers": [
                "The focus of today's lecture on statistical machine translation is on pre-neural approaches, specifically excluding phrase-based ones, and it emphasizes the role of syntax in these approaches."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen02-slide02/text.txt": [
        {
            "question": "What are the two main types of syntactic trees discussed in the lecture, and how are they used in machine translation?",
            "reference-answers": [
                "The two main types of syntactic trees discussed are constituency trees and dependency trees. Constituency trees are used in context-free grammars and are made synchronous for translation, helping to break down sentences hierarchically. Dependency trees, including shallow and deep syntactic trees, are used to capture deeper syntactic structures, with the deep tectorochromatic layer from the Prague school aiding in handling complex sentence structures in translation, though neural MT hasn't fully utilized it yet."
            ]
        },
        {
            "question": "What is the tectorochromatic layer, and what is its current status in the field of machine translation?",
            "reference-answers": [
                "The tectorochromatic layer is a deep syntactic approach, particularly associated with the Prague school, that was developed over a decade ago and has contributed to machine translation for some time. However, it is currently dormant in practical applications, despite remaining active in theoretical research, as neural machine translation systems have yet to fully integrate and utilize this approach."
            ]
        },
        {
            "question": "What is the key difference between the tectorochromatic layer approach and neural machine translation in terms of their application in syntax-based machine translation?",
            "reference-answers": [
                "The tectorochromatic layer approach is a theoretical, deep syntactic method rooted in the Prague School, which has contributed to machine translation but is currently underutilized in practical applications, especially in neural machine translation (NMT). NMT, being data-driven and sequential, does not integrate deep syntax like the tectorochromatic layer, making the latter dormant in application despite its theoretical significance."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen03-slide03/text.txt": [
        {
            "question": "What were the main limitations of phrase-based machine translation systems that neural machine translation aimed to address?",
            "reference-answers": [
                "The main limitations of phrase-based machine translation systems were their lack of grammatical coherence, inability to handle gaps in certain language constructions like circumfixation (e.g., in French negation), and weak reordering modules that couldn't account for grammatical rules such as verb placement in subordinate clauses in German. These issues made the output often ungrammatical and difficult to understand, even if the individual phrases were correct."
            ]
        },
        {
            "question": "What was a major limitation of phrase-based machine translation systems in handling certain language constructions, such as the negation of French verbs like \"I don't know\"?",
            "reference-answers": [
                "A major limitation of phrase-based machine translation systems was their inability to handle language constructions that required gaps, such as the negation of French verbs. For example, translating \"I don't know\" into French requires the structure \"je ne sais pas,\" where \"ne\" and \"pas\" must surround the verb. Phrase-based systems, which did not allow for gaps in phrases, had to memorize all verb forms (positive and negative), making it impractical for languages with many verbs."
            ]
        },
        {
            "question": "What was the main issue with phrase-based machine translation systems regarding their inability to handle gaps in phrases, and how was this limitation exemplified using the French language?",
            "reference-answers": [
                "The main issue with phrase-based machine translation systems was their inability to handle gaps in phrases, such as language constructions requiring circumfixation. This limitation was exemplified in French, where negating a verb (e.g., \"je ne sais pas\") requires the system to place \"ne\" and \"pas\" around the verb. Phrase-based systems, unable to allow gaps in phrases, had to memorize each verb in both positive and negative forms, leading to inefficient and often grammatically incorrect translations."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen04-slide04/text.txt": [
        {
            "question": "What was the hierarchical model known for being the first to do, and what characteristic made it simple?",
            "reference-answers": [
                "The hierarchical model was known for being the first to tackle or solve some of the issues, and its simplicity was what made it easy to understand and use."
            ]
        },
        {
            "question": "Which model was the first to address the issues mentioned, and what was noted about its complexity?",
            "reference-answers": [
                "The hierarchical model was the first to address the issues mentioned, and it was noted as a very simple model."
            ]
        },
        {
            "question": "What was the hierarchical model noted for being the first to do?",
            "reference-answers": [
                "The hierarchical model was noted for being the first to tackle some of these issues."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen05-slide05/text.txt": [
        {
            "question": "What are the key limitations and capabilities of the model introduced by David Chang in 2005 for machine translation?",
            "reference-answers": [
                "**Answer:**\n\nThe model introduced by David Chang in 2005 for machine translation has the following key limitations and capabilities:\n\n**Limitations:**\n- The model only allows gaps in phrases and does not provide a hierarchical or grammatical validation. This means it cannot ensure that the translated sentence has a grammatically correct structure in the target language.\n- It can produce nonsensical translations, such as translating \"do not something\" into \"no something pa,\" which may not make sense in either language.\n\n**Capabilities:**\n- For input sentences with good grammatical structure, the model can preserve the grammatical structure in the target language.\n- It uses a phrase-based approach similar to other phrase-based models but introduces gaps in these phrases, which allows for some flexibility in translation."
            ]
        },
        {
            "question": "What does David Chang's 2005 translation model allow in terms of gaps?",
            "reference-answers": [
                "David Chang's 2005 translation model allows gaps in phrases and that's it."
            ]
        },
        {
            "question": "What is a limitation of David Chang's 2005 model in terms of handling grammatical structures?",
            "reference-answers": [
                "A limitation of David Chang's 2005 model is that it does not validate whether the sentence has a grammatical structure in the target language, as it only allows gaps in phrases and does not check if the translation results in a grammatically correct sentence."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen06-slide06/text.txt": [
        {
            "question": "What is a key difference between phrase-based and hierarchical machine translation models as described in the text?",
            "reference-answers": [
                "A key difference is that hierarchical models allow extracting smaller phrases from larger ones, whereas phrase-based models extract phrases without considering such hierarchical breakdown."
            ]
        },
        {
            "question": "What is the key difference between phrase-based machine translation and hierarchical machine translation when extracting phrases?",
            "reference-answers": [
                "The key difference is that in phrase-based machine translation, phrases are extracted directly from the data without considering hierarchical structure, while in hierarchical machine translation, smaller phrases can be extracted from larger phrases in a structured, tree-like manner."
            ]
        },
        {
            "question": "What is a key difference between the phrase-based and hierarchical models in terms of phrase extraction?",
            "reference-answers": [
                "A key difference is that phrase-based models extract all possible phrases regardless of hierarchy, while hierarchical models allow extracting smaller phrases from larger ones."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen07-slide07/text.txt": [
        {
            "question": "What does the rule allow in terms of gappy phrases and how does it handle recursive structures and co-indexation?",
            "reference-answers": [
                "The rule allows gappy phrases with up to two gaps and can include recursive structures by embedding them within the gappy phrases. Co-indexation is handled through the use of a single non-terminal 'x', which links the source and target phrases to indicate which parts correspond to each other."
            ]
        },
        {
            "question": "What is the maximum number of gaps allowed in the gappy phrase rule described?",
            "reference-answers": [
                "The maximum number of gaps allowed in the gappy phrase rule described is **two**."
            ]
        },
        {
            "question": "How does the hierarchical model described in the text use the rule involving gappy phrases and the non-terminal symbol 'x' to model sentence construction?",
            "reference-answers": [
                "The hierarchical model uses the rule involving gappy phrases with the non-terminal symbol 'x' to model sentence construction by allowing up to two gaps in these phrases. This enables reordering of elements during translation, where the source phrase's parts are co-indexed with the target phrase's parts via 'x'. The model handles recursive structures by embedding any phrase, including those with recursion, into the gappy phrases, thereby capturing the hierarchical and recursive nature of sentence construction."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen08-slide08/text.txt": [
        {
            "question": "What restrictions were imposed on the rules created during the extraction process, and why was it necessary to include at least one terminal?",
            "reference-answers": [
                "Restrictions on the rules included a maximum of six symbols, allowing some non-terminals but requiring at least one terminal. Including a terminal was necessary because it provided evidence for the system to know when to apply the rule, ensuring it only fires when there's a match in the input."
            ]
        },
        {
            "question": "What was the main reason for limiting the number of extracted phrases to no more than two from a longer phrase?",
            "reference-answers": [
                "The main reason for limiting the number of extracted phrases to no more than two from a longer phrase was to ensure that the files remained manageable in size and could fit on the disk, thus preventing the search space from becoming overly cluttered."
            ]
        },
        {
            "question": "What methods were used to limit the number of extracted rules in the phrase-based extraction system?",
            "reference-answers": [
                "The methods used to limit the number of extracted rules were:\n1. Rules were limited to a maximum of six symbols.\n2. At least one symbol had to be a terminal, ensuring some evidence in the input for the rule to fire.\n3. No more than two phrases could be extracted from a longer phrase."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen09-slide10/text.txt": [
        {
            "question": "What do constituency trees primarily represent in terms of sentence structure analysis?",
            "reference-answers": [
                "Constituency trees primarily represent how a sentence is divided into smaller parts, reflecting the structure through bracketing and labeled non-terminals, showing how words combine into phrases or clauses."
            ]
        },
        {
            "question": "What do non-terminals represent in constituency trees?",
            "reference-answers": [
                "Non-terminals in constituency trees represent the structural components or labels that group words into phrases, reflecting the division of the sentence into smaller parts."
            ]
        },
        {
            "question": "What is the primary purpose of constituency trees in describing sentence structure?",
            "reference-answers": [
                "The primary purpose of constituency trees is to divide the sentence into smaller parts by showing how words can be grouped together through bracketing, reflecting a divide and conquer approach to understanding sentence structure."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen10-slide11/text.txt": [
        {
            "question": "What is the most important syntactic element within a verb phrase (VP), according to the text?",
            "reference-answers": [
                "The most important syntactic element within a verb phrase (VP) is the verb."
            ]
        },
        {
            "question": "What is the most important syntactic element within a VP according to the text?",
            "reference-answers": [
                "The most important syntactic element within a VP according to the text is the verb."
            ]
        },
        {
            "question": "What is considered the \"hat\" within a verb phrase (VP) constituent, according to the text?",
            "reference-answers": [
                "The verb is considered the \"hat\" within a verb phrase (VP) constituent."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen11-slide12/text.txt": [
        {
            "question": "What is pulled up to the very root of the dependency tree?  \nOptions: head word, dependent word, terminal, non-terminal.",
            "reference-answers": [
                "head word"
            ]
        },
        {
            "question": "The primary characteristic of the head in a dependency tree is that it is a terminal, meaning it does not have non-terminal labels.",
            "reference-answers": [
                "The primary characteristic of the head in a dependency tree is that it is a terminal, meaning it does not have non-terminal labels."
            ]
        },
        {
            "question": "What is important to note about the head and internal nodes in a dependency tree?",
            "reference-answers": [
                "In a dependency tree, it is important to note that the head is pulled up to the root, and the internal nodes are only terminals, meaning there are no non-terminal labels on the head or internal nodes. The tree structure is based on the relationships between words, with the head word at the root and the other nodes hanging beneath it."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen12-slide13/text.txt": [
        {
            "question": "What is recommended for further understanding of constituency syntax in machine translation?",
            "reference-answers": [
                "The recommended resource for further understanding of constituency syntax in machine translation is Machine Translation Talk Number 10, which includes illustrations and animations of the discussed algorithm."
            ]
        },
        {
            "question": "What visual elements are included in the recommended machine translation talk number 10 that complement the lecture on constituency syntax in machine translation?",
            "reference-answers": [
                "The recommended machine translation talk number 10 includes illustrations and animations of the discussed algorithm."
            ]
        },
        {
            "question": "What does the teacher recommend as a complement to the lecture on constituency syntax in machine translation?",
            "reference-answers": [
                "The teacher recommends machine translation talk number 10 as a complement to the lecture on constituency syntax in machine translation."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen13-slide14/text.txt": [
        {
            "question": "What method does the text describe for assigning probabilities to the production rules in a context-free grammar, and how does it relate to using counts from a tree bank and maximum likelihood estimation?",
            "reference-answers": [
                "The text describes a method for assigning probabilities to production rules in a context-free grammar by counting the observed usage of each rule in a tree bank and then normalizing these counts to calculate probabilities. This process involves dividing the count of each production rule by the total number of observations, resulting in the maximum likelihood estimate for each probability. This approach relates to using counts from a tree bank by leveraging manually annotated data to determine the frequency of each rule's application, and it aligns with maximum likelihood estimation as it uses the observed data to estimate the most probable parameters for the grammar."
            ]
        },
        {
            "question": "What method is described for assigning probabilities to the production rules in a context-free grammar?",
            "reference-answers": [
                "The method described involves counting the observed occurrences of each production rule in a tree bank and then normalizing these counts by dividing by the total number of observations to obtain the maximum likelihood estimate for each probability."
            ]
        },
        {
            "question": "Explain the method for assigning probabilities to each production in a context-free grammar as described in the text.",
            "reference-answers": [
                "To assign probabilities to each production in a context-free grammar, follow these steps:\n\n1. **Use a Tree Bank**: Utilize a tree bank, which is a large collection of manually annotated syntactic trees. These trees represent the valid structures of sentences.\n\n2. **Count Production Occurrences**: For each production rule in the grammar, count how often it appears in the tree bank. This involves tallying the number of times each production is used by the linguists annotating the tree bank.\n\n3. **Normalize the Counts**: To convert these counts into probabilities, divide the count of each production by the total number of observations (i.e., the total number of times any production was used). This normalization ensures that the probabilities sum to 1 for each non-terminal, providing a valid probability distribution.\n\n4. **Compute Sentence Probabilities**: Once probabilities are assigned to each production, the probability of a full tree (and thus the sentence it represents) can be calculated as the product of the probabilities of the individual productions used to generate that tree.\n\nThis method provides a way to estimate the likelihood of each production based on empirical data, enabling the construction of a probabilistic context-free grammar that can evaluate the syntactic validity of sentences."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen14-slide15/text.txt": [
        {
            "question": "What is the purpose of the CKY algorithm in parsing a sentence, and how does it use back pointers to construct the syntactic tree?",
            "reference-answers": [
                "The purpose of the CKY algorithm is to determine if a sentence is valid according to a given grammar and, if so, to construct the syntactic tree that represents its structure. The algorithm uses a bottom-up approach to fill a table where each cell corresponds to a span of the sentence and contains all possible non-terminal analyses for that span. Back pointers are used to track how each span was derived, allowing the algorithm to reconstruct the full syntactic tree. Starting from the final non-terminal (e.g., S), the back pointers trace the derivation path back to the beginning of the sentence, combining spans to form the complete syntactic structure."
            ]
        },
        {
            "question": "How does the CKY or CYK algorithm construct the syntactic tree for a given sentence using the information provided in the text?",
            "reference-answers": [
                "The CKY (or CYK) algorithm constructs the syntactic tree for a given sentence by processing the sentence bottom-up. It fills a table where each cell corresponds to a span of the sentence and contains all possible non-terminal categories (e.g., NP, VP) that can represent that span. For example, the word \"she\" is identified as an NP, and the word \"eats\" can be either a V or a VP. The algorithm then combines these spans to build larger constituents, such as combining V and NP to form a VP for \"eats a fish.\" This VP is further combined with another VP (\"eats a fish with a fork\") and ultimately with the NP \"she\" to form the complete sentence. The algorithm preserves back pointers, which allow reconstructing the syntactic tree by tracing from the final non-terminal (S) down to the full sentence. This method ensures that the syntactic tree is built efficiently and correctly."
            ]
        },
        {
            "question": "What is the specific parse for \"she\" in the sentence \"She eats a fish with a fork\" according to the grammar described in the text?",
            "reference-answers": [
                "The specific parse for \"she\" in the sentence is a noun phrase (NP)."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen15-slide16/text.txt": [
        {
            "question": "How does the synchronous context-free grammar capture the translation process between Chinese and English, as illustrated in the example?",
            "reference-answers": [
                "The synchronous context-free grammar captures the translation process between Chinese and English by formally representing the double generation of source and target languages in parallel. Each rule in the grammar specifies the structure of the source language and the corresponding structure of the target language. In the example provided, the rule demonstrates that the English phrase \"something in somewhere\" is translated from the Chinese structure, where the order of the components is different. This is achieved through the coindexation of non-terminals in the right-hand side of the rule, which indicates that the non-terminals representing the source and target languages have swapped their positions. By doing so, the grammar ensures that the translation process aligns the structures of the two languages correctly, starting from the non-terminal and generating the trees for both languages in parallel, with the non-terminals matching exactly across the source and target trees. This approach effectively captures the reordering and structural differences between Chinese and English, enabling accurate machine translation."
            ]
        },
        {
            "question": "What is the role of the right-hand side of the rules in the context-free grammar used for machine translation as described in the text?",
            "reference-answers": [
                "The right-hand side of the rules in the context-free grammar used for machine translation serves to specify the relationship between the source language and the target language phrases. It captures the double generation by structuring the translation in parallel for both languages. The coindexation of non-terminals (e.g., X0 and X1) indicates how the positions of these elements are swapped between the source and target languages, reflecting the structural differences in word order and phrase arrangement between the two languages. This allows the grammar to formally define how source and target language trees are generated in parallel, ensuring that the translation process accurately reflects the syntactic and structural relationships between the languages involved."
            ]
        },
        {
            "question": "How do the rules in a synchronous context-free grammar handle the differences in word order between the source and target languages in machine translation?",
            "reference-answers": [
                "The rules in a synchronous context-free grammar handle the differences in word order by specifying that the non-terminals on the right-hand side of the rule can be reordered. For example, in the translation from Chinese to English, the rule shows that the source language's structure (e.g., something in Chinese) is translated into a target language's structure with the non-terminals swapped, reflecting the different word order between languages. This reordering is achieved through coindexation, ensuring that the non-terminals match exactly and the translation aligns correctly despite structural differences."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen16-slide17/text.txt": [
        {
            "question": "Why is it impossible to map the constituency trees of the English sentence \"John fell in love with Mary\" and the Czech sentence \"Jan Miluje Marii\" using a Synchronous context-free grammar?",
            "reference-answers": [
                "It is impossible to map the constituency trees of the English sentence \"John fell in love with Mary\" and the Czech sentence \"Jan Miluje Marii\" using a Synchronous context-free grammar because the English tree is deeper than the Czech tree. This difference in tree depth results in a mismatch in the number of productions or rule applications required to align the structures, making synchronization impossible."
            ]
        },
        {
            "question": "Why is it impossible to map the English tree John fell in love with Mary to the Czech tree Jan Miluje Marii using the Synchronous context-free grammar?",
            "reference-answers": [
                "It is impossible to map the English tree John fell in love with Mary to the Czech tree Jan Miluje Marii using the Synchronous context-free grammar because the English tree is deeper than the Czech tree. This difference in tree structure means that the number of productions or rule applications required to map them is different, making it incompatible with the requirements of the Synchronous context-free grammar."
            ]
        },
        {
            "question": "The trees for the English and Czech sentences have different depths, which leads to a mismatch in the number of rule applications required for mapping. This difference in structure prevents the use of Synchronous context-free grammar for translation.",
            "reference-answers": [
                "The trees for the English and Czech sentences have different depths, which leads to a mismatch in the number of rule applications required for mapping. This difference in structure prevents the use of Synchronous context-free grammar for translation."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen17-slide18/text.txt": [
        {
            "question": "What is the key feature of Synchronous Tree Substitution Grammars as described in the text?",
            "reference-answers": [
                "The key feature of Synchronous Tree Substitution Grammars, as described, is that they produce full subtrees rather than single-level productions, ensuring that entire subparts are substituted at once to maintain structural alignment between languages."
            ]
        },
        {
            "question": "How are the verb and object slots in the VP subtree filled in the example provided?",
            "reference-answers": [
                "The verb slot in the VP subtree is filled with the verb \"FEL,\" which maps to \"MILUJE,\" and the object slot is filled with \"MARI,\" which maps to \"MARY.\""
            ]
        },
        {
            "question": "How are the verb FEL and noun MARI used in the VP subtree, and how does this correspond to the structure in the Czech constituency tree?",
            "reference-answers": [
                "In the VP subtree, the verb FEL fills one slot corresponding to the verb, while the noun MARI fills the other slot corresponding to the object or second part. These elements map to the Czech verb phrase structure, where FEL corresponds to MILUJE (verb) and MARI corresponds to MARY (noun). This alignment shows that the VP subtree as a whole corresponds to the Czech verb phrase, not at the level of individual productions but as full subtrees."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen18-slide19/text.txt": [
        {
            "question": "What is the key reason synchronous tree substitution grammars are formally equivalent to synchronous context-free grammars, and how do they map to constituency trees?",
            "reference-answers": [
                "The key reason synchronous tree substitution grammars are formally equivalent to synchronous context-free grammars is that they can condense the inner structure of subtrees using a special non-terminal, such as VP, which includes PP, P, and N. This allows them to create a single production rule that directly corresponds to the target language's structure. They map to constituency trees by using these non-terminals to match the set of productions across languages, even if the tree structures differ, by resorting to large rules that digest entire source and target trees."
            ]
        },
        {
            "question": "What is the advantage of using a single special non-terminal in synchronous tree substitution grammars, and how does it relate to their formal equivalence with synchronous context-free grammars?",
            "reference-answers": [
                "The advantage of using a single special non-terminal in synchronous tree substitution grammars is that it simplifies the grammar by condensing the inner structure of subtrees into one production. This allows for a direct one-to-one mapping between the productions of different languages, ensuring formal equivalence with synchronous context-free grammars. By encapsulating complex structures like VP, PP, P, and N, as well as terminals such as \"love then,\" this special non-terminal enables the grammars to handle the same set of productions efficiently, even when the trees are structurally different."
            ]
        },
        {
            "question": "What is the relationship between synchronous tree substitution grammars and synchronous context-free grammars in terms of their expressive power, and how do they map constituency trees between different languages?",
            "reference-answers": [
                "Synchronous tree substitution grammars and synchronous context-free grammars are formally equivalent in terms of their expressive power. This means they have the same ability to describe the same set of languages or translations. They can map constituency trees between different languages by condensing the inner structure of subtrees into a single production using a special non-terminal, such as VP, which can include other non-terminals like PP, P, and N. This allows for a direct one-to-one correspondence between the sets of productions in the source and target languages. Even if the trees are structurally different, they can still be mapped, though this might require using a very large rule that handles the entire source tree to produce the target tree. However, the focus is not on such large-scale generalizations but rather on more structured and specific mappings."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen19-slide20/text.txt": [
        {
            "question": "What distinguishes the use of synchronous context-free grammars in machine translation from phrase-based MT?",
            "reference-answers": [
                "The use of synchronous context-free grammars in machine translation differs from phrase-based MT in that it integrates parsing and translation in a bottom-up, parallel fashion. While phrase-based MT processes the input sentence in any order and generates the output left to right, often creating multiple hypotheses, synchronous grammars validate the input sentence's correctness while simultaneously constructing the target sentence using the target side of the rules. This approach processes both the input and output in a strict bottom-up manner, distinguishing it from the left-to-right processing of phrase-based MT."
            ]
        },
        {
            "question": "**Question:**  \nWhat is the primary purpose of the language model in the context of synchronous substitution grammars as described in the text?",
            "reference-answers": [
                "The primary purpose of the language model in the context of synchronous substitution grammars is to facilitate the translation process by integrating with the parsing and rule application, ensuring the correct and contextual construction of the target language sentence as the source sentence is processed."
            ]
        },
        {
            "question": "What are the key differences between the processing style of synchronous context-free grammars and phrase-based machine translation in terms of input digestion and output production?",
            "reference-answers": [
                "The key differences are in how the input is digested and the output is produced. Synchronous context-free grammars process the input sentence bottom-up, simultaneously constructing the target sentence in a parallel bottom-up manner. In contrast, phrase-based MT digests the input from left to right and produces the output left to right, often with multiple hypotheses."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen20-slide21/text.txt": [
        {
            "question": "What is the process for scoring partial translations and how does the system determine the correct structure?",
            "reference-answers": [
                "The process for scoring partial translations involves digesting contiguous sequences in the source sentence and constructing corresponding contiguous spans in the target language. Initially, single-word translations like \"China\" or \"Beijing\" do not have language model scores because they are too short to be meaningful. The system uses a CKY parsing approach, where states are labeled with non-terminals indicating valid derivations of sentence spans. However, in a hierarchical model, these labels are generic, lacking specific details needed for accurate scoring. To address this, the system employs state splitting, expanding the search space with more detailed labels that track the exact words at the start and end of each partial translation. This allows the language model to score sequences reliably and determine the correct structure by understanding the connection between phrases."
            ]
        },
        {
            "question": "What is the reason for the necessity of detailed labels in the states for the language model to score partial hypotheses accurately in the described translation process?",
            "reference-answers": [
                "The necessity for detailed labels in the states is because the language model requires the exact words in the partial hypotheses to score them accurately. Without detailed information, the language model cannot determine the context or the sequence of words, making it impossible to apply the language model scores reliably. The detailed labels allow the model to know the specific words at the start and end of each box (hypothesis), enabling accurate scoring based on the target language's trigrams and ensuring that the language model can evaluate the partial translations correctly."
            ]
        },
        {
            "question": "What is the role of state splitting in enabling language model scoring during the translation process described in the text?",
            "reference-answers": [
                "State splitting plays a crucial role in enabling language model scoring by providing more detailed information in the search space. Without state splitting, the states (boxes) in the translation process would only contain labels like 'x,' which lack the necessary context for the language model to score the translations. By splitting states to include the exact words at the start and end of the boxes, the language model can effectively assess the partial hypotheses, ensuring reliable scoring based on the surrounding words. This allows the language model to consider the exact sequence of words when evaluating the translation, improving the accuracy and reliability of the scoring process."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen21-slide22/text.txt": [
        {
            "question": "**Question:** What is the reason that the example provided was considered linguistically inadequate, and how does the added syntactic information from true syntactic trees complicate the parsing process?",
            "reference-answers": [
                "The example was considered linguistically inadequate because it used a hierarchical model without proper syntax, making it insufficient for accurate linguistic analysis. The added syntactic information from true syntactic trees complicates the parsing process by introducing labels that linguists assign, as illustrated by David Cheng's work in 2010."
            ]
        },
        {
            "question": "Question: Explain why the initial example of machine translation using bottom-up parsing was linguistically inadequate and how the addition of syntactic information, as discussed in the slides by David Cheng, complicates the parsing process.",
            "reference-answers": [
                "The initial example of machine translation using bottom-up parsing was linguistically inadequate because it was not proper syntax; it was merely a hierarchical model without the necessary linguistic structure. The addition of syntactic information, as discussed in the slides by David Cheng, complicates the parsing process by introducing more complexity, making it more challenging to handle the added linguistic details."
            ]
        },
        {
            "question": "The example used for bottom-up parsing in machine translation was linguistically inadequate because it lacked proper syntax, relying only on a hierarchical structure. The addition of syntactic information, such as labels assigned by linguists, complicates the parsing process, as illustrated in David Cheng's 2010 slides.",
            "reference-answers": [
                "The example used for bottom-up parsing in machine translation was linguistically inadequate because it lacked proper syntax, relying only on a hierarchical structure. The addition of syntactic information, such as labels assigned by linguists, complicates the parsing process, as illustrated in David Cheng's 2010 slides."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen22-slide23/text.txt": [
        {
            "question": "What are the two main reasons for the more constrained extraction of phrases in the hierarchical model, and how does it compare to the phrase-based and syntactic models?",
            "reference-answers": [
                "The two main reasons for the more constrained extraction of phrases in the hierarchical model are that phrases must be consistent with both the word alignment and the tree structures of the source and target sentences, leading to fewer extracted phrases. Additionally, the hierarchical model's performance is sometimes better than the phrase-based model, but it is more often worse compared to the syntactic model."
            ]
        },
        {
            "question": "What are the two main reasons for the constrained extraction of phrases in the substitution grammars discussed in the text?",
            "reference-answers": [
                "1. The use of both source and target sentence trees ensures that the extracted phrases are consistent with the syntactic structure of both languages.  \n2. The experimental results showed that syntactic models often performed worse compared to other approaches like hierarchical or phrase-based models."
            ]
        },
        {
            "question": "**Question:**  \nWhy are fewer phrases extracted using the new method compared to the previous one?",
            "reference-answers": [
                "The new method extracts fewer phrases because it imposes additional constraints by requiring consistency with both the source and target language's syntactic structures, in addition to word alignments, making the extraction process more stringent."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen23-slide24/text.txt": [
        {
            "question": "What are the reasons why that is difficult?",
            "reference-answers": [
                "1. It requires a deep understanding of complex concepts.  \n2. It may involve multiple steps or processes.  \n3. It can be time-consuming due to the need for careful consideration.  \n4. It might lack clear guidelines or instructions.  \n5. It could involve uncertainty or ambiguity in expectations.  \n6. It may demand creativity or innovation, which can be challenging.  \n7. It might require balancing competing priorities or constraints.  \n8. It can be influenced by external factors that are difficult to control.  \n9. It may involve overcoming personal biases or limitations.  \n10. It could be affected by the need for coordination with others, leading to potential miscommunication or delays."
            ]
        },
        {
            "question": "What are the reasons why that is difficult?",
            "reference-answers": [
                "I'm sorry, but I don't have the specific text you're referring to. Could you please provide the text or additional context so I can help you better?"
            ]
        },
        {
            "question": "What are the reasons why that is difficult?",
            "reference-answers": [
                "I'm sorry, but I don't have the specific text you're referring to. Could you please provide the text or additional context so I can help you better?"
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen24-slide25/text.txt": [
        {
            "question": "What are the main challenges faced by syntactic approaches compared to phrase-based or hierarchical approaches?",
            "reference-answers": [
                "The main challenges faced by syntactic approaches compared to phrase-based or hierarchical approaches are that they lose coverage of their rules due to additional constraints and fail to license the rules to be used, resulting in too few rules in the extracted rule tables and too few license derivations."
            ]
        },
        {
            "question": "Why do syntactic approaches face limitations compared to phrase-based or hierarchical approaches, according to the text?",
            "reference-answers": [
                "Syntactic approaches face limitations because they lose coverage of their rules due to additional constraints, resulting in too few extracted rules and too few derivations, making them less effective compared to phrase-based or hierarchical approaches."
            ]
        },
        {
            "question": "The question is:  \n**Why are syntactic approaches less effective compared to phrase-based or hierarchical approaches according to the text?**",
            "reference-answers": [
                "Syntactic approaches are less effective because they have too few rules and insufficient licensed derivations, leading to limited coverage and effectiveness compared to phrase-based or hierarchical approaches."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen25-slide26/text.txt": [
        {
            "question": "What problem arises in syntactic approaches to phrase-based MT when a translation like \"Brucha Verde\" to \"Green Witch\" is considered?",
            "reference-answers": [
                "In syntactic approaches to phrase-based MT, the problem arises when the translation unit \"Green Witch\" isn't a constituent in the syntactic tree, leading to a loss of syntactic information for accurate translation."
            ]
        },
        {
            "question": "The problem arises because \"Green Witch\" isn't a constituent in the target language's syntactic structure. In a syntactic approach, which relies on breaking down sentences into grammatical constituents, the phrase \"Green Witch\" cannot be properly integrated. This leads to translation issues, such as structural incompatibility or loss of meaning, as the phrase doesn't fit into the target language's syntax effectively.",
            "reference-answers": [
                "The problem arises because \"Green Witch\" isn't a constituent in the target language's syntactic structure. In a syntactic approach, which relies on breaking down sentences into grammatical constituents, the phrase \"Green Witch\" cannot be properly integrated. This leads to translation issues, such as structural incompatibility or loss of meaning, as the phrase doesn't fit into the target language's syntax effectively."
            ]
        },
        {
            "question": "What is a limitation of using a syntactic approach in phrase-based MT according to the text?",
            "reference-answers": [
                "A limitation is that using syntactic information in phrase-based MT leads to a loss of syntactic structure or alignment."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen26-slide27/text.txt": [
        {
            "question": "What is the main issue described in the text regarding the coverage of the translation dictionary?",
            "reference-answers": [
                "The main issue is that the translation dictionary lacks coverage for complex noun phrases, requiring the system to decompose them, which can introduce errors."
            ]
        },
        {
            "question": "What does the text imply about the challenges of translating noun phrases in English into another language?",
            "reference-answers": [
                "The text implies that translating English noun phrases into another language can be challenging because they often function as single units in English, requiring decomposition for translation. This decomposition can introduce errors since the target language may require different structures, such as verb phrases, which are not present in the original noun phrase."
            ]
        },
        {
            "question": "What is the main issue the system encounters when translating the sentence \"Taiwan surplus in trade between the two shores\"?",
            "reference-answers": [
                "The main issue is that the system cannot translate the entire noun phrase \"Taiwan surplus in trade between the two shores\" as a whole because the target language requires a verb phrase, necessitating decomposition that may introduce errors."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen27-slide28/text.txt": [
        {
            "question": "Why does the syntactic-based system experience a loss of coverage due to the dangling verb phrase?",
            "reference-answers": [
                "The syntactic-based system experiences a loss of coverage due to the dangling verb phrase because this phrase is a valid constituent in one language but not in the other language, leading to recognition issues in the language where it is not valid."
            ]
        },
        {
            "question": "The loss of coverage in the syntactic-based system is due to the inability to handle dangling verb phrases that are valid constituents in one language but not in another.",
            "reference-answers": [
                "The loss of coverage in the syntactic-based system is due to the inability to handle dangling verb phrases that are valid constituents in one language but not in another."
            ]
        },
        {
            "question": "Why is there a loss of coverage in the syntactic-based system due to the dangling verb phrase, which is a valid constituent in one language but not in the other?",
            "reference-answers": [
                "The loss of coverage in the syntactic-based system is due to the dangling verb phrase, which is a valid constituent in one language but not in the other. This means that the system's syntactic rules fail to account for the cross-linguistic differences, leading to incomplete coverage in one of the languages."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen28-slide29/text.txt": [
        {
            "question": "What is the purpose of binarizing larger phrases in the syntactic system and how does it help in handling grammatically incorrect sentences?",
            "reference-answers": [
                "The purpose of binarizing larger phrases in the syntactic system is to break them down into smaller, more manageable parts. This allows the system to extract phrases more effectively and handle grammatically incorrect sentences by making the grammar more robust. By converting productions with more than two elements into a sequence of binary productions, the system gains more points for alignment, enabling it to process and translate sentences that might otherwise be too complex or incorrect for the system to handle."
            ]
        },
        {
            "question": "What is the purpose of binarizing phrases in the syntactic system, and how does it help the machine translation system handle incorrect or partially wrong inputs?",
            "reference-answers": [
                "The purpose of binarizing phrases in the syntactic system is to make the grammatical constructions smaller and more manageable, allowing the system to extract smaller phrases more frequently. This decomposition helps the machine translation system handle incorrect or partially wrong inputs by providing more points where alignments can match with these subparts. By breaking down larger phrases into binary productions, the system can extract phrases that would otherwise be non-reachable, making it more robust and capable of digesting grammatically incorrect sentences."
            ]
        },
        {
            "question": "What is the purpose of binarizing productions in the syntactic system?",
            "reference-answers": [
                "The purpose of binarizing productions in the syntactic system is to decompose larger grammatical constructions into smaller, binary steps. This allows for more extraction points and enables the system to handle partially incorrect or non-grammatical inputs by aligning with smaller subphrases, making the system more robust and capable of processing a wider variety of sentences."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen29-slide30/text.txt": [
        {
            "question": "What approach did Ashish Venugopal and Andras Solman use for machine translation that constructs phrases without considering linguistic motivation?",
            "reference-answers": [
                "Ashish Venugopal and Andras Solman used the **syntax augmented machine translation** approach. This method constructs phrases by combining pieces of language without considering whether those pieces have any linguistic motivation. It focuses on the data itself, creating non-terminal units that may not align with traditional grammar, making it useful for translating non-grammatical content."
            ]
        },
        {
            "question": "What was the approach by Ashish Venugopal and Andras Solman, and why was it considered useful for machine translation despite not being based on linguistic grammar?",
            "reference-answers": [
                "The approach by Ashish Venugopal and Andras Solman was called syntax augmented machine translation. It involved constructing artificial phrases by combining pieces like noun phrases, regardless of whether they formed linguistically meaningful constituents. This method was considered useful for machine translation because it focused on the data available, allowing the processing of non-grammatical or irregular phrases that people have written, even if they don't align with traditional grammar rules."
            ]
        },
        {
            "question": "What is another approach to binarization besides the one mentioned?",
            "reference-answers": [
                "Another approach to binarization was syntax augmented machine translation, where artificial phrases are constructed by combining pieces regardless of linguistic motivation, useful for translation purposes even if not based on grammar."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen30-slide31/text.txt": [
        {
            "question": "What is the problem that arises when an NP from a multi-word expression like \"checkpoints\" cannot be plugged into a slot expecting a plural noun in substitution grammar?",
            "reference-answers": [
                "The problem arises because substitution grammar requires non-terminals to match exactly when plugging pieces together. When an NP (noun phrase) from a multi-word expression like \"checkpoints\" is created during parsing, it cannot be plugged into a slot expecting a single noun in plural form. This mismatch causes a failure in constructing the correct synchronous tree."
            ]
        },
        {
            "question": "The problem arises because the synchronous tree substitution grammar expects specific non-terminals to match when plugging in phrases, but multi-word noun phrases like \"checkpoints\" are parsed as NPs, which cannot be substituted into slots expecting a single noun due to a lack of appropriate licensing rules.",
            "reference-answers": [
                "The problem arises because the synchronous tree substitution grammar expects specific non-terminals to match when plugging in phrases, but multi-word noun phrases like \"checkpoints\" are parsed as NPs, which cannot be substituted into slots expecting a single noun due to a lack of appropriate licensing rules."
            ]
        },
        {
            "question": "What is the specific issue that arises when trying to plug the NP (noun phrase) into the slot expecting a noun in plural, given the context of the grammar system described?",
            "reference-answers": [
                "The specific issue is that the NP (noun phrase) cannot be plugged into the slot expecting a noun in plural because the grammar system's synchronous tree substitution grammar requires non-terminals to match exactly. Since the NP is processed as a multi-word phrase (e.g., \"checkpoints\") instead of a single non-terminal, it cannot be directly substituted into the slot expecting a singular noun in plural, leading to a structural mismatch."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen31-slide32/text.txt": [
        {
            "question": "What approach was described to make the model more flexible by allowing certain substitutions while imposing penalties for unexpected ones?",
            "reference-answers": [
                "The approach described was to allow certain substitutions while imposing penalties for unexpected ones, as discussed in David Cheng's paper. This method makes the model more flexible by enabling substitutions that are necessary for the data while adding a cost when substitutions do not align with the grammar."
            ]
        },
        {
            "question": "How are substitution costs handled in a parsing model to make it robust for real inputs, considering the differences between STSG and higher-level approaches, and how does this relate to David Cheng's work on allowing necessary substitutions with penalties?",
            "reference-answers": [
                "In a parsing model, substitution costs are handled by allowing some flexibility to handle real inputs, which often deviate from idealized data. In the case of Substitutional Tree-Structured Grammar (STSG), only matching substitutions are permitted, making it less robust for real-world data. In contrast, higher-level approaches treat all non-terminals as the same, allowing any substitution without restriction. To strike a balance, David Cheng's work introduces a method where substitutions are allowed but with penalties when substitutions deviate from what the grammar anticipates. This approach ensures the model remains robust by allowing necessary substitutions while controlling for deviations through cost penalties."
            ]
        },
        {
            "question": "David Cheng's paper focuses on enhancing the flexibility of the model by allowing certain substitutions based on the data, while imposing penalties for substitutions that are not anticipated by the grammar.",
            "reference-answers": [
                "David Cheng's paper focuses on enhancing the flexibility of the model by allowing certain substitutions based on the data, while imposing penalties for substitutions that are not anticipated by the grammar."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen32-slide33/text.txt": [
        {
            "question": "What method is used to construct the target side during translation as described in the text?",
            "reference-answers": [
                "The target side is constructed in parallel with the consumption of the source during translation."
            ]
        },
        {
            "question": "What is the key process for translation described in the text, and what are its main components?",
            "reference-answers": [
                "The key process for translation described in the text is the bottom-up parsing approach, where translation is achieved through chart parsing. The main components include parsing the source language, constructing the target side in parallel, using a hierarchical model with a single non-terminal, integrating the language model via state splitting, keeping track of words in source boxes for scoring, employing binarization for non-isomorphic trees, and allowing fuzzy matching for flexibility."
            ]
        },
        {
            "question": "**Question:**  \nWhat is the purpose of using the bottom-up parsing approach in the context of machine translation as described in the text?",
            "reference-answers": [
                "The purpose of using the bottom-up parsing approach in machine translation is to systematically digest the input by breaking it into smaller pieces and combining them into larger units, while constructing the target language sentence in parallel. This approach ensures that the language model can score the translation by keeping track of the exact words used in the source, even when the input is not perfectly grammatical or when the training data does not cover specific examples."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen33-slide34/text.txt": [
        {
            "question": "What can the user recommend for an illustration and animation related to the topic?",
            "reference-answers": [
                "The teacher recommends the talk for an illustration and animation related to the topic."
            ]
        },
        {
            "question": "What does the speaker recommend for an illustration and animation?",
            "reference-answers": [
                "The speaker recommends dependency syntax for an illustration and animation."
            ]
        },
        {
            "question": "What does the author recommend for illustration and animation when moving from constituency syntax to dependency syntax?",
            "reference-answers": [
                "The author recommends using illustration and animation for the transition from constituency syntax to dependency syntax."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen34-slide35/text.txt": [
        {
            "question": "What does the text say about the difference between constituency trees and dependency trees?",
            "reference-answers": [
                "The text explains that constituency trees represent the structure of sentences through bracketing, grouping adjacent words into larger units using a divide and conquer method without considering internal history. In contrast, dependency trees focus on the final sentence structure, showing which words depend on others through edges that indicate specific linguistic relationships, such as verbs expecting certain nouns or prepositions, and nouns anticipating adjectives that share their gender."
            ]
        },
        {
            "question": "What is the key difference between constituency trees and dependency trees in terms of how they represent sentence structure and dependencies?",
            "reference-answers": [
                "The key difference is that constituency trees use a divide and conquer approach to represent how adjacent constituents are grouped together, showing the history of bracketing, while dependency trees represent the sentence as it is written, focusing on the dependencies between words without recording the bracketing history. Dependency trees indicate which words depend on others along edges, designed by linguists to capture relationships like verbs subcategorizing for nouns or adjectives sharing the gender of nouns."
            ]
        },
        {
            "question": "What is the main difference between constituency trees and dependency trees as described in the text?",
            "reference-answers": [
                "The main difference is that constituency trees represent the structure of a sentence by showing how adjacent constituents combine through a divide-and-conquer approach, focusing on the bracketing history. In contrast, dependency trees describe the relationships between words in the final sentence, indicating which words depend on others, based on linguistic rules like subcategorization and gender agreement."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen35-slide36/text.txt": [
        {
            "question": "What are the two main errors in the Google translation of the sentence provided, and how do they relate to the source language's structure?",
            "reference-answers": [
                "The two main errors in the Google translation were a bad lexical choice for \"cutting,\" which in English is a general verb but in Czech requires a more specific verb, and an incorrect case for \"trava\" due to the passive/active voice, affecting its grammatical structure in Czech."
            ]
        },
        {
            "question": "What two errors are present in the translation of the sentence about cutting grass, and how do they relate to long distance dependencies and case usage in Czech?",
            "reference-answers": [
                "The two errors in the translation of the sentence about cutting grass are:  \n1. **Lexical choice error**: The translation used an incorrect or unspecific verb for \"cutting,\" which is crucial in Czech due to its detailed lexical distinctions for different types of cutting (e.g., specific verbs for cutting grass vs. cutting bread). This error highlights the importance of lexical specificity in languages like Czech, especially when dealing with long distance dependencies where the verb and its referent can be separated by many words.  \n2. **Case usage error**: The translation failed to correctly assign the grammatical case of the noun \"grass\" (trava), depending on whether the sentence was active or passive. In Czech, the case of the noun changes based on its syntactic role (accusative in active sentences, nominative in passive sentences). This error underscores the relationship between verb form (active/passive) and case usage, which is influenced by the syntactic structure of the sentence, including long distance dependencies."
            ]
        },
        {
            "question": "What are the two errors in the Google translation of the sentence \"The grass around your house should be cut soon\" into Czech, and how are they related to long distance dependencies and case usage?",
            "reference-answers": [
                "The two errors in the Google translation are:  \n1. **Lexical choice**: The translation used the general verb \"cut\" instead of a more specific Czech verb like \"výstřih\" for grass cutting.  \n2. **Case usage**: The noun \"tráva\" (grass) was incorrectly assigned the wrong case, depending on whether the sentence was active or passive.  \n\nThese errors are related to long distance dependencies because the words \"grass\" and \"cutting\" are separated by a long phrase (\"around your house should be\"), making it harder for the translation system to maintain the correct syntactic and semantic connections. Additionally, the case usage error is tied to the verb's voice, which affects whether \"tráva\" is in the accusative or nominative case."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen36-slide37/text.txt": [
        {
            "question": "How does the dependency tree approach compare to n-grams in capturing word relationships, and what statistics support this?",
            "reference-answers": [
                "The dependency tree approach is more effective than n-grams in capturing word relationships because it directly connects dependent words, even if they are not adjacent. For example, in a dependency tree, \"grass\" and \"cut\" are neighbors despite being far apart in the text. In contrast, n-grams require long sequences to capture such relationships. Statistics from a Czech tree bank show that 50% of edges connect neighboring words, and 80% fit within a short range, supporting the idea that phrase-based translation is a good approximation of dependencies. This efficiency makes dependency trees superior for capturing grammatical and lexical choices compared to n-grams."
            ]
        },
        {
            "question": "The text suggests that dependency trees effectively capture grammatical relationships in language processing, often connecting words that are adjacent, but also handling non-adjacent words through their structural dependencies. This approach is more efficient than n-grams, which rely on adjacency and require longer sequences to capture similar relationships.",
            "reference-answers": [
                "The text suggests that dependency trees effectively capture grammatical relationships in language processing, often connecting words that are adjacent, but also handling non-adjacent words through their structural dependencies. This approach is more efficient than n-grams, which rely on adjacency and require longer sequences to capture similar relationships. For example, in a Czech tree bank, 50% of edges link neighbors, and 80% of edges fit in a foreground, indicating that phrase-based translation is a very good approximation of the dependencies. Dependency trees are particularly effective because they can handle non-adjacent words through structural connections, which makes them more efficient than n-grams that rely on adjacency."
            ]
        },
        {
            "question": "Question: Explain why dependency trees are more effective than n-grams in predicting lexical choices and grammatical categories, using the example of the words \"grass\" and \"cut,\" and discuss the statistics provided about edges in the Czech tree bank.",
            "reference-answers": [
                "Dependency trees are more effective than n-grams in predicting lexical choices and grammatical categories because they capture the essential relationships between words, even when they are not adjacent. For example, the words \"grass\" and \"cut\" may be far apart in a sentence, but in a dependency tree, they are connected directly, reflecting their grammatical relationship. This connection remains regardless of how many words are inserted between them, unlike n-grams, which require long sequences to capture such relationships.\n\nStatistics from the Czech tree bank show that 50% of edges link neighboring words, and 80% fit within a short range, indicating that phrase-based translation closely approximates these dependencies. This highlights the efficiency of dependency trees in representing grammatical and lexical connections without relying on long sequences, making them superior to n-grams for predicting context and relationships."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen37-slide38/text.txt": [
        {
            "question": "How can gap phrases be used in hierarchical structures to manage long-distance dependencies?",
            "reference-answers": [
                "Gap phrases can be used in hierarchical structures to manage long-distance dependencies by bridging elements that are far apart. For example, in the sentence \"This grass around your house that should be cut soon,\" the gap phrase \"that should be cut soon\" creates a link between \"grass\" and \"cutting,\" even though they are separated by intervening words. This allows the hierarchical structure to handle distant dependencies effectively."
            ]
        },
        {
            "question": "What is one advantage of using hierarchical phrases with gaps according to the text?",
            "reference-answers": [
                "Hierarchical phrases with gaps allow words that are far apart to be connected, covering the distance between them."
            ]
        },
        {
            "question": "What does the example of \"this grass around your house that should be cut soon\" illustrate about the use of hierarchical phrases with gaps?",
            "reference-answers": [
                "The example illustrates that hierarchical phrases with gaps can effectively cover long-distance dependencies, allowing elements like \"this grass\" and \"cutting\" to be separated by multiple words while maintaining grammatical cohesion."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen38-slide39/text.txt": [
        {
            "question": "What is the advantage of dependency trees over constituency trees in representing sentences with crossing brackets, as discussed in the text?",
            "reference-answers": [
                "Dependency trees have an advantage over constituency trees in representing sentences with crossing brackets because they do not rely on the hierarchical assumption of constituency trees. Instead, they focus on the dependencies between words, making it easier to represent sentences where words are reordered without the complications of crossing brackets. In dependency trees, words like Mary and John are shown to depend on the verb \"loves,\" allowing for a clearer representation of the sentence structure despite word shuffling, especially in languages like Czech with free word order."
            ]
        },
        {
            "question": "How do dependency trees handle the phenomenon of crossing brackets compared to constituency trees?",
            "reference-answers": [
                "Dependency trees handle the phenomenon of crossing brackets differently from constituency trees by not requiring the use of traces or non-terminals. Instead, they show dependencies directly, allowing for a more flexible representation of word order. In Czech and other free word order languages, this approach avoids the complexities of crossed brackets, as the dependencies are maintained through the heads of each constituent, resulting in a simpler and more accurate structure."
            ]
        },
        {
            "question": "What phenomenon occurs when a word like \"Mary\" appears outside the VP in a sentence such as \"Mary John loves,\" and how do linguists explain this using traces?",
            "reference-answers": [
                "The phenomenon is called crossing brackets. When a word like \"Mary\" appears outside the VP in a sentence such as \"Mary John loves,\" it has been moved from its original position in the VP, leaving a trace. Linguists explain this by introducing traces, which are co-indexed variables indicating where the word was moved from, allowing the grammar to account for the word's movement while maintaining the hierarchical structure."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen39-slide40/text.txt": [
        {
            "question": "What is the reason that context-free grammars are inadequate for representing non-projective dependencies like those described in the example of Dutch subordinate clauses?",
            "reference-answers": [
                "Context-free grammars are inadequate for representing non-projective dependencies like those described in the example of Dutch subordinate clauses because they cannot handle the gaps created by non-projective structures. In Dutch, the dependency tree has gaps due to the cross-serial dependencies, which require a specific word order that subjects come before predicates. Context-free grammars cannot represent such non-projective trees without violating their projectivity constraint, which assumes a one-to-one correspondence between nodes and spans in the tree."
            ]
        },
        {
            "question": "What is the specific issue with the dependency tree in the Dutch example discussed in the text?",
            "reference-answers": [
                "The specific issue with the dependency tree in the Dutch example is that it is non-projective due to the presence of a gap in the subtree, which arises from the cross-serial dependencies in Dutch syntax. This gap creates a situation where the dependency tree cannot be represented without breaks, as the subjects and predicates are ordered separately, leading to a non-contiguous arrangement of words in the sentence."
            ]
        },
        {
            "question": "What does the Dutch example illustrate about non-projective dependency trees? Explain the concept of projectivity and non-projectivity in this context.",
            "reference-answers": [
                "The Dutch example illustrates that non-projective dependency trees occur when the structure of a sentence requires dependencies that cannot be represented as a single, continuous block in the tree. Projectivity refers to a dependency tree where all nodes cover a contiguous part of the sentence without any gaps. In contrast, non-projectivity occurs when there are gaps in the tree, meaning some elements are not contiguous. In the Dutch example, the sentence \"that John children saw swim\" demonstrates non-projectivity because the subjects (\"John\" and \"children\") and predicates (\"saw\" and \"swim\") create dependencies that cross over each other, leading to a gap in the dependency tree. This shows that non-projective dependency trees cannot be represented by a context-free grammar, as they require more complex structures to account for the gaps."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen40-slide41/text.txt": [
        {
            "question": "Why is non-projectivity important for machine translation, and how does the Dutch example of \"John Grass saw being cut\" demonstrate this?",
            "reference-answers": [
                "Non-projectivity is important for machine translation because it challenges the way parsers and generators handle sentence structures, particularly when dependencies are not directly connected. The Dutch example \"John Grass saw being cut\" demonstrates this by showing that the verb \"saw\" connects to both \"John\" and \"being cut,\" while \"grass\" is linked to \"being cut\" through another dependency. This non-projective structure highlights the need for translation systems to account for such complex dependencies, as traditional parsing methods might not capture all the relationships accurately."
            ]
        },
        {
            "question": "Why is non-projectivity important for machine translation, as demonstrated by the Dutch sentence \"John Grass saw being cut\"?",
            "reference-answers": [
                "Non-projectivity is important for machine translation because it allows the model to capture dependencies between words that are not directly adjacent, such as the relationship between \"grass\" and \"being cut\" in the Dutch sentence \"John Grass saw being cut.\" This capability is crucial for accurately translating sentences with complex structures that cannot be represented by projective parsing alone."
            ]
        },
        {
            "question": "Why is non-projectivity important for machine translation?",
            "reference-answers": [
                "Non-projectivity is important for machine translation because it allows the system to handle sentence structures where dependencies span across other elements, as seen in Dutch. This ensures accurate translation of languages that require specific word orders, like the example where \"John Grass saw being cut\" correctly captures the relationship between \"grass\" and \"being cut.\""
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen41-slide42/text.txt": [
        {
            "question": "What is the main reason the hierarchical model is inadequate for translating into Czech according to the text?",
            "reference-answers": [
                "The main reason the hierarchical model is inadequate for translating into Czech is because it cannot handle the crossing dependencies in non-projective constructions, which leads to the system making incorrect choices when trying to apply the rules simultaneously."
            ]
        },
        {
            "question": "**Question:**  \nWhy is a hierarchical model insufficient for translating into Czech according to the text?",
            "reference-answers": [
                "A hierarchical model is insufficient for translating into Czech because it cannot handle crossing dependencies in a single derivation. Specifically, the dependencies between the subject and predicate (e.g., John and seeing) and between the verb and its object (e.g., cutting and grass) conflict when applied simultaneously. The hierarchical approach forces the system to choose one rule over the other, leading to incorrect translations, especially with non-projective constructions. This limitation results in a high risk of making the wrong choice in translation."
            ]
        },
        {
            "question": "The hierarchical model is insufficient for translating into Czech because non-projective constructions, such as the dependencies between \"John\" and \"seeing\" and between \"cutting\" and \"grass,\" cannot be resolved simultaneously without disrupting one another, leading to potential errors in translation.",
            "reference-answers": [
                "The hierarchical model is insufficient for translating into Czech because non-projective constructions, such as the dependencies between \"John\" and \"seeing\" and between \"cutting\" and \"grass,\" cannot be resolved simultaneously without disrupting one another, leading to potential errors in translation."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen42-slide43/text.txt": [
        {
            "question": "What percentage of Czech sentences exhibit non-projectivity, and how frequent are the severe cases with multiple gaps?",
            "reference-answers": [
                "About 25% of Czech sentences exhibit non-projectivity, but severe cases with multiple gaps are rare, occurring in only 0.5% of sentences."
            ]
        },
        {
            "question": "What percentage of Czech sentences are reported to be well-nested with at most one gap according to the text?",
            "reference-answers": [
                "99.5%"
            ]
        },
        {
            "question": "What percentage of Czech sentences exhibit severe non-projectivity with multiple gaps?",
            "reference-answers": [
                "0.5%"
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen43-slide44/text.txt": [
        {
            "question": "What is the primary reason dependency trees are more appropriate for Czech, and which researcher found they exhibit less divergence across languages?",
            "reference-answers": [
                "The primary reason dependency trees are more appropriate for Czech is due to the frequent non-projectivity in the language. Heidi Fox found that dependency trees exhibit less divergence across languages."
            ]
        },
        {
            "question": "Question: Why are dependency trees considered more appropriate for Czech compared to constituency trees, and what does Heidi Fox's study suggest about their use across languages?",
            "reference-answers": [
                "Dependency trees are more appropriate for Czech than constituency trees due to the frequent non-projectivity in Czech grammar, which they handle more effectively. Heidi Fox's study indicates that dependency trees exhibit less divergence across languages compared to constituency trees, suggesting their broader applicability."
            ]
        },
        {
            "question": "What makes dependency trees more appropriate for Czech compared to constituency trees, and how does Heidi Fox's study support their effectiveness across languages?",
            "reference-answers": [
                "Dependency trees are more appropriate for Czech compared to constituency trees because Czech frequently exhibits non-projectivity, which constituency trees handle less effectively. Heidi Fox's study supports their effectiveness across languages by showing that dependency trees exhibit less divergence than constituency trees."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen44-slide45/text.txt": [
        {
            "question": "What is the primary idea presented in the text regarding machine translation and dependency trees?",
            "reference-answers": [
                "The primary idea is that dependency trees are used in machine translation, where the trees consist only of terminals (no non-terminals)."
            ]
        },
        {
            "question": "What is true about non-terminals in the dependency trees described in the text?",
            "reference-answers": [
                "There are no non-terminals in the dependency trees described."
            ]
        },
        {
            "question": "What type of tree is used in machine translation when observing a pair of trees with no non-terminals, and what characteristic makes them suitable for this purpose?",
            "reference-answers": [
                "The type of tree used is a dependency tree. The characteristic that makes them suitable for this purpose is that they have no non-terminals, meaning they consist solely of terminals connected by dependencies."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen46-slide47/text.txt": [
        {
            "question": "What is the purpose of decomposing trees into smaller treelets and using substitution grammars synchronously on the source and target sides?",
            "reference-answers": [
                "The purpose is to decompose trees into smaller treelets, creating a dictionary, and use substitution grammars to apply these treelets simultaneously on the source and target sides, thereby covering the source sentence and building the target sentence."
            ]
        },
        {
            "question": "The process involves decomposing source sentence structures (trees) into smaller units called treelets, which are stored in a dictionary. These treelets are then used alongside substitution grammars to align and translate sentences between languages, allowing for the construction of target sentences by applying these units synchronously on both source and target sides.",
            "reference-answers": [
                "The process involves decomposing source sentence structures (trees) into smaller units called treelets, which are stored in a dictionary. These treelets are then used alongside substitution grammars to align and translate sentences between languages, allowing for the construction of target sentences by applying these units synchronously on both source and target sides."
            ]
        },
        {
            "question": "Which step comes first in the described process: decomposing the trees into smaller treelets, extracting the treelets, creating a dictionary of treelets, or using substitution grammars to apply them synchronously?",
            "reference-answers": [
                "The first step is decomposing the trees into smaller treelets."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen47-slide52/text.txt": [
        {
            "question": "What was the approach in the thesis designed to be applicable for, in terms of layers of description?",
            "reference-answers": [
                "The approach was designed to be applicable at any of the layers of description, specifically the surface syntactic level and the deep syntactic level."
            ]
        },
        {
            "question": "What was the goal of the thesis regarding the application of the synchronous tree substitution grammar approach?",
            "reference-answers": [
                "The goal of the thesis was to transfer information using syntactic trees through the synchronous tree substitution grammar approach, applying it at both the surface and deep syntactic levels, and transitioning from shallow structures on the source side to deeper structures on the target side."
            ]
        },
        {
            "question": "What were the layers at which the approach was designed to be applicable?",
            "reference-answers": [
                "The approach was designed to be applicable at the surface syntactic level and the deep syntactic level."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen48-slide49/text.txt": [
        {
            "question": "What does the text discuss regarding deep syntax?",
            "reference-answers": [
                "The text discusses deep syntax and mentions that there is a dedicated section or discussion on it."
            ]
        },
        {
            "question": "What is mentioned as the topic of the dedicated talk?",
            "reference-answers": [
                "deep syntax"
            ]
        },
        {
            "question": "What is mentioned as being related to deep syntax in the text?",
            "reference-answers": [
                "A dedicated empty talk is mentioned as being related to deep syntax."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen49-slide50/text.txt": [
        {
            "question": "What is the key difference between the STSG approach and the tecto-empty machine translation system that contributed to the latter's success?",
            "reference-answers": [
                "The key difference is that the tecto-empty machine translation system did not make the assumptions that the STSG approach did, which contributed to its success."
            ]
        },
        {
            "question": "Why did the STSG approach fail in the context of the tectogrammatic layer application, and what was one of the interesting components of the more successful tecto-empty machine translation system?",
            "reference-answers": [
                "The STSG approach failed in the context of the tectogrammatic layer application because it made assumptions that were not appropriate, leading to its unsuccessful application. The more successful approach, the tecto-empty machine translation system, did not make these assumptions and was therefore more effective. One of the interesting components of this successful system was the use of a hidden Markov 3 model."
            ]
        },
        {
            "question": "**Answer:**  \nOne of the interesting components of the tecto-empty machine translation system is the hidden Markov 3 model.",
            "reference-answers": [
                "One of the interesting components of the tecto-empty machine translation system is the hidden Markov 3 model."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen50-slide51/text.txt": [
        {
            "question": "What is the origin of the textogrammatic layer theory and when was it further developed?",
            "reference-answers": [
                "The textogrammatic layer theory originates from Panini's grammar, dating back thousands of years, and was further developed in the mid-20th century by the Prague linguistic circle during the mid-war era. It was formalized in the 1960s, with additional efforts to automate deep syntactic analysis around 2001 or 2004."
            ]
        },
        {
            "question": "What significant efforts were made in the development of the textogrammatic layer and when did they occur?",
            "reference-answers": [
                "The textogrammatic layer has a long history, with its roots in Panini grammar and thousands of years of linguistics. It was formally developed in the mid-war era by the Prague linguistic circle, and in the sixties of the previous century, it was written down as a form of theory. A significant effort culminated around the millennium, around 2001 or 2004, involving the annotation of many languages, including Czech and English, at a deep syntactic level. Additionally, tools were developed around this time to automate the process of generating deep syntactic trees from input text, marking a major advancement in the field."
            ]
        },
        {
            "question": "When was the textogrammatic layer theory formalized, and what efforts were made around the year 2001 to create tools for deep syntactic analysis?",
            "reference-answers": [
                "The textogrammatic layer theory was formalized in the mid-20th century, during the Prague linguistic circle's work in the mid-war era, and further developed in the sixties. Around the year 2001, efforts focused on creating tools for deep syntactic analysis, involving the annotation of sentences in Czech and English and automating the generation of deep syntactic trees."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen51-slide52/text.txt": [
        {
            "question": "What example is given to illustrate the addition of nodes for entities that were syntactically part of the sentence but not spoken, and how does this relate to the concept of prodrop languages like Czech?",
            "reference-answers": [
                "The example given is the sentence \"he would have gone to the woods.\" In Czech, which is a prodrop language, the subject \"he\" is not pronounced but is implied. At the deep syntactic level, an actor node is added for the subject, which is later pro-dropped. This illustrates how nodes are added for entities that are syntactically part of the sentence but not spoken, reflecting the prodrop feature of Czech where the subject pronoun is omitted but still present in the syntactic structure."
            ]
        },
        {
            "question": "What concept is illustrated by the example of the Czech sentence where the subject pronoun is dropped?",
            "reference-answers": [
                "The concept illustrated by the example is that Czech is a prodrop language, meaning it drops subject pronouns, but the subject is still syntactically represented in the deep structure with a node added for the dropped subject."
            ]
        },
        {
            "question": "What type of node is added for an entity that is part of the sentence syntactically but was not spoken, such as the subject in a prodrop language like Czech?",
            "reference-answers": [
                "The type of node added for an entity that is part of the sentence syntactically but was not spoken is an entity node."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen52-slide52/text.txt": [
        {
            "question": "What are the main components of the tectogrammatical tree as illustrated in the example of \"toby sem mělo zněnit\"?",
            "reference-answers": [
                "The main components of the tectogrammatical tree as illustrated in the example of \"toby sem mělo zněnit\" are:\n\n1. **Predicate**: Represents the core event or action, such as \"changing.\"\n2. **Modality**: An attribute indicating the modal aspect, like \"something should be changed.\"\n3. **Participants**: Includes the actor (the one who should change it) and the patient (the thing that should be changed)."
            ]
        },
        {
            "question": "What changes occur to the predicate when moving from the analytical to the tectogrammatical level of representation?",
            "reference-answers": [
                "When moving from the analytical to the tectogrammatical level, the predicate undergoes changes where auxiliary words are hidden, and the information they carry is stored in attributes. Additionally, active and passive voice distinctions and coreference, such as pronouns, are resolved. This transformation results in a more abstract representation, focusing on the core event and its participants."
            ]
        },
        {
            "question": "What are the main components of the tectogrammatical tree as illustrated in the example of \"toby sem mělo zněnit\" and its English counterpart?",
            "reference-answers": [
                "The main components of the tectogrammatical tree, as illustrated in the example of \"toby sem mělo zněnit\" and its English counterpart, include the predicate (the changing), the modality (something should be changed), the actor (who should change it), and the patient (what should be changed)."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen53-slide52/text.txt": [
        {
            "question": "How do the surface syntactic structures of Czech and English sentences differ in terms of auxiliaries?",
            "reference-answers": [
                "The surface syntactic structures of Czech and English sentences differ in that Czech sentences have more auxiliaries than English, and these auxiliaries do not match well with each other."
            ]
        },
        {
            "question": "How do the syntactic structures of Czech and English differ with respect to the use of auxiliaries?",
            "reference-answers": [
                "The syntactic structures of Czech and English differ in that Czech sentences tend to have more auxiliaries than English sentences, and these auxiliaries do not match well with each other."
            ]
        },
        {
            "question": "How do the syntactic structures of Czech and English differ in terms of auxiliaries?",
            "reference-answers": [
                "Czech sentences have more auxiliaries than English sentences, and these auxiliaries do not align well with each other."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen54-slide52/text.txt": [
        {
            "question": "Are the tectogrammatical trees described in the text isomorphic, and if so, what does this indicate about their structural relationship in terms of the predicate argument structure?",
            "reference-answers": [
                "Yes, the tectogrammatical trees are isomorphic, which means they share the same structural relationship in terms of predicate argument structure. This indicates that both trees represent the same event, where there is someone who should be changing things (the agent) and something that refers to what should be changed (the pronoun, serving as the theme)."
            ]
        },
        {
            "question": "What components are described in the predicate argument structure of the sentence discussed in the text?",
            "reference-answers": [
                "The components described in the predicate argument structure of the sentence are:  \n1. An event of changing.  \n2. Someone who should be changing things.  \n3. Something (a pronoun) that refers to what should be changed."
            ]
        },
        {
            "question": "What structural property do tectogrammatical trees share, and what are the components involved in the predicate argument structure described in the text?",
            "reference-answers": [
                "The structural property that tectogrammatical trees share is that they are isomorphic. The components involved in the predicate argument structure described in the text are: an event of changing, someone who should be changing things, and a pronoun that refers to what should be changed."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen55-slide56/text.txt": [
        {
            "question": "What are the advantages of using tectogrammatic trees for translation between English and Czech?",
            "reference-answers": [
                "The advantages of using tectogrammatic trees for translation between English and Czech include:\n\n- The transfer at the tectogrammatic layer is easier due to smaller structures, with all auxiliary words dropped.\n- Long-distance dependencies and non-projectivities disappear at the tectogrammatic layer, simplifying the process.\n- Word order, which is language-specific, is handled during the generation of the surface tree from the tectogrammatic tree.\n- The vocabulary size is reduced by using lemmas or tectogrammatic lemmas in the trees.\n- The system aims to use smaller datasets (smaller M and fewer parallel trees) while still requiring sufficient data for reliable analysis and synthesis.\n- Features like coreference can be handled effectively."
            ]
        },
        {
            "question": "What is the key advantage of tectogrammatical translation over direct translation according to the text?",
            "reference-answers": [
                "The key advantage is that tectogrammatical translation involves smaller structures, making it easier due to the absence of auxiliary words and the elimination of long distance dependencies and non-projectivities."
            ]
        },
        {
            "question": "What is the advantage of using the tectogrammatical layer in translation compared to direct translation, and how does it handle word order and vocabulary?",
            "reference-answers": [
                "The advantage of using the tectogrammatical layer in translation is that it simplifies the process by reducing the complexity of linguistic structures, such as dropping auxiliary words and eliminating long-distance dependencies, which are not relevant at this layer. This makes translation easier compared to direct translation. Word order is language-specific and is introduced during the generation of the surface tree from the tectogrammatic tree, meaning it is handled during the translation process. The vocabulary size is reduced because lemmas or tectogrammatic lemmas are used in the trees, which helps in managing the vocabulary more efficiently."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen56-slide57/text.txt": [
        {
            "question": "What happens after the input is decomposed into tree leds in the context of synchronous tree substitution grammars?",
            "reference-answers": [
                "After the input is decomposed into tree leds, each tree led is replaced with its translation. The translated tree leds are then joined to create the full tree."
            ]
        },
        {
            "question": "What is the purpose of using synchronous tree substitution grammars in the described approach, and how are the tree leds processed to form the full tree?",
            "reference-answers": [
                "The purpose of using synchronous tree substitution grammars in the described approach is to replace parts of the input tree (specifically the red subtree) with smaller English subtrees, which allows for the decomposition, translation, and subsequent reconstruction of the full tree. The tree leds are processed by first decomposing the input into these tree leds, then replacing each tree led with its corresponding translation, and finally joining these translated tree leds together to form the full tree."
            ]
        },
        {
            "question": "What steps are involved in the approach described for handling the input using synchronous tree substitution grammars?",
            "reference-answers": [
                "The steps involved in the approach are:  \n1. Decompose the input into tree leds.  \n2. Replace these tree leds with their translations.  \n3. Join the translated tree leds to create the full tree."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen57-slide58/text.txt": [
        {
            "question": "What does the STSG approach do with the notes, considering the attributes discussed in the text?",
            "reference-answers": [
                "The STSG approach treats all notes as attributes, ensuring that important information such as modality or tense, which are reflected through morphological variation or auxiliaries, is preserved and not lost."
            ]
        },
        {
            "question": "The STSG approach treats all notes' attributes as attributes.",
            "reference-answers": [
                "The STSG approach treats all notes' attributes as attributes, meaning that these attributes, such as modality or tense, are captured as additional labels or attributes of the nodes rather than being represented in the tree structure itself."
            ]
        },
        {
            "question": "What is the main limitation of using tree structures for representing notes, and how does the STSG approach address this issue?",
            "reference-answers": [
                "The main limitation of using tree structures for representing notes is that they do not retain the additional attributes, such as modality or tense, which are crucial for grammatical analysis. The STSG (Synchronous Tree and Sequence Grammar) approach addresses this by treating all notes as attributes, thereby preserving this information."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen58-slide59/text.txt": [
        {
            "question": "What is the main issue discussed regarding the factorization approaches in the text, and how does it relate to the structure and attributes of the tree layers?",
            "reference-answers": [
                "The main issue discussed is the clash between structure and attributes when factorizing, making it challenging to align tree layers. Factorization approaches, whether along structure (STSG) or attributes (T-layer), struggle to ensure compatibility between layers like lemmas, tense, and modality, highlighting the difficulty in reconciling these aspects."
            ]
        },
        {
            "question": "What is the central challenge discussed in the text regarding the factorization of structures and attributes, and what are the two approaches being considered to address this challenge?",
            "reference-answers": [
                "The central challenge is balancing the factorization of structures (tree-based) with attributes (phrase-based or layer-based). The two approaches considered are:  \n1. **STSG (Structure-based)**: Factorizing along the tree structure, creating layers for lemmas, tense, modality, etc., which may struggle to align across layers.  \n2. **T-layer (Attribute-based)**: Factorizing along attributes, possibly creating fully instantiated phrases or factors for attributes like tense and modality before considering the tree structure."
            ]
        },
        {
            "question": "What is the main problem when deciding whether to use STSG or T-layer factorization in machine translation?",
            "reference-answers": [
                "The main problem is ensuring that the layers of trees, such as lemmas, tense, and verb modality, align structurally when deciding between STSG and T-layer factorization."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen59-slide60/text.txt": [
        {
            "question": "What were the empirical findings regarding the effectiveness of the different tree substitution grammars in the study?",
            "reference-answers": [
                "The empirical findings showed that the phrasal trees approach, which is a simple phrase-based model without reordering, was the most effective. The analytical trees, which involve some structure and require sub-trees to match across sentences, were the second best. The tectogrammatical trees, despite their complexity with many attributes, performed the worst."
            ]
        },
        {
            "question": "Which approach performed best empirically, and why?",
            "reference-answers": [
                "The phrasal trees approach performed best empirically. This was because it was a simple phrase-based model without reordering capabilities, yet it outperformed more complex methods like tectogrammatical trees, which had many attributes and were more complex."
            ]
        },
        {
            "question": "Which approach performed best, second-best, and worst in the empirical evaluation, and why?",
            "reference-answers": [
                "In the empirical evaluation, the phrasal trees approach performed best because it was the simplest and did not allow reordering. The analytical trees approach, which used structure and required subtree matching, was second-best. The worst-performing approach was the tectogrammatical nodes approach, despite its complexity and many attributes."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen60-slide61/text.txt": [
        {
            "question": "What were the main reasons for the failure of the system, and how did the accumulation of errors, structural incompatibility, and combinatorial explosion contribute to it?",
            "reference-answers": [
                "The system failed primarily due to three main reasons: the accumulation of errors along the pipeline, structural incompatibility leading to data loss, and the combinatorial explosion in the tectogrammatical approach. The pipeline, being a sequence of processing tools, caused errors to propagate, resulting in data loss. Structural incompatibility further exacerbated this issue, as discussed in David Cheng's slides, by reducing the number of usable threelets due to incompatibility with word alignment. Additionally, the combinatorial explosion overwhelmed the system with a vast search space, leading to inefficiencies and errors. Furthermore, the system's complexity increased the risk of technical errors and made it difficult to optimize and debug, ultimately contributing to its failure."
            ]
        },
        {
            "question": "What were the main reasons for the failure discussed in the text?",
            "reference-answers": [
                "The main reasons for the failure discussed in the text are:\n\n1. **Cumulation of errors along the pipeline**: Errors in processing tools propagate, leading to data loss due to structural incompatibility.\n\n2. **Limitation in translation rules**: The extraction of translation rules compatible with both word alignment and \"three lets\" reduces the number of \"three lets\" that can be extracted.\n\n3. **Combinatorial explosion in the tectogrammatical approach**: The search space is too large, and the NBEST list varies in unimportant attributes, leading to defective outputs.\n\n4. **Problematic independence assumptions**: \n   - Analyzing and factorizing things that can be copied verbatim from the data.\n   - A complex system increases the risk of technical errors and makes it harder to optimize and tune the weights of independent components."
            ]
        },
        {
            "question": "What are the two main problems that caused the failure, as discussed in the text?",
            "reference-answers": [
                "The two main problems that caused the failure are:\n\n1. **Cumulation of Errors in the Pipeline**: Errors accumulate along the processing pipeline, leading to data loss due to incompatible structures. This issue was discussed in the context of error propagation and incompatibility, affecting the system's overall performance.\n\n2. **Two Strong Independence Assumptions**: \n   - Analyzing and factorizing data by copying verbatim without proper analysis, which is not advisable.\n   - A complex system increasing the risk of technical errors and making it harder to optimize and debug, due to the difficulty in tuning the weights of independent components."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen61-slide62/text.txt": [
        {
            "question": "What is the tecto-empty approach, and which layers does it utilize for machine translation?",
            "reference-answers": [
                "The tecto-empty approach is a transfer-based machine translation method that utilizes the morphological, analytical (for shallow syntax), and tectogrammatic layers."
            ]
        },
        {
            "question": "What approach, based on the tectogrammatic layer, was discussed as being transfer-based and following an analysis-transfer-synthesis pipeline in machine translation?",
            "reference-answers": [
                "The tecto-empty approach was discussed as being transfer-based and following the analysis-transfer-synthesis pipeline in machine translation, utilizing the tectogrammatic layer."
            ]
        },
        {
            "question": "What is the name of the machine translation approach that uses the tectogrammatic layer and is referred to as the \"tecto-empty approach\" in the text?",
            "reference-answers": [
                "The tecto-empty approach."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen62-slide63/text.txt": [
        {
            "question": "Who were the key individuals involved in developing the system, and what were their contributions?",
            "reference-answers": [
                "The key individuals involved in developing the system were Zdeněk Žavokrtsky, who started the project, and Martin Popel, who finished and optimized the system to make it work the best."
            ]
        },
        {
            "question": "What careful structure was man-made, and who were the key contributors to it?",
            "reference-answers": [
                "The careful structure that was man-made consists of a number of processing steps, some rule-based and others statistically trained on various types of data. The key contributors to this structure are Zdeněk Žavokrtsky, who started the work, and Martin Popel, who finished it and made it function optimally."
            ]
        },
        {
            "question": "The colleagues mentioned are Zdeněk Žavokrtsky, who started the processing approach, and Martin Popel, who finished and optimized it. The approach combines rule-based steps with statistically trained models, ensuring grammatical output but with potential errors along the way.",
            "reference-answers": [
                "The colleagues mentioned are Zdeněk Žavokrtsky, who started the processing approach, and Martin Popel, who finished and optimized it. The approach combines rule-based steps with statistically trained models, ensuring grammatical output but with potential errors along the way."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen63-slide64/text.txt": [
        {
            "question": "What model is referred to as the transfer stack in the slides?",
            "reference-answers": [
                "The hidden Markov-3 model is referred to as the transfer stack in the slides."
            ]
        },
        {
            "question": "What is the hidden Markov-3 model referred to as in the context of the slides?",
            "reference-answers": [
                "The hidden Markov-3 model is referred to as the transfer stack in the context of the slides."
            ]
        },
        {
            "question": "What model was presented on the final slide as the transfer stack?",
            "reference-answers": [
                "The hidden Markov-3 model was presented on the final slide as the transfer stack."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen64-slide65/text.txt": [
        {
            "question": "What is the root of the tree in the context of the translation layers described?",
            "reference-answers": [
                "The root of the tree is \"should.\""
            ]
        },
        {
            "question": "The phrase \"should is the root of this tree\" means that \"should\" serves as the central node in the syntactic tree, indicating that it is the governing element from which the sentence's structure and meaning originate, particularly highlighting its role as a modal verb that conveys necessity or obligation.",
            "reference-answers": [
                "\"Should\" is the root of this tree, meaning it is the central node in the syntactic tree and serves as the governing element from which the sentence's structure and meaning originate. It highlights its role as a modal verb that conveys necessity or obligation."
            ]
        },
        {
            "question": "What is the structure of the translation process described, and what are the key components and root of the tree?",
            "reference-answers": [
                "The structure of the translation process involves two layers: a morphological layer and an analytic layer. Each word in both layers has a node, and they are labeled accordingly. The key components include the morphological layer, the analytic layer, and their respective nodes. The root of the tree is \"should.\""
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen65-slide66/text.txt": [
        {
            "question": "What are you required to do first, according to the text?",
            "reference-answers": [
                "You will first identify functional words, these are all the auxiliaries."
            ]
        },
        {
            "question": "What is the first step in identifying functional words?",
            "reference-answers": [
                "The first step in identifying functional words is to identify functional words, as these are all the auxiliaries."
            ]
        },
        {
            "question": "What is the first action mentioned for preparing for the exam?",
            "reference-answers": [
                "The first action mentioned for preparing for the exam is identifying functional words, specifically auxiliaries."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen66-slide67/text.txt": [
        {
            "question": "QUESTION:  \nWhere will the auxiliaries belong according to the text?",
            "reference-answers": [
                "The auxiliaries will belong to your mark edges."
            ]
        },
        {
            "question": "What does the text suggest about the placement of auxiliaries when contracting mark edges?",
            "reference-answers": [
                "Auxiliaries should be placed where the mark edges are contracted."
            ]
        },
        {
            "question": "Where will the auxiliaries be placed when contracting mark edges?",
            "reference-answers": [
                "The auxiliaries will be placed within the contracted mark edges."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen67-slide68/text.txt": [
        {
            "question": "What was the purpose of contracting them according to the TEXT?",
            "reference-answers": [
                "The purpose was to build the backbone of the tectogram article."
            ]
        },
        {
            "question": "What was created by contracting them and building the backbone of the tectogram article 3?",
            "reference-answers": [
                "the backbone of the tectogram article 3"
            ]
        },
        {
            "question": "What is the purpose of contracting the X-series in the tectogram article 3?",
            "reference-answers": [
                "The purpose of contracting the X-series in the tectogram article 3 is to build the backbone of the article, ensuring it only contains content-bearing words by hiding and contracting the X-series."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen68-slide69/text.txt": [
        {
            "question": "What is the grammatical category of \"this machine\" as described in the text?",
            "reference-answers": [
                "The grammatical category of \"this machine\" is a noun.\n\nANSWER: noun"
            ]
        },
        {
            "question": "What grammatical category did the machine serve as in the sentence?",
            "reference-answers": [
                "The machine served as a noun."
            ]
        },
        {
            "question": "What grammatical category did \"this machine\" serve as in the sentence?",
            "reference-answers": [
                "noun"
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen69-slide70/text.txt": [
        {
            "question": "What is the form of the verb in the sentence?",
            "reference-answers": [
                "The verb in the sentence is in its base form."
            ]
        },
        {
            "question": "What grammatical information is provided about the verb in the text?",
            "reference-answers": [
                "The verb's grammatical information includes:  \n- Tense: simple present  \n- Number: singular  \n- The verb was in base form (unconditional)."
            ]
        },
        {
            "question": "What is the correct form of the verb in the sentence considering the grammatical context provided?",
            "reference-answers": [
                "The correct form of the verb is in the simple present tense, singular, and in its base form."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen70-slide71/text.txt": [
        {
            "question": "What percentage of sentences are assumed to be isomorphic when moving from the source to the target tree, and what is the success rate of this assumption?",
            "reference-answers": [
                "The percentage of sentences assumed to be isomorphic when moving from the source to the target tree is 92%, and the success rate of this assumption is also 92%."
            ]
        },
        {
            "question": "What percentage of sentences are assumed to go beyond the isomorphic trees in the described translation process?",
            "reference-answers": [
                "8%"
            ]
        },
        {
            "question": "What percentage of sentences go beyond the assumption that the trees are isomorphic when moving from the source to the target side tector-hormatical tree?",
            "reference-answers": [
                "8%"
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen71-slide72/text.txt": [
        {
            "question": "What are the possible Czech translations for the English word \"machine\"?",
            "reference-answers": [
                "The possible Czech translations for the English word \"machine\" are **počítač**, **stroj**, or **strojový**."
            ]
        },
        {
            "question": "In what ways can subject complements be translated?",
            "reference-answers": [
                "Subject complements can be translated in several ways: they can be translated as complements, as subjects, or as adverbs. Additionally, attributes can be translated using either an attribute, a genitive noun, or an adjective in this attributive position."
            ]
        },
        {
            "question": "What are the possible translations for a complement in the context of lexical translation?",
            "reference-answers": [
                "The possible translations for a complement in the context of lexical translation are: complement, subject, or adverb."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen72-slide73/text.txt": [
        {
            "question": "**Question:**  \nWhat is the Markov 3 model used for in the context described in the text?",
            "reference-answers": [
                "The Markov 3 model is used to select the best combination of options."
            ]
        },
        {
            "question": "What method is mentioned for selecting the best combination in the talk?",
            "reference-answers": [
                "Markov 3 model"
            ]
        },
        {
            "question": "What does the speaker refer to when mentioning the \"last slide of this talk today\"?",
            "reference-answers": [
                "The speaker refers to the part of the talk where the best combination selected using the Markov 3 model will be demonstrated or discussed."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen73-slide74/text.txt": [
        {
            "question": "What is the first step in producing the final surface syntactic tree?",
            "reference-answers": [
                "The first step is to clone the tree."
            ]
        },
        {
            "question": "What is the first step in producing the final surface syntactic tree?",
            "reference-answers": [
                "The first step is cloning the tree."
            ]
        },
        {
            "question": "What is the first step in producing the final surface syntactic tree according to the text?",
            "reference-answers": [
                "The first step is to clone the tree."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen74-slide75/text.txt": [
        {
            "question": "What grammatical information is provided about \"Snudny\" in the text?",
            "reference-answers": [
                "The grammatical information provided about \"Snudny\" is that it was in singular form and in the positive degree."
            ]
        },
        {
            "question": "What is known about the form of \"Snudny\" in terms of number and comparison?",
            "reference-answers": [
                "\"Snudny\" is known to be in the singular form and in the positive degree."
            ]
        },
        {
            "question": "What is the grammatical form of the translation and Snudny according to the text?",
            "reference-answers": [
                "- Translation: singular\n- Snudny: positive"
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen75-slide76/text.txt": [
        {
            "question": "**Question:** How are cases and numbers applied to adjectives in the system?",
            "reference-answers": [
                "The system applies cases and numbers to adjectives by having them inherit these from the nouns they modify. This ensures grammatical correctness through an explicit linguistic representation."
            ]
        },
        {
            "question": "What grammatical aspect do adjectives inherit from nouns?",
            "reference-answers": [
                "Adjectives inherit cases and numbers from nouns."
            ]
        },
        {
            "question": "**Question:** What do adjectives inherit from nouns in the system described?",
            "reference-answers": [
                "Adjectives inherit cases and numbers from nouns."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen76-slide77/text.txt": [
        {
            "question": "What does the teacher need to do while preparing for the exam?",
            "reference-answers": [
                "The teacher needs to add functional words and ..."
            ]
        },
        {
            "question": "What must be added to a sentence according to the text?",
            "reference-answers": [
                "functional words"
            ]
        },
        {
            "question": "What is the process described for completing the sentence?",
            "reference-answers": [
                "The process involves starting with a sentence and then adding functional words and other necessary components to complete it."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen77-slide78/text.txt": [
        {
            "question": "What was something that previous approaches could not do?",
            "reference-answers": [
                "Reordering sentences to introduce non-projectivities is something previous approaches could not do."
            ]
        },
        {
            "question": "What effect does reordering the sentence have that was not possible before?",
            "reference-answers": [
                "Reordering the sentence can introduce non-projectivities, which were not possible with previous approaches."
            ]
        },
        {
            "question": "What aspect of sentence reordering could previous approaches not achieve, as mentioned in the text?",
            "reference-answers": [
                "The aspect of sentence reordering that previous approaches couldn't achieve is the introduction of non-projectivities."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen78-slide79/text.txt": [
        {
            "question": "What is finally generated based on demorphology?",
            "reference-answers": [
                "The word forms."
            ]
        },
        {
            "question": "What does \"demorphology\" mean in the context of the TEXT?",
            "reference-answers": [
                "Demorphology refers to the process of breaking words down into their constituent morphemes to study their structure and meaning."
            ]
        },
        {
            "question": "What process involves breaking down words into their smallest units of meaning?",
            "reference-answers": [
                "The process involves breaking down words into their smallest units of meaning is called **demorphology**."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen79-slide80/text.txt": [
        {
            "question": "What does the speaker suggest about translation?",
            "reference-answers": [
                "The speaker suggests that translation should be easy."
            ]
        },
        {
            "question": "What does the text suggest about the ease of translation?",
            "reference-answers": [
                "The text suggests that the translation should be easy."
            ]
        },
        {
            "question": "What does the text suggest about the ease of translation?",
            "reference-answers": [
                "Translation should be easy."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen80-slide81/text.txt": [
        {
            "question": "What is the purpose of using the Hidden Markov Model in selecting labels for target site nodes, as described in the text?",
            "reference-answers": [
                "The purpose of using the Hidden Markov Model (HMM) is to select the best combination of labels for target site nodes by taking into account the context, specifically the surrounding nodes, to ensure the most appropriate and contextually informed lexical choices are made for each node."
            ]
        },
        {
            "question": "What is the purpose of the Hidden Markov Model in the context of selecting labels for target site nodes in the described translation task?",
            "reference-answers": [
                "The purpose of the Hidden Markov Model is to select the best combination of labels for each target site node by considering the surrounding context, ensuring optimal translations that account for grammatical and contextual nuances."
            ]
        },
        {
            "question": "What is the main goal of the Hidden Markov 3 model as described in the text, and how does it aim to achieve this goal?",
            "reference-answers": [
                "The main goal of the Hidden Markov 3 model is to find the best selection of labels for each target site node, given the source nodes, in combination. To achieve this, the model takes into account the context by considering the surrounding nodes and selects the best combination of labels, incorporating the lexical choice and the most important condensed information about how each node is used in the sentence."
            ]
        }
    ],
    "nmt-class/lecture07-syntax-in-smt/screen81-slide82/text.txt": [
        {
            "question": "What is the process used in the model to determine the optimal translation of a source sentence into the target language, considering the context and probabilities of word combinations?",
            "reference-answers": [
                "The model employs a process akin to the Hidden Markov Model (HMM) but adapted for translation. It treats the source sentence as a branching tree structure, where each node represents a word and has multiple possible translations in the target language. The model evaluates each node's translation options by considering both the translation probabilities (emission probabilities linking source and target words) and the context-dependent probabilities of word combinations in the target language (transition probabilities). This evaluation is done in a bottom-up fashion, considering all possible combinations to determine the most probable sequence of translations, resulting in the optimal translation of the source sentence into the target language."
            ]
        },
        {
            "question": "What is the main idea behind the described method for machine translation?",
            "reference-answers": [
                "The main idea is to use a method similar to Hidden Markov Models (HMMs) but applied to a tree structure in a bottom-up approach. This method considers each node in the tree as a potential translation choice, influenced by context and probabilities from both translation (emission) and language (transition) models, to select the most likely sequence of target words."
            ]
        },
        {
            "question": "The process described generalizes the Hidden Markov Model by using a tree structure instead of a linear sequence, where each node's translation choice is influenced by its context. It combines translation probabilities (emission) and language model probabilities (transition) in a bottom-up manner to find the optimal translation path.",
            "reference-answers": [
                "The process generalizes the Hidden Markov Model (HMM) by using a tree structure instead of a linear sequence, where each node's translation choice is influenced by its context. For example, the word \"Překlad\" can be translated as \"translation\" or \"arcade,\" and \"strojovi\" can be \"machine\" or \"engine,\" with probabilities determined by training data. The model combines translation probabilities (emission) and language model probabilities (transition) in a bottom-up manner to find the optimal translation path, considering all possible combinations to select the best sequence of translations."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen01-slide02/text.txt": [
        {
            "question": "What is the standard model discussed in the lecture and what other topic is being merged with it?",
            "reference-answers": [
                "The standard model discussed in the lecture is the transform model, and the other topic being merged with it is including syntax in neural machine translation."
            ]
        },
        {
            "question": "What is the standard model discussed in the lecture and what recent focus has been included in neural machine translation?",
            "reference-answers": [
                "The standard model discussed in the lecture is the transformer model, and the recent focus has been on including syntax in neural machine translation."
            ]
        },
        {
            "question": "What is the standard model discussed in the lecture on statistical machine translation, and what other topic is it being merged with?",
            "reference-answers": [
                "The standard model discussed in the lecture is the **transform model**, and it is being merged with the topic of **including syntax in neural machine translation**."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen02-slide03/text.txt": [
        {
            "question": "What is one of the most important components of the transformer architecture discussed, and what multiple ways are suggested to incorporate explicit syntax into neural machine translation?",
            "reference-answers": [
                "One of the most important components of the transformer architecture discussed is the self-attention mechanism. The multiple ways suggested to incorporate explicit syntax into neural machine translation include using syntactic information for the network structure, adding it to each individual token, and incorporating it into the attention mechanism."
            ]
        },
        {
            "question": "What factors influence whether syntactic information is useful in transformer-based neural machine translation models, and what does the text suggest about the transformer's ability to learn without explicit syntactic information?",
            "reference-answers": [
                "The factors influencing whether syntactic information is useful in transformer-based neural machine translation models include the size of the training data, the languages involved, and the quality of explicit linguistic annotations. The text suggests that transformers can learn many things without explicit syntactic information, implying their ability to handle translation effectively without relying on pre-specified linguistic rules."
            ]
        },
        {
            "question": "According to the text, what conclusion is drawn about the need for explicit syntactic information in transformer-based neural machine translation models?",
            "reference-answers": [
                "The text concludes that transformer networks can learn many aspects of syntax and structure on their own without the need for explicit syntactic information."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen03-slide04/text.txt": [
        {
            "question": "What role does the attention mechanism play in the decoder during each step of the translation process?",
            "reference-answers": [
                "The attention mechanism in the decoder during each step of the translation process determines the weight given to each input position, enabling the decoder to focus on relevant parts of the source sentence and control both the word order and the general flow of the translation."
            ]
        },
        {
            "question": "What is the role of the attention mechanism in the decoder at each decoding step when translating a sentence?",
            "reference-answers": [
                "The attention mechanism in the decoder at each decoding step assigns weights to each input position, enabling the decoder to consider all source positions while producing the output. This allows the decoder to focus on relevant parts of the input sentence, controlling the word order and the general flow of the translation."
            ]
        },
        {
            "question": "How does the attention mechanism assist the decoder in processing the input sentence during translation?",
            "reference-answers": [
                "The attention mechanism dynamically assigns weights to each input position as the decoder translates the sentence, enabling it to focus on the most relevant parts of the input during each decoding step."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen04-slide05/text.txt": [
        {
            "question": "What is the purpose of the attention energies in the described decoder mechanism, and how are they normalized?",
            "reference-answers": [
                "The attention energies determine how important each state is, and they are normalized using softmax."
            ]
        },
        {
            "question": "What information does the decoder state accumulate as the model processes the input?",
            "reference-answers": [
                "The decoder state accumulates information about what has been produced so far as the model processes the input."
            ]
        },
        {
            "question": "What is the role of attention energies in determining the importance of each state, and how are they normalized?",
            "reference-answers": [
                "The attention energies determine the importance of each state by assigning scores to them, and these scores are normalized using softmax to form a probability distribution."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen05-slide07/text.txt": [
        {
            "question": "What does the decoder do next after considering the context vector and the last word produced?",
            "reference-answers": [
                "The decoder does an output projection to the full vocabulary size of the target language."
            ]
        },
        {
            "question": "What elements does the decoder consult to determine the next word in the sequence?",
            "reference-answers": [
                "The decoder consults its previous state, the context vector, and the last word produced to determine the next word in the sequence."
            ]
        },
        {
            "question": "What does the decoder do after considering the previous state, context vector, and last word produced?",
            "reference-answers": [
                "The decoder uses the previous state, context vector, and last word produced to decide the next word."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen06-slide07/text.txt": [
        {
            "question": "What is the text moving to discuss next?",
            "reference-answers": [
                "The text is moving to discuss the transformer model next."
            ]
        },
        {
            "question": "What model are they transitioning to in the next section?",
            "reference-answers": [
                "The next section is transitioning to the Transformer model."
            ]
        },
        {
            "question": "What model are they moving to?",
            "reference-answers": [
                "The transformer model."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen07-slide08/text.txt": [
        {
            "question": "What is the central component of the Transformer model, and why is it emphasized in the paper?",
            "reference-answers": [
                "The central component of the Transformer model is attention, which is emphasized in the paper because the title \"Attention is all you need\" underscores its importance as the key innovation and central focus of the model."
            ]
        },
        {
            "question": "When was the transformer model introduced?",
            "reference-answers": [
                "2017"
            ]
        },
        {
            "question": "Why did attention become the central component of the Transformer model?",
            "reference-answers": [
                "Attention became the central component of the Transformer model because the paper emphasized it as the key innovation, and the figures summarized the equations in a way that highlighted attention's importance."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen08-slide09/text.txt": [
        {
            "question": "What blog post does the teacher recommend and why?",
            "reference-answers": [
                "The teacher recommends the blog post \"Transformer Illustrated\" because it provides a clear and simple explanation and the pictures are particularly liked. Additionally, for those interested in equations, there is an annotated paper available in PyTorch code."
            ]
        },
        {
            "question": "What blog post is recommended for its clear and simple explanation of the topic, with the author reusing its pictures?",
            "reference-answers": [
                "The recommended blog post is \"Transformer Illustrated.\""
            ]
        },
        {
            "question": "What resource does the author recommend for someone who prefers equations over visual explanations?",
            "reference-answers": [
                "The author recommends reading the paper annotated in PyTorch code for someone who prefers equations over visual explanations."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen09-slide10/text.txt": [
        {
            "question": "How many layers of encoders and decoders are there in the transformer model according to the original paper?",
            "reference-answers": [
                "The transformer model, as described in the original paper, consists of six layers of encoders and six layers of decoders."
            ]
        },
        {
            "question": "How many layers of encoders and decoders are there in the original transformer model as described in the paper?",
            "reference-answers": [
                "The original transformer model, as described in the paper, consists of six layers of encoders and six layers of decoders."
            ]
        },
        {
            "question": "**Answer:**  \nThe transformer model is designed with multiple layers of encoders and decoders, each playing a distinct role in processing and refining the input sentence. The encoders work to transform the input sentence into an intermediate representation, while the decoders progressively refine this representation to produce the final output. Specifically, the original paper suggests using six layers of encoders and six layers of decoders. Each decoder layer refines the output based on the previous layer's information, allowing for gradual improvements in predictions. Residual connections are employed to maintain the flow of information, enabling the network to skip layers if needed, which helps in training deeper networks effectively. Additionally, each decoder has access to the encoded information from the original input, allowing it to reconsider the context for more accurate predictions. The final representation of the sentence is flexible in length, accommodating varying input lengths, and this representation serves as the input for the decoder layers, which then produce the refined output through sequential processing.",
            "reference-answers": [
                "The transformer model consists of multiple layers of encoders and decoders, designed to process and refine input sentences. As per the original paper, it uses six encoder layers and six decoder layers. Encoders transform the input into intermediate representations, while decoders progressively refine these representations to produce the final output. Each decoder layer refines predictions based on the previous layer's output, with residual connections allowing the network to skip layers and maintain information flow. Decoders access the encoder's context, enabling reconsideration for accurate predictions. The final sentence representation is flexible in length, used as decoder input to produce refined outputs through sequential processing."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen10-slide11/text.txt": [
        {
            "question": "What is considered a critical component in the Transformer model's computation, as highlighted in the text?",
            "reference-answers": [
                "The self-attention is considered a critical component in the Transformer model's computation."
            ]
        },
        {
            "question": "What is the critical component in both the encoder and decoder of the transformer model, described as being essential for sensible translations?",
            "reference-answers": [
                "The critical component in both the encoder and decoder of the transformer model is the self-attention mechanism, which is essential for sensible translations."
            ]
        },
        {
            "question": "What are the two main components of the sublayers in both the encoder and decoder in a transformer model?",
            "reference-answers": [
                "The two main components of the sublayers in both the encoder and decoder in a transformer model are self-attention and a standard feedforward network."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen11-slide12/text.txt": [
        {
            "question": "What happens after processing subwords in the described model?",
            "reference-answers": [
                "After processing subwords, the model converts each word into a dense vector using an embedding layer. These vectors are then fed into self-attention, allowing mutual dependencies between words. The resulting intermediate representations, with residual connections, permit the network to move information freely, and finally, a feedforward network processes the information."
            ]
        },
        {
            "question": "What is the role of self-attention in the flow of information within the network?",
            "reference-answers": [
                "Self-attention allows every word to consider every other word, enabling mutual dependencies between words to be studied. It facilitates the flow of information from any position to any other position, making the network free to move information as needed. The correspondence to positions is a side effect and not obligatory."
            ]
        },
        {
            "question": "What is the role of the embedding layer in the described model?",
            "reference-answers": [
                "The embedding layer converts the sparse one-hot representations of the words into dense vectors, which are then fed into the self-attention mechanism."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen12-slide13/text.txt": [
        {
            "question": "What type of processing is performed token by token in encoder layers, where each token undergoes the same transformation?",
            "reference-answers": [
                "The type of processing performed token by token in encoder layers, where each token undergoes the same transformation, is feed-forward processing."
            ]
        },
        {
            "question": "What happens to the token representations after each token is processed in the first encoder layer?",
            "reference-answers": [
                "The token representations are transformed individually at each token in the first encoder layer, resulting in the same number of token representations as the input. These transformed representations are then passed to the next layer of the encoder."
            ]
        },
        {
            "question": "What happens to the number of tokens after each encoder layer transformation?",
            "reference-answers": [
                "The number of tokens remains the same after each encoder layer transformation."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen13-slide14/text.txt": [
        {
            "question": "What is the purpose of positional encoding in the transformer model, and how is it implemented?",
            "reference-answers": [
                "The purpose of positional encoding in the transformer model is to provide the network with information about the positions of tokens within a sentence. This is crucial because the order of words significantly impacts the meaning of a sentence, and without positional information, the network would treat shuffled sentences as equivalent. \n\nPositional encoding is implemented by introducing vectors that vary with each token's index. These vectors are added to the word embeddings. The construction of these vectors is inspired by the Fourier transform, where each element of the vector changes at different frequencies—fastest for the first element and progressively slower for subsequent elements. This creates a unique pattern for each position, enabling the model to distinguish the location of each token. These positional encoding vectors are added directly to the word embeddings without normalization, ensuring that even repeated words in different positions are processed correctly."
            ]
        },
        {
            "question": "What method is used to construct the positional encoding vectors in a transformer model?",
            "reference-answers": [
                "The positional encoding vectors in a transformer model are constructed using a method similar to the Fourier transform style of information processing. Each element of the vector flips at a different frequency, with the first element changing the fastest and subsequent elements changing at progressively slower rates. This creates a pattern where each position in the vector corresponds to a different sine wave, allowing the model to encode positional information effectively."
            ]
        },
        {
            "question": "What method does the paper use to construct the positional encoding vectors in a transformer model?",
            "reference-answers": [
                "The paper uses a method similar to the Fourier transform to construct the positional encoding vectors. Each element of the vector corresponds to a sine wave with a different frequency, where the first element flips at the fastest rate, and each subsequent element flips at a slower rate. This creates a unique pattern of sine waves that the model can use to understand the position of each token in the sequence."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen14-slide15/text.txt": [
        {
            "question": "What is the core element of the transformer network according to the text?",
            "reference-answers": [
                "The core element of the transformer network is the self-attention."
            ]
        },
        {
            "question": "What is the core element of the transformer network discussed in the text?",
            "reference-answers": [
                "self-attention"
            ]
        },
        {
            "question": "What is the core element of the transformer network discussed in the text?",
            "reference-answers": [
                "\"self-attention\""
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen15-slide16/text.txt": [
        {
            "question": "What is the key trade-off between RNNs and CNNs when processing sequences, and how does each model handle it?",
            "reference-answers": [
                "The key trade-off between RNNs and CNNs when processing sequences lies in how they handle the sequence and the computational requirements. RNNs process sequences step by step, reusing the same parameters across time steps, which allows them to capture dependencies over time but can lead to issues with exploding or vanishing gradients and requires processing each step sequentially. In contrast, CNNs use convolutional kernels to process multiple positions in the sequence simultaneously, allowing for fixed-size parameter reuse and faster processing but potentially missing some local information due to their fixed kernel size and stride."
            ]
        },
        {
            "question": "What limitation of recurrent neural networks led to the exploration of alternative architectures such as convolutional neural networks?",
            "reference-answers": [
                "The limitation of recurrent neural networks (RNNs) that led to the exploration of alternative architectures like convolutional neural networks (CNNs) is the linear computational cost and the problem of exploding or vanishing gradients due to the deep unrolling over many steps, which makes training difficult and processing slow for long sequences."
            ]
        },
        {
            "question": "The trade-offs between using a fixed kernel size in convolutional neural networks (CNNs) and the self-attention mechanism in transformers involve the ability to capture long-range dependencies versus computational efficiency. CNNs, with their fixed kernel size, may struggle with capturing distant relationships as their receptive field is limited by the kernel size, potentially requiring deeper networks to achieve broader context. In contrast, transformers use self-attention, allowing them to consider all parts of the sequence simultaneously, dynamically adjusting importance and efficiently handling long sequences without the need for larger kernels or deeper layers. While transformers offer flexibility and efficiency in processing entire sequences in constant time, CNNs may be more computationally efficient for certain tasks due to their localized processing. Thus, the choice between the two depends on the application's need for handling sequence length and dependencies versus computational resource constraints.",
            "reference-answers": [
                "The trade-offs between using a fixed kernel size in convolutional neural networks (CNNs) and the self-attention mechanism in transformers involve the ability to capture long-range dependencies versus computational efficiency. CNNs, with their fixed kernel size, may struggle with capturing distant relationships as their receptive field is limited by the kernel size, potentially requiring deeper networks to achieve broader context. In contrast, transformers use self-attention, allowing them to consider all parts of the sequence simultaneously, dynamically adjusting importance and efficiently handling long sequences without the need for larger kernels or deeper layers. While transformers offer flexibility and efficiency in processing entire sequences in constant time, CNNs may be more computationally efficient for certain tasks due to their localized processing. Thus, the choice between the two depends on the application's need for handling sequence length and dependencies versus computational resource constraints."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen16-slide17/text.txt": [
        {
            "question": "What is the computational complexity of self-attention networks in terms of memory and operations, and how does parallel processing affect this trade-off?",
            "reference-answers": [
                "The computational complexity of self-attention networks is O(N²) for both memory and operations, where N is the length of the sequence. Parallel processing mitigates the trade-off by enabling computations to be executed in parallel, allowing long sequences to be processed efficiently in one time step."
            ]
        },
        {
            "question": "What is the trade-off of using self-attention networks in terms of memory and computational operations, and how does their design address this challenge?",
            "reference-answers": [
                "The trade-off of using self-attention networks is that both memory and computational operations increase quadratically with the length of the sequence (O(N²)). However, their design addresses this challenge by enabling parallel computation, allowing all calculations to be performed simultaneously on a GPU with sufficient arithmetic logic units. This parallel processing capability ensures that even very long sequences can be processed in a single time step, making the approach feasible despite the high computational and memory costs."
            ]
        },
        {
            "question": "How does the number of operations in a self-attention network scale with the length of the sequence?",
            "reference-answers": [
                "The number of operations in a self-attention network scales quadratically with the length of the sequence, i.e., it is O(N²)."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen17-slide18/text.txt": [
        {
            "question": "What is the purpose of self-attention when processing an input of arbitrary length?",
            "reference-answers": [
                "The purpose of self-attention is to aggregate information from an input of arbitrary length into a fixed-size vector, allowing the network to learn and focus on the most important parts during training."
            ]
        },
        {
            "question": "What is the primary goal of self-attention when processing an input sequence?",
            "reference-answers": [
                "The primary goal of self-attention when processing an input sequence is to aggregate the information from an arbitrarily long input into a fixed-size vector, allowing the network to determine what is important and what is not."
            ]
        },
        {
            "question": "What is the primary purpose of self-attention when processing inputs of varying lengths into a fixed-size vector?",
            "reference-answers": [
                "The primary purpose of self-attention is to aggregate information from an input of arbitrary length into a fixed-size vector in a trainable manner, allowing the network to determine what is important."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen18-slide19/text.txt": [
        {
            "question": "How are keys, queries, and values generated in self-attention?",
            "reference-answers": [
                "In self-attention, keys, queries, and values are generated by projecting the input word embeddings using different projection matrices. Each of these three components (keys, queries, and values) are simply three different views of the input units, created by projecting the input embeddings through separate projection matrices. The network determines which part of the input should serve as the key, query, or value by learning the appropriate projection matrices."
            ]
        },
        {
            "question": "The three projection matrices (queries, keys, and values) are distinct because each serves a unique role in the self-attention mechanism, allowing the model to focus on different aspects of the input.",
            "reference-answers": [
                "The three projection matrices (queries, keys, and values) are distinct because each serves a unique role in the self-attention mechanism, allowing the model to focus on different aspects of the input."
            ]
        },
        {
            "question": "What role do projection matrices play in the generation of keys, queries, and values within self-attention?",
            "reference-answers": [
                "Projection matrices are used to project the input word embeddings into keys, queries, and values by applying different projection matrices for each. Each matrix transforms the input embeddings into a specific view—queries, keys, or values—allowing the network to learn how to represent the input in these ways and decide what is important for each."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen19-slide20/text.txt": [
        {
            "question": "How are queries and keys used to compare elements within a sentence?",
            "reference-answers": [
                "Queries and keys are used to compare elements within a sentence by calculating the dot product between the query vector of one position and the key vector of another position. This process involves each position consulting every other position, resulting in a comparison of every element with all others."
            ]
        },
        {
            "question": "What is the purpose of comparing the query and key vectors for each position in the mechanism described?",
            "reference-answers": [
                "The purpose of comparing the query and key vectors for each position is to assess the relationships and relevance between different elements in the sentence. By calculating the dot product between the query vector (from one position) and the key vector (from another position), the mechanism evaluates how each position interacts with others. This comparison helps identify which parts of the sentence are important (salient) and how they relate to one another, allowing the model to focus on meaningful connections between words."
            ]
        },
        {
            "question": "How does the comparison of every position with every other position result in a square structure in the described mechanism?",
            "reference-answers": [
                "The comparison of every position with every other position results in a square structure because each position is compared with all other positions, including itself. This creates a matrix where the number of rows and columns corresponds to the number of positions in the sentence, forming a square. For example, if there are five positions, each position will compare with itself and the other four positions, resulting in a 5x5 matrix."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen20-slide21/text.txt": [
        {
            "question": "What type of normalization is applied to the scores after accounting for the dimensionality of the vector representation?",
            "reference-answers": [
                "The normalization applied after accounting for the dimensionality is the softmax normalization."
            ]
        },
        {
            "question": "What normalization steps are mentioned in the text?",
            "reference-answers": [
                "The normalization steps mentioned are: normalizing the scores to account for dimensionality and applying softmax normalization."
            ]
        },
        {
            "question": "What normalization method is applied to the scores to account for the dimensionality of the vector representation before applying softmax normalization?",
            "reference-answers": [
                "The normalization method applied to the scores before softmax is done in some way to account for the dimensionality of the vector representation."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen21-slide22/text.txt": [
        {
            "question": "What does self-attention do in the example when transforming the sentence?",
            "reference-answers": [
                "Self-attention in the example focuses on the current word while considering minimal information from surrounding words, preserving most of the current word's information and aggregating the sentence's context to create a new representation for each token position."
            ]
        },
        {
            "question": "In the example, how does self-attention at position one process the information from the sentence?",
            "reference-answers": [
                "In the example, self-attention at position one heavily focuses on the current position itself (position one), preserving most of the information from that word. It also considers a small amount of information from position two. This process aggregates information from the entire sentence to create a new representation at each token position, resulting in the same number of positions after self-attention."
            ]
        },
        {
            "question": "What does self-attention do when transforming the representation of each token in a sentence, according to the text?",
            "reference-answers": [
                "Self-attention transforms the representation of each token by aggregating information from the entire sentence at every token position, creating a new representation while preserving the same number of positions."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen22-slide23/text.txt": [
        {
            "question": "What is the process for performing self-attention on input words, and how does it achieve constant time computation?",
            "reference-answers": [
                "To perform self-attention on input words, you first produce query, key, and value representations for each position. These are then matched, normalized, and used as weights to combine the values, resulting in an intermediate representation. This entire process is matrix-based, allowing for constant time computation regardless of input length."
            ]
        },
        {
            "question": "What are the main steps in the self-attention mechanism as described?",
            "reference-answers": [
                "The main steps in the self-attention mechanism involve producing query, key, and value representations for each position, matching keys and queries with normalization, applying soft max normalization, and combining the values to obtain the intermediate representation."
            ]
        },
        {
            "question": "How is the intermediate representation obtained in the self-attention mechanism?",
            "reference-answers": [
                "The intermediate representation is obtained by computing queries, keys, and values for each input position. These are then matched, normalized using softmax to get attention weights, and combined with the values to produce the intermediate representation."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen23-slide22/text.txt": [
        {
            "question": "How many attention heads are there in the description provided?",
            "reference-answers": [
                "The answer is 1."
            ]
        },
        {
            "question": "What is the role of a single attention head in the context of GPU processing?",
            "reference-answers": [
                "A single attention head processes a specific part of the input sequence in parallel on the GPU, allowing efficient computation of attention mechanisms for different aspects or features of the data."
            ]
        },
        {
            "question": "What component's attention is handled on the GPU, as mentioned in the text?",
            "reference-answers": [
                "The attention of one head is handled on the GPU."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen24-slide23/text.txt": [
        {
            "question": "In the transformer model, what term is used to describe the parallel processing steps where each node or item in a sequence considers or observes the entire sequence?",
            "reference-answers": [
                "hat"
            ]
        },
        {
            "question": "What is the term used to describe the multiple views or considerations of the input sequence in the transformer model, where each view is processed in parallel?",
            "reference-answers": [
                "hat"
            ]
        },
        {
            "question": "In the transformer model, what is the term used to describe the parallel views or considerations of the input sequence?",
            "reference-answers": [
                "The term used to describe the parallel views or considerations of the input sequence in the transformer model is **hat**."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen25-slide24/text.txt": [
        {
            "question": "What are the matrices in each head of the standard transform model used for?",
            "reference-answers": [
                "The matrices in each head of the standard transformer model are used to specify what the current token contributes to the keys, queries, and values. These matrices are initialized randomly but become specialized during training, allowing some heads to focus on specific aspects like preceding words, dependency governors, or the word itself."
            ]
        },
        {
            "question": "How are the weight matrices for keys, queries, and values in each head of the standard transform model initialized?",
            "reference-answers": [
                "The weight matrices for keys, queries, and values in each head of the standard transformer model are initialized randomly. During training, these matrices specialize, allowing each head to focus on different aspects of the input."
            ]
        },
        {
            "question": "What is the role of the matrices in each head of the standard transform model?",
            "reference-answers": [
                "The matrices in each head of the standard transform model define the keys, queries, and values. They are initially random but adapt during training, allowing each head to specialize in different features, such as context, dependencies, or the word itself, while the feedforward layer handles further transformations."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen26-slide25/text.txt": [
        {
            "question": "How are the eight different possible outputs handled after being generated in parallel?",
            "reference-answers": [
                "The eight different possible outputs are handled by concatenating them together and then projecting the concatenated result to the desired size."
            ]
        },
        {
            "question": "How are the eight times longer outputs handled after processing through the independent attention heads?",
            "reference-answers": [
                "The eight times longer outputs are handled by concatenating them and projecting them to the desired size."
            ]
        },
        {
            "question": "How are the eight outputs handled to produce the final result?",
            "reference-answers": [
                "The eight outputs are concatenated and then projected to the desired size."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen27-slide26/text.txt": [
        {
            "question": "What is the process of self-attention in a transformer model, and how is the final representation obtained?",
            "reference-answers": [
                "The process of self-attention in a transformer model begins with the input sequence of word embeddings combined with positional encodings. These embeddings are passed through multiple layers, where each layer uses the representations from the previous layer as input. In the decoder, self-attention is applied twice: first, using only the previous decoder information, and second, incorporating information from the encoder.\n\nEach self-attention head is defined by weight matrices that generate keys, queries, and values. The keys and values are compared, and the queries determine the weights applied to the values, resulting in a new representation for each head. With eight such heads, their outputs are concatenated and passed through a projection matrix to reduce the dimensionality back to the original size, producing the final Z representation used for the next layer."
            ]
        },
        {
            "question": "The final Z vector in self-attention is created by concatenating the outputs of all eight attention heads and then applying a projection matrix to reduce the dimensionality back to the original representation size.",
            "reference-answers": [
                "The final Z vector in self-attention is created by concatenating the outputs of all eight attention heads and then applying a projection matrix to reduce the dimensionality back to the original representation size."
            ]
        },
        {
            "question": "The weight matrices in each head of the self-attention mechanism are responsible for projecting the input sequence into keys, queries, and values. These matrices allow each head to independently focus on different parts of the input, enabling the computation of attention scores and the generation of a weighted sum of values. This process allows each head to capture diverse information from the input sequence.",
            "reference-answers": [
                "The weight matrices in each head of the self-attention mechanism are responsible for projecting the input sequence into keys, queries, and values. These matrices allow each head to independently focus on different parts of the input, enabling the computation of attention scores and the generation of a weighted sum of values. This process allows each head to capture diverse information from the input sequence. The resulting outputs from all eight heads are concatenated, and a final projection matrix reduces this combined output to the original dimension, providing the input for the next layer's calculation."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen28-slide27/text.txt": [
        {
            "question": "Why is masking used in decoder self-attention in a Transformer?",
            "reference-answers": [
                "Masking is used in decoder self-attention to prevent the decoder from accessing future words it hasn't produced yet. This is done by setting future positions to zeros, ensuring the decoder only attends to the information available up to the current step."
            ]
        },
        {
            "question": "How does decoder self-attention prevent looking into the future during the decoding process?",
            "reference-answers": [
                "Decoder self-attention prevents looking into the future by using masking. When the decoder generates the output one step at a time, it cannot access information from words that have not yet been produced. To achieve this, the attention mechanism sets the attention scores for all positions following the current position to zero. This ensures that the decoder only attends to the words it has already generated, preventing it from accessing future information."
            ]
        },
        {
            "question": "In which type of attention mechanism does the decoder use masking to prevent looking at future words?",
            "reference-answers": [
                "The decoder uses masking in the decoder self-attention mechanism to prevent looking at future words."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen29-slide28/text.txt": [
        {
            "question": "How did the attention mechanism in head number 1 of layer 5 identify the antecedent of the pronoun \"it\" in the example sentence?",
            "reference-answers": [
                "The attention mechanism in head number 1 of layer 5 identified the antecedent of the pronoun \"it\" by assigning higher attention weights to the positions in the sentence where the antecedent was located. This allowed the network to focus on the relevant information needed to determine the correct antecedent for the pronoun."
            ]
        },
        {
            "question": "Which specific part of the model, along with its layer and head number, is responsible for identifying the antecedent of a pronoun in the given example sentence?",
            "reference-answers": [
                "The specific part of the model responsible for identifying the antecedent of the pronoun in the example sentence is **head number 1 in layer 5 of the encoder**."
            ]
        },
        {
            "question": "What did the attention head in layer 5 learn about the antecedent of the pronoun in the given sentence?",
            "reference-answers": [
                "The attention head in layer 5 learned to assign higher attention weights to the antecedent of the pronoun \"it,\" demonstrating its ability to identify the correct reference in the sentence."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen30-slide29/text.txt": [
        {
            "question": "What do the two heads in the model focus on, and why are both perspectives important for understanding and translating sentences?",
            "reference-answers": [
                "The two heads in the model focus on different aspects of sentence structure. The first head examines the syntax of the current clause, identifying the subject and ensuring it fits with the predicate. The second head focuses on the verb parts of the clause, ensuring the subject and predicate align syntactically. Both perspectives are crucial for a comprehensive understanding of sentence meaning and accurate translation, as the first head handles noun structure and the second ensures correct verb usage."
            ]
        },
        {
            "question": "What are the two main things that the heads focus on when processing the sentence?",
            "reference-answers": [
                "The two main things the heads focus on when processing the sentence are the subject in the clause and the verb parts of the clause."
            ]
        },
        {
            "question": "What does the second head primarily focus on in the clause?",
            "reference-answers": [
                "The second head primarily focuses on the verb parts of the clause."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen31-slide30/text.txt": [
        {
            "question": "What is the purpose of pruning weights after training in neural networks?",
            "reference-answers": [
                "The purpose of pruning weights after training in neural networks is to scale down the network and save computation time by removing weights that do not contribute significantly to the final result."
            ]
        },
        {
            "question": "What is one way to optimize neural networks after training that was mentioned in the text?",
            "reference-answers": [
                "One way to optimize neural networks after training is to prune the weights, as many of them do not contribute significantly to the final result, allowing the network to be scaled down and saving computation time."
            ]
        },
        {
            "question": "What is the effect of pruning many weights on a trained neural network?",
            "reference-answers": [
                "Pruning many weights on a trained neural network allows it to be scaled down, saving computation time without significantly affecting its performance."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen32-slide31/text.txt": [
        {
            "question": "What did some of the early papers in neural machine translation include before the transformer model was introduced?",
            "reference-answers": [
                "Explicit information in neural machine translation was included in some of the early papers before the transformer model was introduced."
            ]
        },
        {
            "question": "Why was explicit information included in some of the early papers in neural machine translation before the transformer model?",
            "reference-answers": [
                "Explicit information was included in some early papers in neural machine translation before the transformer model because those papers were written before the transformer architecture was introduced."
            ]
        },
        {
            "question": "The early papers in neural machine translation were published before the transformer model and included explicit information because the transformer model hadn't been introduced yet.",
            "reference-answers": [
                "The early papers in neural machine translation were published before the transformer model and included explicit information because the transformer model hadn't been introduced yet."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen33-slide33/text.txt": [
        {
            "question": "How can linguistic information be integrated into a neural machine translation system?",
            "reference-answers": [
                "Linguistic information can be integrated into a neural machine translation system in three main ways. First, by structuring the network based on the source sentence's syntactic structure, which helps the system capture long-range dependencies that recurrent neural networks might miss. Second, by adding explicit linguistic information, such as syntax or morphology, to each token through a multitasking mechanism, where the network is trained to produce sequences of words and additional linguistic features. Third, by incorporating this linguistic information into the attention mechanism of the transformer model, enhancing its ability to focus on relevant parts of the input."
            ]
        },
        {
            "question": "What are two main ways linguistic information can be integrated into a neural machine translation system according to the text?",
            "reference-answers": [
                "Two main ways linguistic information can be integrated into a neural machine translation system are: (1) structuring the network based on the source syntax, and (2) adding explicit linguistic information to each token through a multitasking approach."
            ]
        },
        {
            "question": "What are the two main methods discussed for incorporating linguistic information into a neural machine translation system?",
            "reference-answers": [
                "The two main methods discussed for incorporating linguistic information into a neural machine translation system are:\n\n1. **Structuring the network based on source syntax**: This involves using tools like parsers and taggers to pre-process sentences and then constructing the network to mimic the syntactic structure, which helps in handling distant dependencies that RNNs might otherwise miss.\n\n2. **Adding explicit linguistic information as factors**: This method involves embedding linguistic information (like morphological or syntactic features) into each token, using a multitasking approach where the network is trained to produce both word sequences and additional information, enhancing the main translation task."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen34-slide33/text.txt": [
        {
            "question": "What steps are involved in applying the approach mentioned in the text?",
            "reference-answers": [
                "Unfortunately, the text does not provide specific details about the steps involved in applying the approach. More context or information would be needed to outline the exact steps."
            ]
        },
        {
            "question": "What is the approach mentioned in the text?",
            "reference-answers": [
                "The text does not provide specific information about the approach."
            ]
        },
        {
            "question": "What is the approach mentioned in the text?",
            "reference-answers": [
                "The approach mentioned in the text is that it exists."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen35-slide34/text.txt": [
        {
            "question": "What is the key difference between the modified three LSTMs and the standard LSTMs in handling syntactic information?",
            "reference-answers": [
                "The key difference is that the modified three LSTMs use dependency trees to consider the syntactic structure by aggregating information from all relevant nodes, whereas standard LSTMs only consider the immediate previous token, potentially missing syntactic connections."
            ]
        },
        {
            "question": "What is the primary modification made to traditional LSTMs to address the limitations of considering only the previous token in a sequence?",
            "reference-answers": [
                "The primary modification made to traditional LSTMs is using the dependency tree structure to inform the state, allowing the network to consider the syntactic dependencies between words rather than just the previous token in the sequence."
            ]
        },
        {
            "question": "What are the two main differences in how tree LSTMs handle children nodes, and what tree structures do they correspond to?",
            "reference-answers": [
                "The two main differences in how tree LSTMs handle children nodes are: (1) for dependency trees, the order of children is not important and the memories from all children are summed, allowing for any number of children; (2) for constituency trees, the structure is designed to handle a fixed number of children, often achieved through binarization. These approaches correspond to dependency trees and constituency trees, respectively."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen36-slide35/text.txt": [
        {
            "question": "What additional component does the encoder-decoder model incorporate beyond the standard sequence-to-sequence approach, and how does it mimic human syntactic structure?",
            "reference-answers": [
                "The encoder-decoder model incorporates an additional component that includes additional states traversed bottom up, allowing the model to reconsider and mimic human syntactic structure by following dependencies deemed important during this second step of deliberation."
            ]
        },
        {
            "question": "What aspect of human language does the upper part of the structure mimic?",
            "reference-answers": [
                "The upper part of the structure mimics the syntactic structure of the sentence."
            ]
        },
        {
            "question": "What additional elements does the structure add to the normal bidirectional encoder, and how do they enhance the model's capabilities?",
            "reference-answers": [
                "The structure adds additional states that are traversed bottom-up, enabling the encoder and decoder to reconsider and incorporate syntactic dependencies important to humans. This enhances the model's capabilities by making it less rigid than the three LSTMs, while retaining the effective linear backbone of the standard sequence-to-sequence approach and adding explicit knowledge through syntactic structure."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen37-slide36/text.txt": [
        {
            "question": "What refinement was introduced in the paper to improve Chinese to English translation and how does it change the network structure?",
            "reference-answers": [
                "The refinement introduced in the paper allows information to flow not only from bottom up (from words to the top of the syntactic tree) but also backwards, enabling every node to consider its predecessors and successors in the encoding. This change involves running a long thread from each position up to the whole tree and back down to the same node, allowing each node to bring information from its governors back to itself, thus improving Chinese to English translation."
            ]
        },
        {
            "question": "What refinement was specified in the paper to improve information flow in the network, and what was the main benefit of this refinement?",
            "reference-answers": [
                "The refinement specified in the paper allows information to flow bidirectionally in the network, both from bottom up and backwards. This bidirectional flow enables each node to consider both its predecessors and successors, as well as its governors in the constituency parts of the tree. The main benefit of this refinement was improved performance in Chinese to English translation."
            ]
        },
        {
            "question": "What modification was made to the network structure to enable bidirectional information flow, and how did this enhance translation?",
            "reference-answers": [
                "A modification was made to enable bidirectional information flow by allowing each node to consider both predecessors and successors, as well as governors in constituency parts, through a long thread that runs up to the entire tree and back down. This enhanced translation by improving the integration of context, particularly benefiting Chinese-to-English translation."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen38-slide37/text.txt": [
        {
            "question": "The structure that allows each node to access information from its governor and children is the dependency tree. In this structure, each node has one governor (a parent node) and can have multiple children. The computation respects this dependency structure, with one weight matrix determining how each node uses information from its governor and another weight matrix aggregating information from its children.",
            "reference-answers": [
                "The structure that allows each node to access information from its governor and children is the dependency tree. In this structure, each node has one governor (a parent node) and can have multiple children. The computation respects this dependency structure, with one weight matrix determining how each node uses information from its governor and another weight matrix aggregating information from its children."
            ]
        },
        {
            "question": "How does the computation in graph convolution networks respect the dependency tree structure, and what role do weight matrices play in this process?",
            "reference-answers": [
                "In graph convolution networks, the computation respects the dependency tree structure by allowing each node to access information from its governor (one parent) and its children (multiple dependents). The network processes each node's information based on its position in the dependency tree, ensuring that the computation flows according to the hierarchical relationships. Weight matrices play a crucial role by specifying how each node utilizes the information from its governor and children. One matrix determines the contribution of the governor's information, while the other defines how the node incorporates the information from its multiple children, enabling the network to effectively model the dependency structure."
            ]
        },
        {
            "question": "How do graph convolution networks integrate dependency parse structures into their computation, and what is the purpose of the weight matrices in this integration?",
            "reference-answers": [
                "Graph convolution networks integrate dependency parse structures into their computation by processing each word (node) in the context of its dependency tree. Each node has one governor (parent) and multiple children. The computation flows through the dependency tree, allowing each node to access information from its governor and children. Weight matrices are used to determine how each node processes the information from its governor and children, respectively. The purpose of these weight matrices is to parameterize how the network uses the dependency structure to compute the representation of each word, enabling the network to capture the structural relationships in the text."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen39-slide38/text.txt": [
        {
            "question": "What are the conceptually simpler approaches mentioned, and do they include standard sequence-to-sequence or transformer-style processing?",
            "reference-answers": [
                "The conceptually simpler approaches mentioned include standard sequence-to-sequence or transformer-style processing."
            ]
        },
        {
            "question": "What approaches were discussed as conceptually simpler and mainly use standard sequence to sequence or transformer style processing?",
            "reference-answers": [
                "Approaches that were discussed as conceptually simpler mainly use standard sequence to sequence or transformer style processing."
            ]
        },
        {
            "question": "Which approaches are described as conceptually simpler and mainly use standard sequence-to-sequence or transformer processing?",
            "reference-answers": [
                "Approaches that are described as conceptually simpler and mainly use standard sequence-to-sequence or transformer processing are discussed."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen40-slide39/text.txt": [
        {
            "question": "What is the purpose of super tags in encoding syntactic information, and how do they help in constructing sentence structures based on the ideas discussed in the text?",
            "reference-answers": [
                "The purpose of super tags is to encode syntactic information at each token, allowing neural networks to process this structured data more efficiently. By labeling tokens with these tags, which are derived from linguistic frameworks like Combinatory Category Grammar (CCG), the network can directly construct sentence structures. Super tags encapsulate valency frames or subcategorization requirements, indicating what a word needs to be complemented with (e.g., whether a verb requires a preposition). This compact representation enables the network to learn patterns, such as recognizing when a verb demands a specific complement, thereby facilitating accurate sentence construction."
            ]
        },
        {
            "question": "What is the primary purpose of using super tags in syntactic parsing, as discussed in the text?",
            "reference-answers": [
                "The primary purpose of using super tags in syntactic parsing is to encode syntactic information more explicitly and efficiently, allowing the parser to make decisions based on this structured data. Super tags bring distant syntactic dependencies closer to the words involved, enabling the network to learn and process the sentence structure more effectively."
            ]
        },
        {
            "question": "What is the method of encoding syntactic information at each token using Combinatory Category Grammars (CCGs), and how do these tags capture valency frames to aid in parsing?",
            "reference-answers": [
                "The method of encoding syntactic information at each token using Combinatory Category Grammars (CCGs) involves labeling tokens with structured super tags. These super tags are designed to reflect how each word fits into the sentence's structure. By explicitly encoding valency frames—such as whether a verb requires a preposition like \"in\"—the CCG tags capture the syntactic and semantic requirements of words. This allows the network to learn patterns, such as recognizing when a verb like \"is\" needs an \"in\" after it, by making distant dependencies more explicit and manageable. The use of these compact, structured tags enables efficient parsing by reducing the complexity of the information the network needs to process."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen41-slide39/text.txt": [
        {
            "question": "What do the slashes in the CCG text represent, and what do they specify about the words in the text?",
            "reference-answers": [
                "The slashes in the CCG text represent the valence information, specifying the types of complements each word requires to form a fully saturated element in the sentence. They indicate that a verb needs to be complemented by a noun phrase from the right-hand side and a propositional phrase from the left-hand side."
            ]
        },
        {
            "question": "What is the role of the CCG text in specifying the valence of verbs?",
            "reference-answers": [
                "The CCG text specifies the valence of verbs by indicating the required complements using slashes. For example, it shows that a verb needs a noun phrase from the right-hand side and a propositional phrase from the left-hand side to be fully saturated. This provides the valence information, detailing the necessary elements a verb requires to form a valid sentence."
            ]
        },
        {
            "question": "What do the slashes in CCG represent in terms of a word's valence requirements?",
            "reference-answers": [
                "The slashes in CCG represent the valence requirements of a word by specifying the necessary complements (such as noun phrases and propositional phrases) needed to fully saturate the word, making it a valid part of the sentence."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen42-slide39/text.txt": [
        {
            "question": "What event is mentioned at the beginning of the text?",
            "reference-answers": [
                "The text does not specify any particular event."
            ]
        },
        {
            "question": "What happened next?",
            "reference-answers": [
                "The answer cannot be generated as the text provided does not give enough information about what happened next. Please provide the complete text or more details for the question to be answered accurately."
            ]
        },
        {
            "question": "What happened next?",
            "reference-answers": [
                "There is no further information provided to determine what happened next."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen43-slide40/text.txt": [
        {
            "question": "How can syntactic information be incorporated into each token in the context of the discussion provided?",
            "reference-answers": [
                "To incorporate syntactic information into each token, we can annotate each token with its part of speech (e.g., noun, verb, adjective), its grammatical role (e.g., subject, object, predicate), its morphological features (e.g., tense, aspect, number, case), and its semantic role in the sentence. This allows for a deeper understanding of the sentence structure and facilitates tasks like parsing, translation, and information extraction by providing clear syntactic and semantic cues for each token."
            ]
        },
        {
            "question": "How can syntactic information be incorporated into each token in a system?",
            "reference-answers": [
                "To incorporate syntactic information into each token in a system, it is essential to identify the part of speech (noun, verb, adjective, etc.) for each token, include grammatical features such as tense, aspect, number, and gender, and represent the syntactic role of each token (e.g., subject, object, modifier). Additionally, dependencies between tokens should be captured to reflect their grammatical relationships. This structured representation can be achieved by tagging each token with appropriate syntactic attributes, ensuring that the system can process and understand the grammatical structure of the text."
            ]
        },
        {
            "question": "Question: How is syntactic information added to each token in the discussion?",
            "reference-answers": [
                "The syntactic information is added to each token by including the part-of-speech tag, morphological features, and dependency relations. This allows for a deeper understanding of the sentence structure."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen44-slide41/text.txt": [
        {
            "question": "**Question:**  \nWhat is the purpose of interleaving sequences in neural machine translation, and how does it benefit the translation process?",
            "reference-answers": [
                "The purpose of interleaving sequences in neural machine translation is to incorporate additional structured information, such as morphological or syntactic features, into the translation process. This is achieved by integrating these features alongside the main translation sequences, allowing the model to consider both linguistic and grammatical context. \n\nInterleaving benefits the translation process by enabling the decoder to make more informed decisions. For example, it allows the model to first decide on syntactic categories (like noun or verb) before producing the actual word, which improves accuracy. This approach also helps handle dependencies and structure more effectively, as seen in examples with CCG tags and lemmas. Overall, interleaving enhances the model's ability to produce accurate and contextually appropriate translations, especially in languages with rich morphology."
            ]
        },
        {
            "question": "What technique was discussed for incorporating morphological and syntactic information into machine translation systems, and how was it applied in specific language pairs like Czech and using CCG text?",
            "reference-answers": [
                "The technique discussed for incorporating morphological and syntactic information into machine translation systems was the use of interleaving multiple target sequences. This involved integrating additional sources of information, such as morphological tags and CCG (Combinatory Categorial Grammar) tags, into the translation process. For instance, in English-to-Czech translation, the method involved interleaving vocabulary and lemmas, which allowed the recurrent neural network to produce the correct sequence without errors. Similarly, using CCG text, the network first determined the syntactic category (e.g., noun, verb) before selecting the appropriate word, enhancing the translation accuracy. This approach was applied in English-to-German and English-to-Romanian translations, demonstrating its effectiveness in handling morphologically complex languages like Czech."
            ]
        },
        {
            "question": "What is the benefit of interleaving sequences in the decoder when considering multiple target sequences in machine translation?",
            "reference-answers": [
                "Interleaving sequences in the decoder allows the network to consider multiple target sequences simultaneously, especially when they align position-wise. This approach helps the network make syntactic decisions before lexical ones, improving translation quality. By interleaving, the decoder can handle varying sequence lengths without mismatch, as seen in examples like translating to Czech or using CCG tags, leading to better alignment and flow of morphological and syntactic information."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen45-slide42/text.txt": [
        {
            "question": "What was the purpose of the single dummy tag used in the experiments described in the text?",
            "reference-answers": [
                "The purpose of the single dummy tag was to act as a placeholder or dummy token in the experiments, likely to simulate or replace the need for detailed linguistic information, allowing the model to predict target syntax using a secondary decoder without relying on complex CCG tags."
            ]
        },
        {
            "question": "What single dummy tag was used by the teacher in their approach with CCG tags, as exemplified by \"beep\"?",
            "reference-answers": [
                "The single dummy tag used by the teacher was \"beep.\""
            ]
        },
        {
            "question": "What is the name of the paper that was published in the Prague Building of Mathematical Linguistics, and what does the term \"dummies\" refer to in the context of the paper?",
            "reference-answers": [
                "The paper is titled \"Replacing Linguists with Dummies,\" and \"dummies\" refer to the single dummy tags used in their approach."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen46-slide43/text.txt": [
        {
            "question": "What was the reason the network failed to learn anything useful when the vocabulary size was set to one for the interleaved tag?",
            "reference-answers": [
                "The network failed to learn anything useful when the vocabulary size was set to one for the interleaved tag because the training setup was poorly designed, resulting in insufficient complexity for the network to extract meaningful patterns."
            ]
        },
        {
            "question": "What did the experiments reveal about the impact of using interleaved versus correct CCG tags on the model's performance?",
            "reference-answers": [
                "The experiments showed that using interleaved tags (whether random or interleaved) resulted in better performance compared to using correct CCG tags. This is because the interleaved setup allowed the network an additional time step to deliberate about the next word, providing increased depth and improving performance."
            ]
        },
        {
            "question": "Which setup resulted in better performance: the interleaved setup with random tags or the multi-decoder setup with CCG tags?",
            "reference-answers": [
                "The interleaved setup with random tags resulted in better performance."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen47-slide44/text.txt": [
        {
            "question": "What happens when a network is given a task that is too easy in a multitask setting?",
            "reference-answers": [
                "When a network is given a task that is too easy in a multitask setting, it fails to learn the more complex or \"interesting\" task. The network optimizes for the easy part and neglects the harder task, resulting in a baseline performance that is worse than methods like CCGs or random tags, with CCGs being only negligibly better than random tags."
            ]
        },
        {
            "question": "What happens if a neural network is provided with a task that is too easy in a multitask learning scenario?",
            "reference-answers": [
                "If a neural network is provided with a task that is too easy in a multitask learning scenario, it will optimize for the easy task and fail to learn the more complex or interesting task. As a result, the network's performance on the harder task will be worse than when using other methods like CCGs or random tags, with CCGs only marginally outperforming random tags."
            ]
        },
        {
            "question": "What happens when a neural network is given a task that is too easy in a multitask learning scenario?",
            "reference-answers": [
                "When a neural network is given a task that is too easy in a multitask learning scenario, it optimizes for the easy task and fails to learn the more complex or interesting task. This results in the network not learning anything substantial, leading to a baseline performance that is worse than using CCGs or random tags, with CCGs only being negligibly better than random tags."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen48-slide45/text.txt": [
        {
            "question": "What alternative approach is mentioned for encoding syntactic neighborhood into word factors using convolutional neural networks?",
            "reference-answers": [
                "An alternative approach mentioned involves using convolutional neural networks (CNNs) to encode the syntactic neighborhood by first parsing the input sentence. The parse is then utilized for CNN derivation, which constructs compact representations for each token. These syntax-informed embeddings are combined with normal word embeddings, providing an alternative to formal tagging mechanisms like CCG super tags."
            ]
        },
        {
            "question": "What does the text suggest is the source of the gain when increasing the depth of the network?",
            "reference-answers": [
                "The text suggests that the source of the gain when increasing the depth of the network is the increased depth itself, which makes the network more powerful rather than from the linguistic information."
            ]
        },
        {
            "question": "What does the text suggest about the impact of increasing the network's depth on its power in linguistic tasks?",
            "reference-answers": [
                "The text suggests that increasing the network's depth enhances its power in linguistic tasks, as the gain comes from the increased depth rather than the linguistic information itself."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen49-slide46/text.txt": [
        {
            "question": "How was linguistic information integrated into the attention mechanism?",
            "reference-answers": [
                "Linguistic information was integrated into the attention mechanism by incorporating features such as syntax, semantics, and morphology. This allowed the model to better understand context and language-specific patterns, making the attention mechanism more effective and nuanced."
            ]
        },
        {
            "question": "Question: Explain how linguistic information is used in the context of attention.",
            "reference-answers": [
                "Linguistic information, which includes elements such as syntax, semantics, and context, plays a crucial role in attention mechanisms by helping models focus on relevant parts of the input. By incorporating these linguistic features, attention can better understand the relationships between words and sentences, enhancing tasks like translation and summarization. For instance, in translation, attention can leverage syntactic structures to produce more accurate outputs, while in summarization, semantic understanding aids in highlighting key points. Thus, linguistic information guides attention to prioritize important elements, improving overall performance in these tasks."
            ]
        },
        {
            "question": "What benefit does the text associate with using linguistic information in attention mechanisms?",
            "reference-answers": [
                "I cannot answer the question without the provided TEXT. Please share the TEXT so I can extract the relevant information to answer the question."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen50-slide47/text.txt": [
        {
            "question": "What did the paper propose to do in order to avoid repeating words in the output during English-Chinese translation?",
            "reference-answers": [
                "The paper proposed modifying the attention mechanism by restricting how attention is calculated, specifically by changing or constraining the network structure in the attention part based on the source syntax. This approach helps avoid repeating words in the output during English-Chinese translation."
            ]
        },
        {
            "question": "What method did the paper propose to modify the attention calculation in order to prevent repetitive words in the output?",
            "reference-answers": [
                "The paper proposed modifying the attention calculation by restricting or changing the network structure in the attention part based on the source syntax, which helps prevent repetitive words in the output."
            ]
        },
        {
            "question": "The paper suggests modifying attention computation by incorporating source syntax constraints to prevent word repetition in the output, particularly for English-Chinese translation.",
            "reference-answers": [
                "The paper modifies attention computation by incorporating source syntax constraints, which helps prevent word repetition in the output, particularly beneficial for English-Chinese translation."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen51-slide48/text.txt": [
        {
            "question": "How did the experiment modify the training objective to incorporate syntactic structure into the attention mechanism?",
            "reference-answers": [
                "The experiment modified the training objective by adding a constraint where one of the attention heads (specifically head number one) is required to produce a representation of the dependency structure of the source sentence. This constraint was incorporated as a penalty in the training objective, ensuring that the network not only focused on matching the target sequence (via cross-entropy loss) but also learned to capture the syntactic structure in the attention mechanism."
            ]
        },
        {
            "question": "What method was used to constrain the attention head in the transformer encoder to represent the syntactic dependency structure of the source sentence during training?",
            "reference-answers": [
                "The method used was adding a constraint to the training objective where one of the attention heads in the transformer encoder (specifically head number one) is required to represent the dependency structure of the source sentence. This constraint is implemented by penalizing the network if it fails to produce the correct parse, thereby encouraging the attention head to capture the syntactic structure."
            ]
        },
        {
            "question": "The main objective of the experiment was to enhance the transformer model's ability to capture syntactic structures by incorporating dependency representations from the attention mechanism into the training process, thereby improving both translation quality and syntactic understanding.",
            "reference-answers": [
                "The main objective of the experiment was to enhance the transformer model's ability to capture syntactic structures by incorporating dependency representations from the attention mechanism into the training process, thereby improving both translation quality and syntactic understanding."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen52-slide49/text.txt": [
        {
            "question": "At which layer does the transformer model produce the best dependency parse according to the study?",
            "reference-answers": [
                "The transformer model produces the best dependency parse at layers four or five according to the study."
            ]
        },
        {
            "question": "According to the study, how does training the transformer model to predict dependency parses affect translation quality, and which layer produces the best parses?",
            "reference-answers": [
                "Training the transformer model to predict dependency parses improves translation quality. The best parses are produced in layer four or five, while forcing the model to predict parses earlier still helps with translation but results in less accurate parses."
            ]
        },
        {
            "question": "At which layer does the transformer model produce the best dependency parses according to the study?",
            "reference-answers": [
                "The transformer model produces the best dependency parses at layers four or five according to the study."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen53-slide50/text.txt": [
        {
            "question": "What does the dummy parse represent in terms of dependency trees?",
            "reference-answers": [
                "The dummy parse represents a linear sequence where each word depends only on its immediate predecessor, forming a simple chain of dependencies."
            ]
        },
        {
            "question": "What is the structure of the linear parse used in training the transformer?",
            "reference-answers": [
                "The structure of the linear parse used in training the transformer is a simplified dependency tree where each word depends only on its predecessor, forming a linear sequence. In matrix form, this results in a diagonal structure where the first word depends on the root, and each subsequent word depends on the immediately preceding word."
            ]
        },
        {
            "question": "What is the structure of the dependency trees in the dummy parse described in the text?",
            "reference-answers": [
                "The structure of the dependency trees in the dummy parse is a linear sequence where each word depends solely on its immediate predecessor, forming a simple chain without branches. In matrix form, this is represented as a diagonal, indicating each word's dependency on the previous word."
            ]
        }
    ],
    "nmt-class/lecture08-transformer-and-syntax-in-nmt/screen54-slide51/text.txt": [
        {
            "question": "What impact does using a dummy parse have on translation quality, and why is it effective according to the text?",
            "reference-answers": [
                "Using a dummy parse improves translation quality by making the attention matrices peaked, providing explicit information about preceding words, and potentially enhancing numerical training properties, leading to better performance than using a true parse."
            ]
        },
        {
            "question": "How does using a dummy parse in the transformer model affect the attention matrices, and why does this lead to better translation quality according to the text?",
            "reference-answers": [
                "Using a dummy parse in the transformer model causes the attention matrices to be peaked. This leads to better translation quality because the model benefits from having clear, explicit information about preceding words, which is particularly helpful when this information is learned or available in the first layer. The exact linguistic explanation is unclear, but the numerical properties of the setup, such as improved stability and weight distribution, likely contribute to the enhanced performance."
            ]
        },
        {
            "question": "What role does the dummy parse play in improving translation quality according to the text?",
            "reference-answers": [
                "The dummy parse improves translation quality by encouraging the attention matrices to be peaked, which helps the transformer focus on relevant information. This effect is achieved regardless of whether the attention is directed towards the true parse or a diagonal parse, and it particularly benefits the model when such information is learned or available early in the process, likely enhancing the stability or distribution of the model's weights."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen01-slide01/text.txt": [
        {
            "question": "What is the main topic of the lecture?",
            "reference-answers": [
                "The main topic of the lecture is word and sentence representations in machine translation."
            ]
        },
        {
            "question": "**Question:** What is the main topic that the lecture will discuss?",
            "reference-answers": [
                "The main topic of the lecture is the discussion on word and sentence representations."
            ]
        },
        {
            "question": "What technical aspect will the lecture discuss regarding machine translation systems?",
            "reference-answers": [
                "The lecture will discuss the technical aspect of word and sentence representations in machine translation systems."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen02-slide02/text.txt": [
        {
            "question": "What is the technical part of the lesson going to cover?",
            "reference-answers": [
                "The technical part of the lesson will cover representations learned by the system, specifically focusing on word level and sentence level representations, as well as aspects of meaning, including what it means to mean something and expectations from sentence meaning."
            ]
        },
        {
            "question": "What two levels of representations are discussed in the text?",
            "reference-answers": [
                "The two levels of representations discussed are word level representations and sentence level representations."
            ]
        },
        {
            "question": "What are the two levels of representations discussed in the text?",
            "reference-answers": [
                "The two levels of representations discussed are word level and sentence level."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen03-slide03/text.txt": [
        {
            "question": "What is the key difference between the triangle discussed in machine translation and the WOKUA triangle?",
            "reference-answers": [
                "The key difference is that the triangle discussed in machine translation does not include an interlingua at the top, unlike the WOKUA triangle, which has the source language, target language, and interlingua at the top."
            ]
        },
        {
            "question": "Which triangle is different from the WOKUA triangle in the context of machine translation?",
            "reference-answers": [
                "The triangle in machine translation is different from the WOKUA triangle, which includes the source language, target language, and interlingua at the top."
            ]
        },
        {
            "question": "How does the triangle used in machine translation described here differ from the WOKUA triangle?",
            "reference-answers": [
                "The triangle used in machine translation described here differs from the WOKUA triangle in that it is not the same as the WOKUA triangle, which consists of the source language, target language, and interlingua at the top."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen04-slide04/text.txt": [
        {
            "question": "According to the semiotics triangle by Richards and Ogden, what is the source or lower corner of the triangle called, and what does it represent?",
            "reference-answers": [
                "The source or lower corner of the triangle is called the symbol, and it represents the message, which is something you hear or see."
            ]
        },
        {
            "question": "What does the source, or lower corner of the triangle, correspond to in the semiotics triangle by Richards and Ogden?",
            "reference-answers": [
                "The source, or lower corner of the triangle, corresponds to the symbol, to something that you hear or see, the message."
            ]
        },
        {
            "question": "What does the source or lower corner of the semiotics triangle by Richards and Ogden correspond to?",
            "reference-answers": [
                "the source or lower corner of the semiotics triangle by Richards and Ogden corresponds to the symbol, to something that you hear or see, the message."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen05-slide05/text.txt": [
        {
            "question": "What is the purpose of the symbol mentioned in the context of the sentence?",
            "reference-answers": [
                "The purpose of the symbol is to stand for some situation and correspond to some situation."
            ]
        },
        {
            "question": "What is the symbol in the sentence that stands for a situation?",
            "reference-answers": [
                "Danny approached the chair with a yellow bag."
            ]
        },
        {
            "question": "What is the symbol in the text meant to represent?",
            "reference-answers": [
                "The symbol in the text is meant to represent Danny approaching the chair with a yellow bag."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen06-slide06/text.txt": [
        {
            "question": "What are the two different situations described in the text regarding Danny and the bag in relation to the chair?",
            "reference-answers": [
                "The two different situations are: Danny is holding the bag and approaching the chair, and Danny is approaching the chair with the bag already on it."
            ]
        },
        {
            "question": "What are the two different situations that the sentence can refer to according to the text?",
            "reference-answers": [
                "The two different situations are:  \n1. Danny is holding the bag and with the bag he is approaching the chair.  \n2. Danny is approaching the chair and there is already the bag on the chair."
            ]
        },
        {
            "question": "What does the text say about the ambiguity of the sentence involving Danny, the bag, and the chair?",
            "reference-answers": [
                "The text states that the sentence is ambiguous, referring to two different situations. In one, Danny holds the bag and approaches the chair with it, while in the other, Danny approaches the chair with the bag already on it. This ambiguity arises from the meaning representation or thought behind the sentence."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen07-slide07/text.txt": [
        {
            "question": "What do syntacticists understand about encoding different meanings in constituency trees?",
            "reference-answers": [
                "Syntacticists understand how to encode different meanings in constituency trees by positioning elements, such as the yellow back, in different parts of the tree, reflecting the distinct structures for each meaning."
            ]
        },
        {
            "question": "How does the position of the yellow back differ in the constituency trees for the two different situations discussed?",
            "reference-answers": [
                "The position of the yellow back will differ in the constituency trees for the two different situations, as it will essentially be hanging elsewhere in two different positions within the tree. The exact nature of these differences would depend on the specific situations discussed in the lectures."
            ]
        },
        {
            "question": "Where would the yellow back be placed in the constituency trees for the two different meanings discussed?",
            "reference-answers": [
                "The yellow back would be placed in different positions in the constituency trees, reflecting the two different meanings discussed."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen08-slide08/text.txt": [
        {
            "question": "What are the two differing interpretations regarding the bag's connection described in the text?",
            "reference-answers": [
                "The two differing interpretations are: the bag was connected to the person because he was holding it, and the bag was connected to the chair because the bag was already sitting on the chair."
            ]
        },
        {
            "question": "How do the two systems differ in their representation of the bag's connection to the person or the chair?",
            "reference-answers": [
                "The two systems differ in how they represent the bag's connection: one connects the bag to the person because he was holding it, and the other connects the bag to the chair because the bag was already sitting on it."
            ]
        },
        {
            "question": "How do the two systems differ in linking the bag to either the person or the chair?",
            "reference-answers": [
                "The two systems differ in how they link the bag to either the person or the chair. In one system, the bag is connected to the person because he was holding it, while in the other system, the bag is connected to the chair because the bag was already sitting on the chair. This difference lies in the specific link or property that connects the bag to either the person or the chair."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen09-slide09/text.txt": [
        {
            "question": "How does processing videos enable artificial neural networks to perceive differences?",
            "reference-answers": [
                "Processing videos allows artificial neural networks to perceive differences because the representations and activations within the network change, enabling the network to detect and understand the variations."
            ]
        },
        {
            "question": "How does processing videos affect the ability of artificial neural networks to perceive differences based on their internal representations and activations?",
            "reference-answers": [
                "Processing videos affects the ability of artificial neural networks to perceive differences by altering the internal representations and activations within the network. As a result, the network has the opportunity to detect and discern differences based on these changes."
            ]
        },
        {
            "question": "How does processing videos affect the representations or activations in artificial neural networks?",
            "reference-answers": [
                "Processing videos affects the representations or activations in artificial neural networks by causing the representations and activations to be different from other cases, allowing the networks to perceive these differences."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen10-slide10/text.txt": [
        {
            "question": "What methods do human translators use to handle ambiguous situations when translating technical texts?",
            "reference-answers": [
                "Human translators handle ambiguous situations when translating technical texts by using context to disambiguate, consulting informants or colleagues for technical terms, preserving ambiguity in the target language when clarity is not possible, adding warnings to alert the reader of unclear terms, and including translator's comments or notes to explain unresolved ambiguities."
            ]
        },
        {
            "question": "How do human translators handle ambiguous or unclear situations in the source text during translation?",
            "reference-answers": [
                "Human translators handle ambiguous or unclear situations in the source text by disambiguating through context, seeking expert advice for technical terms, preserving ambiguity if resolution is impossible, adding warnings to alert readers, and including translator comments to clarify uncertainties."
            ]
        },
        {
            "question": "What strategies do human translators use when they encounter ambiguous terms in the original text that are unclear even after considering the context?",
            "reference-answers": [
                "When human translators encounter ambiguous terms that remain unclear despite considering the context, they employ several strategies. First, they may consult with informants or colleagues, especially for technical or specialized vocabulary. If the ambiguity persists, they might preserve the ambiguity in the target language, reflecting the uncertainty as closely as possible. Additionally, translators may include comments or warnings in the text to alert the reader about the ambiguity and provide their interpretation."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen11-slide11/text.txt": [
        {
            "question": "In which years and language pairs did machine translation systems surpass human translators according to the WMT competition?",
            "reference-answers": [
                "In 2018, machine translation systems surpassed human translators in English to Czech translation. In 2019, systems outperformed humans in English to German translation."
            ]
        },
        {
            "question": "What translation task in which year did a machine translation system surpass human performance in the WMT competition?",
            "reference-answers": [
                "The machine translation system surpassed human performance in the English-to-German translation task in 2019."
            ]
        },
        {
            "question": "What did the evaluation results from the WMT competitions in 2018 and 2019 indicate about the performance of machine translation systems compared to humans?",
            "reference-answers": [
                "The evaluation results from the WMT competitions in 2018 and 2019 indicated that machine translation systems performed as well as or better than humans in specific language pairs. In 2018, English-to-Czech translation showed systems surpassing professional translation at the segment level. In 2019, English-to-German translation featured a system that outperformed humans, demonstrating that machine translation can match human performance."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen12-slide12/text.txt": [
        {
            "question": "What percentage of the training data used for the English to Czech machine translation system was parallel, and how was the remaining portion utilized?",
            "reference-answers": [
                "40% of the training data was parallel, and the remaining 60% came from the two languages, utilizing an earlier version of the system to use that data."
            ]
        },
        {
            "question": "What are the two main reasons mentioned for why machine translation systems could be effective?",
            "reference-answers": [
                "The two main reasons mentioned for why machine translation systems could be effective are the extensive amount of training data they are exposed to and the meaningful internal representations created by the neural network."
            ]
        },
        {
            "question": "What percentages of the training data for the 2018 Charles University Transformer system were parallel and monolingual, respectively?",
            "reference-answers": [
                "40% of the training data was parallel, and 60% was monolingual."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen13-slide13/text.txt": [
        {
            "question": "What is the translation of the sentence \"ma, ma, mele, maso\" when translated using Google?",
            "reference-answers": [
                "The translation of the sentence \"ma, ma, mele, maso\" when translated using Google is \"ma, mis, min, sink, meat.\""
            ]
        },
        {
            "question": "What does Google Translate translate the Czech sentence \"ma, ma, mele, maso\" as?",
            "reference-answers": [
                "ma, mis, min, sink, meat"
            ]
        },
        {
            "question": "What is the translation of the Czech sentence \"ma, ma, mele, maso\" as provided in the text?",
            "reference-answers": [
                "The translation of the Czech sentence \"ma, ma, mele, maso\" is \"ma, mis, min, sink, meat.\""
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen14-slide14/text.txt": [
        {
            "question": "What issue arises when translating questions in free word order languages using neural machine translation like Google Translate?",
            "reference-answers": [
                "The issue is that neural machine translation systems, like Google Translate, do not understand the linguistic structure of free word order languages. They rely on processing words in specific orders they were trained on. When translating questions where the word order deviates from this training data, the system fails to preserve the intended meaning, resulting in nonsensical translations. This highlights the system's lack of adaptability beyond its training data, leading to significant performance decline when encountering unfamiliar structures."
            ]
        },
        {
            "question": "What issue does the text highlight about neural machine translation when handling word order deviations?",
            "reference-answers": [
                "The text highlights that neural machine translation struggles with handling deviations in word order, as the neural MT trained by Google fails to produce accurate translations when the word order is outside its training data, resulting in a significant drop in performance and a complete loss of the original meaning."
            ]
        },
        {
            "question": "What issue does the text highlight regarding Google's neural machine translation system's ability to understand context beyond its training data?",
            "reference-answers": [
                "The text highlights that Google's neural machine translation system does not understand context beyond its training data, as evidenced by its inability to correctly translate sentences with word orders outside its training, leading to nonsensical outputs."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen15-slide15/text.txt": [
        {
            "question": "Why were the translations generated by Microsoft Bing Translator incorrect according to the text?",
            "reference-answers": [
                "The translations were incorrect because Microsoft Bing Translator was trained on text that lacked sufficient data about mincing meat or related phrases, leading to missing verbs and incorrect interpretations of the input sentences."
            ]
        },
        {
            "question": "What was missing in the training data that caused the translation to lose the verb and produce incorrect sentences?",
            "reference-answers": [
                "The training data lacked sufficient context or examples related to mincing meat, causing the translation to lose the verb and produce incorrect sentences."
            ]
        },
        {
            "question": "The issue described with Microsoft Bing Translator is that it sometimes produces incorrect translations due to insufficient training data, particularly for specific phrases or languages, leading to the loss of context and verbs, as seen in the examples provided.",
            "reference-answers": [
                "The issue with Microsoft Bing Translator, as described, is that it sometimes produces incorrect translations due to insufficient training data, particularly for specific phrases or languages. This results in the loss of context and verbs, as seen in the examples provided."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen16-slide16/text.txt": [
        {
            "question": "What is the purpose of the word order in the examples provided?",
            "reference-answers": [
                "The purpose of the word order in the examples provided is to highlight the topic-focused articulation in the Czech sentence."
            ]
        },
        {
            "question": "\"Is the word order in the Czech sentence appropriate for highlighting the topic-focused articulation?\"",
            "reference-answers": [
                "Yes, the word order in the Czech sentence is appropriate for highlighting the topic-focused articulation."
            ]
        },
        {
            "question": "What was the reason for the specific word order in the Czech sentence described in the text?",
            "reference-answers": [
                "The specific word order in the Czech sentence was used to highlight the topic-focused articulation, as the user believes this is appropriate for the sentence structure."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen17-slide17/text.txt": [
        {
            "question": "What is the reason for the system's unacceptable output when asked if mom has meat?",
            "reference-answers": [
                "The system's output was unacceptable because it stepped out of the training data, leading to an unexpected response."
            ]
        },
        {
            "question": "What question did the system ask about the mom and the meat?",
            "reference-answers": [
                "Does mom have meat?"
            ]
        },
        {
            "question": "Why is the system's output considered unacceptable?",
            "reference-answers": [
                "The system's output is unacceptable because it provided an irrelevant response (\"does mom have meat?\") to the original question about reordering sentences, stepping outside the training data's scope."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen18-slide19/text.txt": [
        {
            "question": "What is the significance of a \"cut\" in a neural network and what is represented at this cut?",
            "reference-answers": [
                "A \"cut\" in a neural network is a set of neurons where every path from input to output must cross. At this cut, the network's representation of the entire input is captured, meaning the activations of these neurons encode the input, which is then used to construct the output."
            ]
        },
        {
            "question": "What must exist at every cut in a neural network according to the text?",
            "reference-answers": [
                "A representation of the whole input must exist at every cut in a neural network."
            ]
        },
        {
            "question": "What must be present at every cut of the network according to the text?",
            "reference-answers": [
                "At every cut of the network, there must be something which represents the whole input."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen19-slide20/text.txt": [
        {
            "question": "What was the purpose of using three lines in the 2D space, and how does transforming the coordinates into a new ABC space based on distances from these lines help in separating inner points from circumference points?",
            "reference-answers": [
                "The purpose of using three lines in the 2D space was to create separation boundaries that could help distinguish between points in the center and those on the circumference. Each neuron in the hidden layer represented a line, effectively dividing the original space. By transforming the original coordinates into a new ABC space based on the distances from these three lines, the points were repositioned in a way that made it easier to separate inner points from those on the circumference. This transformation increased the dimensionality of the data, allowing for a clearer distinction between the two classes in the new space."
            ]
        },
        {
            "question": "What does the transformation of coordinates into distances from three lines in the hidden layer allow the network to do, and why does this make it easier to classify points as either inner or on the circumference?",
            "reference-answers": [
                "The transformation of coordinates into distances from three lines in the hidden layer allows the network to create a higher-dimensional representation of the points. This makes it easier to classify points as either inner or on the circumference because, in the new 3D space, the points are projected onto three coordinates representing their distances from each of the three lines. This transformation effectively creates a more complex separation boundary, enabling the network to distinguish between inner and circumference points with greater ease."
            ]
        },
        {
            "question": "What is the role of the hidden layer with three neurons in transforming the original 2D space into a new coordinate system for easier separation of inner and circumference points?",
            "reference-answers": [
                "The hidden layer with three neurons transforms the original 2D space by creating three new coordinates based on the distances and sides relative to each neuron's line. This new ABC space simplifies the separation of inner and circumference points."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen20-slide21/text.txt": [
        {
            "question": "Is the new representation in the hidden space only useful for separating the classes we are trying to separate, or can it be used for something else?",
            "reference-answers": [
                "The text does not explicitly state whether the new representation in the hidden space can be used for something else beyond separating the classes being targeted. It only poses the question about the representation's potential utility beyond its specific training purpose."
            ]
        },
        {
            "question": "What is the purpose of the hidden space's representation compared to the original space in terms of separating classes and its potential use beyond the initial task?",
            "reference-answers": [
                "The hidden space's representation is a new space trained to better separate the classes compared to the original space. It may be useful beyond the initial task for other purposes."
            ]
        },
        {
            "question": "Is the new representation in the hidden space solely used for separating classes, or is it useful for something else?",
            "reference-answers": [
                "The new representation in the hidden space is designed specifically to separate classes and the text does not provide information on whether it is useful for anything else beyond that."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen21-slide22/text.txt": [
        {
            "question": "What was the purpose of providing hand-drawing examples along with the input picture and its representation in the new space?",
            "reference-answers": [
                "The purpose was to provide visual examples to help students understand and study the transformation process of input pictures into the new space, serving as a reference for exam preparation."
            ]
        },
        {
            "question": "What was the purpose of the hand-drawing examples provided by the teacher?",
            "reference-answers": [
                "The purpose of the hand-drawing examples was to provide visual illustrations that help clarify the main task, showing how an input picture is represented in a new space."
            ]
        },
        {
            "question": "What does the teacher mean by \"new space\" in the context of the provided text?",
            "reference-answers": [
                "The teacher refers to \"new space\" as a transformed or redefined framework used to represent the input picture, likely involving a different arrangement or system for visualization or analysis."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen22-slide23/text.txt": [
        {
            "question": "Question: Why is the ABC representation considered good?",
            "reference-answers": [
                "The ABC representation is considered good because it allows to separate the border from the face in the picture."
            ]
        },
        {
            "question": "Why is the ABC representation considered good?",
            "reference-answers": [
                "The ABC representation is considered good because it allows the separation of the border from the face in the picture."
            ]
        },
        {
            "question": "The ABC representation is considered good because it allows for the separation of the border from the face in the picture.",
            "reference-answers": [
                "The ABC representation is considered good because it allows for the separation of the border from the face in the picture."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen23-slide24/text.txt": [
        {
            "question": "What can happen when the network processes a very complicated input?",
            "reference-answers": [
                "the network learns to process this complicated thing into something which..."
            ]
        },
        {
            "question": "What can happen if your input is very complicated and the network learns to process it into something which...",
            "reference-answers": [
                "The TEXT provided is incomplete and does not contain enough information to generate a meaningful answer. Please provide the full TEXT so I can assist you effectively."
            ]
        },
        {
            "question": "What can occur when a complicated input is processed by the network?",
            "reference-answers": [
                "Other things can also happen if your input is something very complicated and the network learns to process this complicated thing into something which... produces errors or nonsensical results."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen24-slide25/text.txt": [
        {
            "question": "What makes a hidden representation good according to the text?",
            "reference-answers": [
                "A hidden representation is considered good if it is something we like, understand, and resembles something we are happy to see, making sense in some way."
            ]
        },
        {
            "question": "Under what condition is a hidden representation considered good according to the text?",
            "reference-answers": [
                "A hidden representation is considered good when it resembles something we like and understand."
            ]
        },
        {
            "question": "What is the criterion for a hidden representation to be considered good according to the text?",
            "reference-answers": [
                "A hidden representation is considered good if it is something we like and understand, as it resembles something we are happy to see and makes sense."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen25-slide26/text.txt": [
        {
            "question": "What are the key aspects that make a representation considered \"good\" in machine learning?",
            "reference-answers": [
                "A representation is considered \"good\" in machine learning if it effectively solves the main task the network is trained for, performs well on secondary tasks, serves as a bridge between different task interfaces, and is interpretable and meaningful."
            ]
        },
        {
            "question": "What are the key aspects of a good representation?",
            "reference-answers": [
                "A good representation should enable solving the main task the network is trained for, allow for secondary tasks, work well at task interfaces, and be interpretable and meaningful."
            ]
        },
        {
            "question": "What are the key characteristics of a good representation in the context of machine learning, as described in the text?",
            "reference-answers": [
                "1. A good representation should allow the network to solve the main task it is trained for.  \n2. It can also be useful for secondary tasks, as seen with pre-trained word embeddings that can be applied to various tasks like sentiment classification.  \n3. Good representations should work well at task interfaces, such as in speech translation where the same representation can be used for both speech recognition and translation tasks.  \n4. They should make sense in some way, meaning the representations should relate well to expected features or be interpretable through evaluations, like checking if a classifier can access features such as sentence tense from the representation."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen26-slide28/text.txt": [
        {
            "question": "What are word embeddings and how are they used in language modeling tasks?",
            "reference-answers": [
                "Word embeddings are continuous numerical representations of words that capture semantic and syntactic information in a compact form with fewer dimensions. They are trained on large datasets to learn these representations, which can then be used in various natural language processing tasks. In language modeling tasks, embeddings are typically learned from two main approaches: predicting a word from its surrounding context (e.g., in models like Word2Vec's skip-gram) or predicting the surrounding context given a word (e.g., in Word2Vec's CBOW or GloVe's global co-occurrence approach). These embeddings help in capturing meaningful properties of words that are useful for translating sentences and performing other language-related tasks."
            ]
        },
        {
            "question": "What is the primary purpose of word embeddings according to the text?",
            "reference-answers": [
                "The primary purpose of word embeddings, as described in the text, is to provide a continuous representation of words that reduces the complexity of dealing with a large and diverse vocabulary, thereby facilitating easier handling in various natural language processing tasks."
            ]
        },
        {
            "question": "What are the two most well-known word embeddings mentioned in the text, and how do they differ in their approach to word representation?",
            "reference-answers": [
                "The two most well-known word embeddings mentioned are Word2Vec and GloVe. Word2Vec predicts a word from its context, focusing on local neighboring words, while GloVe predicts likely neighbors based on global co-occurrence statistics across a corpus, considering broader context."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen27-slide29/text.txt": [
        {
            "question": "What does subtracting the embedding of \"man\" from \"king\" and adding the embedding of \"woman\" result in, according to the text?",
            "reference-answers": [
                "The result of subtracting the embedding of \"man\" from \"king\" and adding the embedding of \"woman\" is the embedding of \"queen.\""
            ]
        },
        {
            "question": "What happens when you take the embedding of the word \"king,\" subtract the embedding of the word \"man,\" and add the embedding of the word \"woman\"?",
            "reference-answers": [
                "When you take the embedding of the word \"king,\" subtract the embedding of the word \"man,\" and add the embedding of the word \"woman,\" you will get the embedding of the word \"queen.\""
            ]
        },
        {
            "question": "An example of a relation observed in the continuous space of word embeddings is the linear relationship where the embedding of \"king\" minus the embedding of \"man\" plus the embedding of \"woman\" results in the embedding of \"queen,\" demonstrating a form of gender-based analogy in the vector space.",
            "reference-answers": [
                "An example of a relation observed in the continuous space of word embeddings is the linear relationship where the embedding of \"king\" minus the embedding of \"man\" plus the embedding of \"woman\" results in the embedding of \"queen,\" demonstrating a form of gender-based analogy in the vector space."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen28-slide30/text.txt": [
        {
            "question": "What are the two types of questions in the test set associated with Word2Egg, and what do they typically include?",
            "reference-answers": [
                "The two types of questions in the test set associated with Word2Egg are semantic questions and syntactic or morphosyntactic questions. Semantic questions typically include topics such as countries and currencies, family relations, and words related to female and male family members. Syntactic or morphosyntactic questions typically include verbs with different tenses, plurals for nouns, and opposites for adjectives or adverbs."
            ]
        },
        {
            "question": "What are the two types of questions included in the test set associated with Word2Egg?",
            "reference-answers": [
                "The two types of questions are semantic and syntactic/morphosyntactic. Semantic questions involve topics like countries and currencies, family relations, and words related to female and male family members. Syntactic/morphosyntactic questions involve verbs in different tenses, plurals for nouns, and opposites for adjectives or adverbs."
            ]
        },
        {
            "question": "What are the two types of questions in the test set associated with Word2Egg, and what do they focus on?",
            "reference-answers": [
                "The two types of questions are semantic and syntactic/morphosyntactic. Semantic questions focus on meaning and context, such as countries and currencies, family relations, and male and female family members. Syntactic/morphosyntactic questions focus on the structure and forms of words, such as verb tenses, noun plurals, and opposites for adjectives or adverbs."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen29-slide31/text.txt": [
        {
            "question": "What did the evaluation of the test set reveal about its structure and the limitations in the types of questions used?",
            "reference-answers": [
                "The evaluation revealed that the test set, created by combining words from a small seed, had a limited variety of relations, primarily focusing on three semantic types and syntactic questions from 300 word pairs. This structure allowed the model to exploit patterns, potentially overfitting, and did not cover diverse relations like those tested in other studies. Thus, the test set's limitations in question types and structure affected its ability to comprehensively evaluate the model's capabilities."
            ]
        },
        {
            "question": "How was the test set constructed and what were its limitations?",
            "reference-answers": [
                "The test set was constructed by Tomáš Mikolov and included around 8,000 semantic and 10,000 syntactic or morphosyntactic questions. The semantic questions were limited to three specific types: country-city, country-currency, and masculine-feminine family member relations. The syntactic questions were based on only 300 distinct word pairs, with each question involving 35 different pairs. This construction led to limitations, including a narrow focus on specific relations and a small seed of word pairs, which allowed the network to exploit patterns and overfit. Additionally, the test set did not cover a wide range of relations, such as those explored in other studies, making it less comprehensive and potentially biased."
            ]
        },
        {
            "question": "What concern was raised regarding the test set used in the evaluation?",
            "reference-answers": [
                "The concern raised is that the test set was too narrow in the types of relations it covered, focusing only on a limited number of semantic and syntactic categories, and used a small number of word pairs, which might have allowed the network to exploit patterns or regularities, making the evaluation less representative or reliable."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen30-slide32/text.txt": [
        {
            "question": "What was the accuracy of the system on the test set?",
            "reference-answers": [
                "The accuracy of the system on the test set was 43."
            ]
        },
        {
            "question": "What was the accuracy of the model on the test set?",
            "reference-answers": [
                "43"
            ]
        },
        {
            "question": "What was the accuracy of the V2VAC model on the test set?",
            "reference-answers": [
                "43%"
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen31-slide33/text.txt": [
        {
            "question": "What was the main factor that caused the subgram model to underperform compared to the word2vec model on the original test set?",
            "reference-answers": [
                "The main factor that caused the subgram model to underperform compared to the word2vec model on the original test set was the difference in the size of the training data."
            ]
        },
        {
            "question": "On the original test set, how did the subgram model's performance compare to word2vec, and what was the key difference between the two models?",
            "reference-answers": [
                "On the original test set, the subgram model's performance was slightly lower (42) compared to word2vec. The key difference between the two models was the size of the training data."
            ]
        },
        {
            "question": "What was the main reason for the performance difference between the subgram model and the word2vec model on the original test set?",
            "reference-answers": [
                "The main reason for the performance difference was the size of the training data."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen32-slide34/text.txt": [
        {
            "question": "In the context of analyzing morphological regular patterns, how does the performance of a system based on nine specific rules compare to systems using word embeddings?",
            "reference-answers": [
                "In the context of analyzing morphological regular patterns, a system based on just nine specific rules can significantly outperform systems using word embeddings. This is demonstrated by the fact that the rule-based approach surpasses the performance of word embedding systems in both datasets mentioned. The effectiveness of the rules highlights that when the task involves understanding regular patterns or morphological behavior, explicit knowledge (in the form of these nine rules) can be more powerful than emergent representations learned through deep learning methods like word embeddings."
            ]
        },
        {
            "question": "What does the text suggest about the use of nine rules for regular patterns in achieving superior performance?",
            "reference-answers": [
                "The text suggests that using nine rules for regular patterns can achieve superior performance, particularly when the task involves analyzing morphological behavior or testing regular patterns, as this approach surpasses the performance of more complex systems using word embeddings."
            ]
        },
        {
            "question": "What does the speaker suggest about the performance of writing nine rules compared to using word embeddings?",
            "reference-answers": [
                "The speaker suggests that writing nine rules for regular patterns results in significantly better performance compared to using word embeddings, especially for tasks involving morphological analysis."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen33-slide35/text.txt": [
        {
            "question": "How many citations did the original paper, which was submitted to the conference on Text, Speech, and Dialogue in March 2016, receive?",
            "reference-answers": [
                "The original paper received seven citations."
            ]
        },
        {
            "question": "Which paper was submitted later, and which one has more citations now?",
            "reference-answers": [
                "The follow-up paper by Tomáš Mikolov was submitted later, and it has more citations now."
            ]
        },
        {
            "question": "What is the number of citations the paper on subword word embeddings received, and how does it compare to the follow-up paper by Tomáš Mikolov?",
            "reference-answers": [
                "The paper on subword word embeddings received 7 citations, while the follow-up paper by Tomáš Mikolov has 3,000 citations."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen34-slide36/text.txt": [
        {
            "question": "What is the relationship between releasing code and implementing fast code and the spread of ideas?",
            "reference-answers": [
                "Releasing code and implementing fast code can complement an idea and contribute to its spread. As seen in the example of the FastEx paper, which has 3,000 citations, providing a fast and good implementation can help an idea gain popularity and reach a wider audience. In contrast, not releasing code and not implementing fast code, as in your paper, may limit the spread of the idea."
            ]
        },
        {
            "question": "What is one difference between the two papers mentioned in the text, and how did that difference contribute to the spread of one idea over the other?",
            "reference-answers": [
                "One difference between the two papers is that the FastEx paper released the code and implemented a fast and good code, which complemented their idea and contributed to its widespread adoption, while the other paper did not release the code or implement fast code, which likely hindered the spread of its idea."
            ]
        },
        {
            "question": "What did the FastEx paper do that led to its high citation count?",
            "reference-answers": [
                "The FastEx paper achieved a high citation count by complementing its idea with fast and good code, which helped spread its influence across many publications."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen35-slide37/text.txt": [
        {
            "question": "What are the two types of similarities observed in human annotations, and which models are noted for capturing each type?",
            "reference-answers": [
                "The two types of similarities observed in human annotations are **relatedness** and **similarity**. Monolingual models, when creating word embeddings, reflect **non-specific relatedness**, while models trained on parallel data, such as neural machine translation (neural MT), capture **conceptual similarity**."
            ]
        },
        {
            "question": "What is the distinction between \"relatedness\" and \"similarity\" in the context of word embeddings, and how do monolingual models and neural machine translation (MT) models reflect these aspects differently?",
            "reference-answers": [
                "In the context of word embeddings, \"relatedness\" refers to words that appear in similar contexts, such as \"teacher\" being related to \"student\" or \"coffee\" being related to \"cup.\" On the other hand, \"similarity\" refers to words that share similar concepts, such as \"teacher\" being similar to \"professor\" or \"car\" being similar to \"train.\" \n\nMonolingual models, when creating word embeddings, tend to reflect \"relatedness,\" meaning they capture words that appear together in similar contexts. In contrast, models trained on parallel data, such as neural machine translation (MT), reflect \"similarity,\" capturing the conceptual relationship between words. This distinction is important because conflating these two notions can lead to misleading results in evaluations."
            ]
        },
        {
            "question": "What are the two distinct types of similarities discussed in the text, and which models are noted for capturing each type?",
            "reference-answers": [
                "The two distinct types of similarities discussed are relatedness (words appearing in similar contexts, captured by monolingual models) and similarity (conceptual closeness, captured by neural machine translation models)."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen36-slide38/text.txt": [
        {
            "question": "What is the next subject the teacher will discuss?",
            "reference-answers": [
                "The teacher will next discuss sentence representations."
            ]
        },
        {
            "question": "What does the text suggest as the next focus after smaller units?",
            "reference-answers": [
                "The next focus after smaller units is sentence representations."
            ]
        },
        {
            "question": "What does the text mention as the larger units to move on to?",
            "reference-answers": [
                "The larger units mentioned are sentence representations."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen37-slide39/text.txt": [
        {
            "question": "What is the role of the fixed-length vector in the encoder-decoder architecture?",
            "reference-answers": [
                "The fixed-length vector in the encoder-decoder architecture serves as a compact representation of the entire input sentence, capturing its meaning so the decoder can use it to generate the target sentence."
            ]
        },
        {
            "question": "What is the location of the cut in the network and what does it contain?",
            "reference-answers": [
                "The cut is located right before the decoder and contains the fixed-length vector representation of the entire input sentence."
            ]
        },
        {
            "question": "What is stored in the cut between the encoder and decoder that contains the entire representation of the sentence?",
            "reference-answers": [
                "The fixed-length vector is stored in the cut between the encoder and decoder, containing the entire representation of the sentence."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen38-slide40/text.txt": [
        {
            "question": "What is the primary factor influencing the placement of sentences in the two-dimensional plot?",
            "reference-answers": [
                "The primary factor influencing the placement of sentences in the two-dimensional plot is the meaning of the sentence, specifically who is receiving or giving the card, rather than the word order."
            ]
        },
        {
            "question": "What is the primary factor that determines the placement of the sentences in the plotted graph according to the text?",
            "reference-answers": [
                "The primary factor that determines the placement of the sentences in the plotted graph is the meaning of the sentence, specifically who is receiving or giving the card."
            ]
        },
        {
            "question": "What primarily determines the grouping of sentences in the plot?",
            "reference-answers": [
                "The primary factor determining the grouping of sentences in the plot is the meaning of the sentence, specifically who is receiving the card."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen39-slide41/text.txt": [
        {
            "question": "According to Raymond Moody, what is the issue with representing the meaning of a whole sentence in a single fixed-size vector?",
            "reference-answers": [
                "The issue is that it's impossible to capture the meaning of an entire sentence in a single fixed-size vector, which creates a bottleneck."
            ]
        },
        {
            "question": "Why do attention-based systems perform better than systems that use a single fixed-size vector to represent sentence meaning?",
            "reference-answers": [
                "Attention-based systems perform better because they avoid the bottleneck of using a single fixed-size vector to represent the entire sentence. Instead, they attend to all the words in the source sentence as they process it, allowing them to capture the full meaning without the limitations of a fixed-size vector."
            ]
        },
        {
            "question": "Is it possible to encapsulate the meaning of an entire sentence into a single fixed-size vector?",
            "reference-answers": [
                "No, it is not possible to encapsulate the meaning of an entire sentence into a single fixed-size vector, as Raymond Moody points out. There are problems with this approach, and attention-based systems perform better because they avoid this bottleneck by attending to all words in the source sentence."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen40-slide42/text.txt": [
        {
            "question": "What is the intended outcome of discussing aspects of meaning in this context?",
            "reference-answers": [
                "The intended outcome is to clarify expectations and ensure understanding."
            ]
        },
        {
            "question": "The aspects of meaning discussed in the text are introduced but not detailed further.",
            "reference-answers": [
                "The aspects of meaning discussed in the text are introduced but not detailed further."
            ]
        },
        {
            "question": "What is the main focus of the discussion on aspects of meaning as presented in the text?",
            "reference-answers": [
                "The main focus of the discussion on aspects of meaning as presented in the text is what is expected from the discussion."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen41-slide43/text.txt": [
        {
            "question": "What example is given to illustrate semantic segmentation in pictures?",
            "reference-answers": [
                "The example given is: \"If you have people in blue coat, then that would be those two little boys there.\""
            ]
        },
        {
            "question": "What does the author explain as the semantic representation of pictures?",
            "reference-answers": [
                "The author explains that the semantic representation of pictures involves labeling each pixel with a class label, which allows for the identification and segmentation of objects or elements within the image, such as identifying \"people in blue coat\" within a picture."
            ]
        },
        {
            "question": "What does the author describe as the process of labeling each pixel in a picture with some class label?",
            "reference-answers": [
                "The author describes the process as semantic segmentation, where each pixel in a picture is labeled with a class label to create a semantic representation."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen42-slide44/text.txt": [
        {
            "question": "Question: What aspect of a program's semantics is mentioned as being undecidable?",
            "reference-answers": [
                "termination or halting"
            ]
        },
        {
            "question": "What is the undecidable aspect of computer programs discussed in the text?",
            "reference-answers": [
                "the program will stop at all or not."
            ]
        },
        {
            "question": "What is the meaning of a sentence in the context of discussing the semantics of a program's expected output?",
            "reference-answers": [
                "a description of the program's expected output"
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen43-slide45/text.txt": [
        {
            "question": "What approach is described in the text that structures meaning from morphology to semantics, and how is meaning defined at each level?",
            "reference-answers": [
                "The approach described is the structuralist approach to linguistic meaning, based on the Textogramalical Theory by Petrus Gall. Meaning is structured from morphology up to semantics, with units at each lower level combining to form units at higher levels. At each level, meaning is defined by the relation to the surrounding layer."
            ]
        },
        {
            "question": "What is the structuralist approach to linguistic meaning, and how does it relate to the layers of linguistic description mentioned in the text?",
            "reference-answers": [
                "The structuralist approach to linguistic meaning focuses on the hierarchical structure of language, analyzing meaning through layers from morphology to semantics. Each layer defines meaning based on its relation to adjacent layers, with lower layers (like morphology) contributing to the construction of higher layers (like semantics). This approach, as seen in Petrus Gall's Textogramatical Theory, emphasizes how units at each level combine to form meaningful expressions."
            ]
        },
        {
            "question": "Question: According to the text, how does the structuralist approach define linguistic meaning, and which levels of linguistic description does it involve?",
            "reference-answers": [
                "The structuralist approach defines linguistic meaning as being defined at each level of linguistic description, from morphology over syntax up to semantics. It involves the levels of linguistic description from morphology over syntax up to semantics, as per the Textogramalical Theory by Petrus Gall."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen44-slide46/text.txt": [
        {
            "question": "Question: What aspects of meaning in sentences are better captured by traditional symbolic theories of formal semantics compared to continuous representations from neural systems, and vice versa?",
            "reference-answers": [
                "The aspects of meaning in sentences that are better captured by traditional symbolic theories of formal semantics compared to continuous representations from neural systems include abstraction, compositionality, and the handling of ambiguity. Symbolic theories are based on abstract concepts such as objects, types, and classes, which allow for clear distinctions and relationships between different elements. They also emphasize compositionality, where the meaning of a sentence can be broken down into the meanings of its parts and combined in a structured way. Additionally, symbolic theories handle ambiguity more effectively by distinguishing between different possible interpretations of expressions.\n\nOn the other hand, continuous representations from neural systems are better suited for capturing the vagueness of meaning, as they can represent the imprecision and applicability of sentences across multiple situations. Neural systems also excel in learning and adapting to new tasks, allowing meanings to emerge as a byproduct of training. Furthermore, continuous representations are well-suited for statefulness, where the meaning of a sentence can be dynamically modified over time as it is processed or interpreted in different contexts."
            ]
        },
        {
            "question": "What aspects of sentence meaning are better captured by traditional symbolic theories versus neural network-based continuous representations, and why?",
            "reference-answers": [
                "Traditional symbolic theories are better suited for capturing abstraction, such as the classification of objects into types and classes, and compositionality, which involves breaking down meanings into component parts. These theories excel in handling ambiguity through clear differences in symbolic representations, such as formulas. In contrast, neural network-based continuous representations excel at capturing the vagueness and ambiguity inherent in sentence meanings, as well as statefulness, where the meaning of a sentence can evolve dynamically over time."
            ]
        },
        {
            "question": "Which aspects of sentence meaning are better captured by traditional symbolic theories of formal semantics compared to continuous representations emerging from neural network training?",
            "reference-answers": [
                "The aspects of sentence meaning better captured by traditional symbolic theories of formal semantics compared to continuous representations include abstraction (e.g., objects, types, and classes) and compositionality. These theories excel at representing and processing these aspects, which are critical for understanding the structure and relationships in sentence meanings."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen45-slide47/text.txt": [
        {
            "question": "What is the crucial aspect for understanding complex sentences according to Chris Manning?",
            "reference-answers": [
                "The crucial aspect for understanding complex sentences according to Chris Manning is compositionality."
            ]
        },
        {
            "question": "What does Chris Manning emphasize as crucial for understanding complex sentences?",
            "reference-answers": [
                "Chris Manning emphasizes the compositionality of meaning as crucial for understanding complex sentences."
            ]
        },
        {
            "question": "What aspect is crucial for understanding complex sentences, according to Chris Manning?",
            "reference-answers": [
                "The compositionality of meaning."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen46-slide48/text.txt": [
        {
            "question": "What are the two main operations discussed by Carverne and Canavra for representing structure with vectors, and how do these operations aid in creating such representations?",
            "reference-answers": [
                "The two main operations discussed by Carverne and Canavra are multiplication and permutation of vector elements. Multiplication allows combining variable vectors by multiplying them with values, adding them together, and then recovering specific variable values by querying the vector. Permutation involves rearranging vector elements, which helps in representing structures like lists by isolating the head and tail through specific permutations, enabling the creation of hierarchical and compositional representations."
            ]
        },
        {
            "question": "What operations do Carverne and Canavra use to represent structure in vectors, and what challenges do neural networks face in learning such compositional representations?",
            "reference-answers": [
                "Carverne and Canavra use two main operations to represent structure in vectors: multiplication and permutation of vector elements. These operations allow the representation of hierarchical structures or variable assignments. The challenge for neural networks is that while they have the capacity to learn compositional structures, they do not consistently do so. Sometimes, instead of learning meaningful compositional representations, the networks memorize the input data, making it unpredictable whether they will acquire the desired compositional understanding."
            ]
        },
        {
            "question": "How do the operations of multiplication and permutation of vector elements contribute to representing compositional structures, as described in the text?",
            "reference-answers": [
                "The operations of multiplication and permutation of vector elements contribute to representing compositional structures by enabling the combination and isolation of variables. Multiplication allows for the representation of variable assignments by combining vectors, while permutation facilitates the exploration of different hierarchical structures or variations. These operations support the meaningful composition of structures, such as lists, by isolating specific elements (e.g., head and tail) through vector permutations, thus enabling the representation of compositional relationships in a continuous space."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen47-slide49/text.txt": [
        {
            "question": "According to the text, how should ambiguous expressions be represented in semantic space?",
            "reference-answers": [
                "Ambiguous expressions should be represented as distributions over points in the semantic space, rather than as single points. Each expression corresponds to a distribution that reflects multiple possible meanings, such as two peaks for \"apple\" representing both the fruit and the company."
            ]
        },
        {
            "question": "How should ambiguous words like \"apple\" ideally be represented in semantic space according to the text?",
            "reference-answers": [
                "Ambiguous words like \"apple\" should ideally be represented as distributions in the semantic space, where the distribution is peaked around the areas corresponding to their various meanings (e.g., fruits and companies), rather than as single points."
            ]
        },
        {
            "question": "How does the text suggest we should model the meanings of ambiguous words like \"apple\" in a semantic space?",
            "reference-answers": [
                "The text suggests that ambiguous words like \"apple\" should be modeled as distributions in the semantic space, where the embedding for \"apple\" is not a single point but a distribution with peaks corresponding to its different meanings, such as fruits and companies."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen48-slide50/text.txt": [
        {
            "question": "What is the advantage of the decoder's state representation compared to the encoder's stateless approach in handling tasks like processing jokes?",
            "reference-answers": [
                "The decoder's state representation captures the evolving state of mind after processing input and generating part of the output, which is crucial for tasks like processing jokes. This dynamic state allows for better handling of ambiguity and context, unlike the encoder's stateless approach which maps expressions deterministically and struggles with such tasks."
            ]
        },
        {
            "question": "What is the advantage of a stateful decoder in language processing tasks, and how does it address the challenges of ambiguity compared to a stateless approach?",
            "reference-answers": [
                "The advantage of a stateful decoder in language processing tasks is its ability to capture and maintain context, which allows it to better handle tasks that require understanding evolving meaning, such as processing jokes. This statefulness corresponds to the decoder's state representation, which reflects the context after processing input and generating output. Unlike a stateless approach, which relies on the encoder state and suffers from ambiguity issues, a stateful decoder can address these challenges by maintaining a dynamic and context-aware state that evolves as the model processes the input."
            ]
        },
        {
            "question": "What is the main advantage of using a stateful decoder in language processing tasks, as described in the text?",
            "reference-answers": [
                "The main advantage of using a stateful decoder in language processing tasks is that it captures the evolution of the state representation, which corresponds to the model's ability to track context and meaning effectively, as seen in tasks like processing jokes where ambiguity and context are crucial."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen49-slide51/text.txt": [
        {
            "question": "What experiment did Markus Dreyer and Daniel Marku conduct in 2012 involving Chinese and Arabic sentences translated into English?",
            "reference-answers": [
                "Markus Dreyer and Daniel Marku conducted an experiment in 2012 where they asked people to translate Chinese sentences and Arabic sentences into English, producing as many translations as possible, resulting in tens of thousands of possible translations."
            ]
        },
        {
            "question": "What did the experiment by Markus Dreyer and Daniel Marku reveal about the number of possible translations for an Arabic sentence into English?",
            "reference-answers": [
                "The experiment revealed that a single Arabic sentence can have dozens of thousands of possible translations into English."
            ]
        },
        {
            "question": "What does the experiment involving translating Chinese and Arabic sentences into English indicate about the continuity of linguistic meanings?",
            "reference-answers": [
                "The experiment indicates that the space of linguistic expressions is large, as a single Arabic sentence can be translated into numerous English sentences, suggesting that meanings might be continuous due to the vast number of possible expressions."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen50-slide52/text.txt": [
        {
            "question": "What was the main exploration regarding the paraphrased Czech sentences in the context of sentence embeddings?",
            "reference-answers": [
                "The main exploration was whether the paraphrased Czech sentences, despite their varied word order and structures, would occupy the same area in the sentence embedding space, or if their manifolds would become entangled and interfere with each other or with embeddings of other sentences."
            ]
        },
        {
            "question": "What is being explored regarding the relationship between the manifolds of different sentences in the context of sentence embedding spaces?",
            "reference-answers": [
                "The exploration focuses on whether paraphrased sentences, despite varying structures and word orders, will occupy the same region in sentence embedding spaces. Additionally, it examines how the manifolds of different sentences might interfere or entangle with each other in these spaces."
            ]
        },
        {
            "question": "The question is: Will the paraphrased Czech translations of the same English sentence cluster together in the sentence embedding space, or will they interfere with the embeddings of other English sentences?",
            "reference-answers": [
                "The paraphrased Czech translations of the same English sentence will cluster together in the sentence embedding space because they share the same meaning, even though their word sequences differ. They are unlikely to interfere with the embeddings of other English sentences, as each English sentence has its own distinct meaning and manifold."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen51-slide53/text.txt": [
        {
            "question": "What is the process being described for exploring the space of sentences?",
            "reference-answers": [
                "The process involves proposing directions of exploration, asking human informants to modify sentences in specific directions, having human annotators provide intermediate or extrapolated sentences, validating these relations, and then creating a partial ordered set of sentences."
            ]
        },
        {
            "question": "What method do they use to explore the continuous space of sentences and create a partial ordered set?",
            "reference-answers": [
                "They propose directions of exploration, ask human informants to modify sentences in specific directions, and use human annotators to provide sentences between examples or extrapolations. This process validates relations and creates a partial ordered set of sentences."
            ]
        },
        {
            "question": "The text suggests that the process of creating a partial ordered set of sentences involves proposing directions for exploring sentence variations, modifying sentences in these directions with human informants, having annotators insert sentences between examples or extrapolate further, and then validating the relationships to organize them into a partial ordered set.",
            "reference-answers": [
                "The process of creating a partial ordered set of sentences involves proposing directions for exploring sentence variations, modifying sentences in these directions with human informants, having annotators insert sentences between examples or extrapolate further, and then validating the relationships to organize them into a partial ordered set."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen52-slide52/text.txt": [
        {
            "question": "What was the teacher preparing for?",
            "reference-answers": [
                "The teacher was preparing for an exam."
            ]
        },
        {
            "question": "What is the teacher's objective in preparing for the exam?",
            "reference-answers": [
                "To assess students' understanding of the subject matter."
            ]
        },
        {
            "question": "Why do all the sentences have the same meaning?",
            "reference-answers": [
                "All the sentences have the same meaning because they come from the same input sentence."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen53-slide53/text.txt": [
        {
            "question": "What structure is the partial ordered set related to in the context of embeddings?",
            "reference-answers": [
                "The partial ordered set is related to the manifold of some embeddings."
            ]
        },
        {
            "question": "What structure is formed when sentences are modified and organized into a partial ordered set?",
            "reference-answers": [
                "The structure formed is a partial ordered set."
            ]
        },
        {
            "question": "What is being organized into a partial ordered set and related to the manifold of some embeddings?",
            "reference-answers": [
                "The modified sentences are being organized into a partial ordered set and related to the manifold of some embeddings."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen54-slide54/text.txt": [
        {
            "question": "What are the different types of sentence modifications discussed in the text?",
            "reference-answers": [
                "The different types of sentence modifications discussed include politeness, tense (future or past), the speaker's belief, willingness or ability, degree or intensity, and negation."
            ]
        },
        {
            "question": "What types of sentence modifications are discussed in the text?",
            "reference-answers": [
                "The types of sentence modifications discussed include politeness, tense (future or past), belief, willingness or ability, degree (e.g., a handful vs. massive crowds), negation, and complexity/length."
            ]
        },
        {
            "question": "What are the different types of sentence modifications discussed in the text?",
            "reference-answers": [
                "The different types of sentence modifications discussed include politeness, tense (future or past), the speaker's belief, willingness or ability, degree or intensity, and negation."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen55-slide55/text.txt": [
        {
            "question": "How does context help in understanding vague expressions when someone asks you to \"do the thingy there\"?",
            "reference-answers": [
                "Context helps in understanding vague expressions because it provides enough information for the listener to interpret the unclear words based on the situation or surrounding details."
            ]
        },
        {
            "question": "What does the text suggest about using vague expressions when asking someone to do a task?",
            "reference-answers": [
                "The text suggests that vague expressions can still be effective when asking someone to do a task because the context allows the listener to understand the meaning."
            ]
        },
        {
            "question": "What does the text suggest about the use of phrases like \"do the thingy there\" in communication?",
            "reference-answers": [
                "The text suggests that phrases like \"do the thingy there\" can still be understandable in communication because the context allows humans to interpret the meaning, even if the words are vague or imprecise."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen56-slide56/text.txt": [
        {
            "question": "What does the text primarily discuss in terms of language use?",
            "reference-answers": [
                "The text primarily discusses the role of politeness in language use, showing how the same meaning can be expressed with different levels of politeness."
            ]
        },
        {
            "question": "Which of the following sentences is the impolite version of \"I tried to call you; were you busy?\"?  \nA) You never answer your phone.  \nB) Did you receive my calls?  \nC) Why didn't you pick up the phone?  \nD) Have you been busy lately?",
            "reference-answers": [
                "The impolite version is A) You never answer your phone."
            ]
        },
        {
            "question": "What is the main idea the TEXT is trying to convey about the pairs of sentences it provides?",
            "reference-answers": [
                "The main idea is that the same meaning can be expressed with different levels of politeness, as shown by the pairs of sentences provided."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen57-slide57/text.txt": [
        {
            "question": "What are the two extremes used by the teacher to demonstrate the politeness and rudeness dimension when creating sentences?",
            "reference-answers": [
                "The two extremes used by the teacher are \"can you move so that I can see the screen?\" (polite) and \"you aren't made of glass, you know\" (rude)."
            ]
        },
        {
            "question": "What types of sentences did the annotators create to express the same request with varying levels of politeness?",
            "reference-answers": [
                "The annotators created sentences that ranged from informal to more polite requests. Examples include: \"Hey can you move,\" \"Please move,\" \"Could you move a little bit,\" and \"You're blocking the screen.\" These sentences reflect varying levels of politeness in expressing the same request."
            ]
        },
        {
            "question": "What examples are given by annotators for sentences that fall between the two extremes of politeness?",
            "reference-answers": [
                "The examples given by annotators for sentences that fall between the two extremes of politeness include:\n\n1. \"Hey can you move\"  \n2. \"Please move\"  \n3. \"Could you move a little bit, you're blocking the screen.\""
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen58-slide58/text.txt": [
        {
            "question": "What method is suggested for organizing different wordings according to the text?",
            "reference-answers": [
                "A third set of annotators is suggested to organize the different wordings on a scale."
            ]
        },
        {
            "question": "What method was used to organize the different wordings by the third set of annotators?",
            "reference-answers": [
                "The method used was to put them on the scale."
            ]
        },
        {
            "question": "What action was taken to organize the different wordings?",
            "reference-answers": [
                "A third set of annotators was asked to organize the different wordings by putting them on a scale."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen59-slide59/text.txt": [
        {
            "question": "Question:  \nBased on the discussion, can we consider the set of sentences as forming a partially ordered set (poset) in terms of their politeness or rudeness? Explain your reasoning.",
            "reference-answers": [
                "Yes, we can consider the set of sentences as forming a partially ordered set (poset) in terms of their politeness or rudeness. This is because some sentences can be compared in terms of their politeness or rudeness—meaning one sentence is either more or less polite/rude than another. However, there are also sentences that are incomparable, meaning their relative politeness or rudeness cannot be determined. This incomparability aligns with the definition of a poset, where not all elements need to be comparable. Therefore, the set of sentences, with the relation of politeness or rudeness, satisfies the criteria for a poset."
            ]
        },
        {
            "question": "Question: Can we identify similar relations between sentences based on their degree of politeness or rudeness, given that they form a partially ordered set?",
            "reference-answers": [
                "Yes, we can identify similar relations between sentences based on their degree of politeness or rudeness, but only for those sentences that are comparable within the partially ordered set. For sentences that are totally incomparable, such relations cannot be determined."
            ]
        },
        {
            "question": "Can we always find comparable relations between sentences in terms of their politeness or rudeness, even when dealing with a partially ordered set?",
            "reference-answers": [
                "No, we cannot always find comparable relations between sentences in terms of their politeness or rudeness because they form a partially ordered set where some sentences are totally incomparable."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen60-slide60/text.txt": [
        {
            "question": "What is the goal of unfolding the manifold in the context of learned representations?",
            "reference-answers": [
                "The goal is to have a similar structure in the highly dimensional space by unfolding the manifold."
            ]
        },
        {
            "question": "What is the goal when unfolding the manifold of learned sentence representations in a highly dimensional space?",
            "reference-answers": [
                "The goal is to have a similar structure in the highly dimensional space when unfolding the manifold of learned sentence representations."
            ]
        },
        {
            "question": "What is the purpose of unrolling the manifold in the context of learned representations?",
            "reference-answers": [
                "The purpose of unrolling the manifold is to ensure that the learned representations maintain a similar structure in the higher-dimensional space, preserving their useful properties."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen61-slide61/text.txt": [
        {
            "question": "What is the main question being discussed regarding the representation of the partial ordered set of sentences in the semantic space?",
            "reference-answers": [
                "The main question being discussed is which continuous representation will have a better match with the manually collected data for the partial ordered set of sentences in the semantic space."
            ]
        },
        {
            "question": "What structure is described as a semantic representation in the given context?",
            "reference-answers": [
                "The structure described as a semantic representation is a partial ordered set of sentences, also known as a poset."
            ]
        },
        {
            "question": "What is being compared in the text to find the best match with manual data in semantic representations?",
            "reference-answers": [
                "Different continuous representations are being compared to find the best match with manually collected data in semantic representations."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen62-slide62/text.txt": [
        {
            "question": "What are the main factors that influence the evaluation of sentence representations?",
            "reference-answers": [
                "The main factors that influence the evaluation of sentence representations include semantic meaning, contextual understanding, syntactic structure, and the ability to perform downstream tasks such as analogy detection and inference. Additionally, human evaluation plays a crucial role in assessing the naturalness and coherence of representations, as automated metrics alone may not capture these aspects fully."
            ]
        },
        {
            "question": "What is the initial focus of the text?",
            "reference-answers": [
                "The initial focus of the text is evaluating sentence representation."
            ]
        },
        {
            "question": "What is the main reason for comparing sentence representations when evaluating them?",
            "reference-answers": [
                "To evaluate sentence representations, comparing them helps assess the model's ability to capture meaning, ensuring similar sentences have similar representations, which is crucial for tasks like classification and similarity detection. This comparison also aids in understanding the model's generalization and pattern recognition capabilities, aligning its outputs with expected human understanding."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen63-slide63/text.txt": [
        {
            "question": "What is SenteVal primarily used for?",
            "reference-answers": [
                "SenteVal is primarily used for evaluating and comparing sentence representations by scoring them on various tasks, helping researchers understand the properties and capabilities of these models."
            ]
        },
        {
            "question": "What is SenteVal used for in the context of sentence representations?",
            "reference-answers": [
                "SenteVal is used to evaluate and compare sentence representations by scoring their vector representations across various tasks, helping to understand the properties and capabilities of different representations."
            ]
        },
        {
            "question": "What is the purpose of the Blackbox NLP workshop mentioned in the text?",
            "reference-answers": [
                "The purpose of the Blackbox NLP workshop is to explore and understand what transformer models know and don't know, focusing on the properties of their representations and their capabilities and limitations in NLP research."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen64-slide64/text.txt": [
        {
            "question": "What was the primary goal of the experiment described in the text, and how did it relate semantic representations to translation quality?",
            "reference-answers": [
                "The primary goal of the experiment was to determine how much semantic information was captured by the attention model and sequence-to-sequence architecture. It related semantic representations to translation quality by comparing the equality of translation, measured through BLEU scores, to various levels or measures of semantics."
            ]
        },
        {
            "question": "What was the main goal of the 2018 experiment discussed in the text?",
            "reference-answers": [
                "The main goal of the 2018 experiment was to identify how much semantic information was captured by the attention model and sequence-to-sequence architecture, and to relate this to translation quality as measured by BLEU scores."
            ]
        },
        {
            "question": "What was the primary goal of the experiment described in the text?",
            "reference-answers": [
                "The primary goal was to determine how much semantic information the attention model's representations contribute to translation quality, as measured by BLEU scores."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen65-slide65/text.txt": [
        {
            "question": "What is the purpose of using Senteval when evaluating neural network representations of movie reviews?",
            "reference-answers": [
                "The purpose of using Senteval is to evaluate how well neural network representations of movie reviews capture relevant features by assessing its ability to predict the correct score. If Senteval frequently predicts the correct score, it indicates that the neural network's representations are effective."
            ]
        },
        {
            "question": "Question: What is the purpose of the Senteval tool when evaluating neural network representations of movie reviews?",
            "reference-answers": [
                "The purpose of the Senteval tool is to evaluate the quality of neural network representations of movie reviews by assessing how well it can predict the correct score based on these representations. If the representations contain relevant features, Senteval will predict the score accurately, indicating a good representation."
            ]
        },
        {
            "question": "What is the purpose of using the Senteval tool in evaluating the hidden representations of sentences derived from movie reviews?",
            "reference-answers": [
                "The purpose of using the Senteval tool is to evaluate the hidden representations of sentences derived from movie reviews by assessing how often it can correctly predict the score, indicating the quality of the representations."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen66-slide66/text.txt": [
        {
            "question": "What would happen to the sentence representation method if it does not effectively highlight important things in the reference source?",
            "reference-answers": [
                "The sentence representation method would score badly in the classification task."
            ]
        },
        {
            "question": "What would be the consequence of a representation that does not effectively highlight important aspects in the reference source for a classification task?",
            "reference-answers": [
                "The consequence would be a low score in the classification task, and the sentence representation method would score badly."
            ]
        },
        {
            "question": "What are the consequences of a sentence representation that fails to emphasize key elements from the source text?",
            "reference-answers": [
                "The consequences are that the representation will not serve well in the classification task, leading to a low score in the classification task and the sentence representation method scoring badly."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen67-slide67/text.txt": [
        {
            "question": "What are the classification tasks mentioned in the text?",
            "reference-answers": [
                "The classification tasks mentioned are movie sentiment analysis, product review polarity, and question type identification."
            ]
        },
        {
            "question": "What are some of the classification tasks that these individual sentence representations are used for?",
            "reference-answers": [
                "- Movie sentiment analysis  \n- Product review polarity  \n- Assessing if someone is assessing a product  \n- Question type classification (e.g., asking for a number, location, translation)"
            ]
        },
        {
            "question": "What classification tasks are mentioned as being carried out using individual sentence representations?",
            "reference-answers": [
                "The classification tasks mentioned are movie sentiment analysis, product review polarity, question type (e.g., whether a question asks for a number or location), and translating between numbers and words or identifying locations."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen68-slide68/text.txt": [
        {
            "question": "What was the result when the system classified the sentences \"The couple is at a restaurant\" and \"a cute couple at a pub\" in terms of their relationship?",
            "reference-answers": [
                "The system classified the sentences as neutral, but they should have been entailed."
            ]
        },
        {
            "question": "What is a possible example of an entailment relationship in natural language inference tasks, as described in the text?",
            "reference-answers": [
                "A possible example of an entailment relationship is when one sentence implies the other. For example, the sentence \"a square full of people and life\" entails \"the square is busy.\""
            ]
        },
        {
            "question": "Question: Can you provide an example of each relationship (entailment, neutral, contradiction) between two sentences based on the given information?",
            "reference-answers": [
                "- Entailment: \"A square full of people and life\" and \"The square is busy.\"  \n- Neutral: \"The couple is at a restaurant\" and \"a cute couple at a pub.\"  \n- Contradiction: \"It is raining outside\" and \"The sun is shining brightly.\""
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen69-slide69/text.txt": [
        {
            "question": "What method is used to evaluate sentence embeddings when there is no training data available?",
            "reference-answers": [
                "When there is no training data available, the method used to evaluate sentence embeddings is to measure the similarity between the embeddings and predict the similarity based on that similarity."
            ]
        },
        {
            "question": "Question: How can you evaluate sentence similarity using sentence embeddings according to the text?",
            "reference-answers": [
                "You can evaluate sentence similarity using sentence embeddings by either training a regression model on the training data if available, or by measuring the similarity between the embeddings directly if there is no training data."
            ]
        },
        {
            "question": "What are the two main methods for evaluating sentence similarity as described in the text?",
            "reference-answers": [
                "The two main methods are evaluating similarity through human informants providing similarity scores and using sentence embeddings, either by training regression on training data or measuring the similarity between embeddings directly."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen70-slide70/text.txt": [
        {
            "question": "How are the translations used as paraphrases in the evaluation?",
            "reference-answers": [
                "The translations are used as paraphrased versions of the Chinese sentence in the evaluation."
            ]
        },
        {
            "question": "How are the many translations of the Chinese sentence used in the evaluation mentioned?",
            "reference-answers": [
                "The many translations of the Chinese sentence are used as paraphrases in the evaluation."
            ]
        },
        {
            "question": "What is the paraphrase style of evaluation based on?",
            "reference-answers": [
                "The paraphrase style of evaluation is based on the Chinese sentence and its many translations, which are considered paraphrases of the same sentence."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen71-slide71/text.txt": [
        {
            "question": "**Question:** What does the Coco dataset consist of?",
            "reference-answers": [
                "The Coco dataset consists of images, each accompanied by five captions created by people."
            ]
        },
        {
            "question": "What is the structure of the Coco dataset?",
            "reference-answers": [
                "The Coco dataset consists of images, each accompanied by five captions created by individuals. These captions describe the same image but use different expressions to convey the meaning."
            ]
        },
        {
            "question": "How many captions are associated with each image in the Coco dataset?",
            "reference-answers": [
                "Each image in the Coco dataset is associated with five captions."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen72-slide72/text.txt": [
        {
            "question": "What constitutes the members of each cluster mentioned in the text?",
            "reference-answers": [
                "The members of each cluster are the individual sentences, either the five descriptions of each picture or the 500 translations of the single Chinese sentence."
            ]
        },
        {
            "question": "What do the clusters correspond to, and how are their members structured?",
            "reference-answers": [
                "The clusters correspond to one of the meanings, either the original Chinese sentence or the picture. The members of each cluster are individual sentences: five descriptions of each picture or 500 translations of the single sentence."
            ]
        },
        {
            "question": "What is the typical number of sentences in a cluster that describes a picture?",
            "reference-answers": [
                "The typical number of sentences in a cluster that describes a picture is five."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen73-slide73/text.txt": [
        {
            "question": "What is the main purpose of the Davis Bulldin Index according to the text?",
            "reference-answers": [
                "The main purpose of the Davis Bulldin Index is to check how well a cluster is separated from other clusters, ensuring that individual sentences or pictures are clearly separated and far away from each other."
            ]
        },
        {
            "question": "What does the Davis Bulldin Index measure in the context of cluster analysis?",
            "reference-answers": [
                "The Davis Bulldin Index measures how well clusters are separated from each other in cluster analysis. It assesses the distinctness of clusters, aiming for clear separation where individual sentences or pictures are far apart."
            ]
        },
        {
            "question": "What does the Davis Bulldin Index measure in the context of clustering?",
            "reference-answers": [
                "The Davis Bulldin Index measures how well-separated the clusters are from each other. It is desirable for the clusters to be far apart and clearly separated."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen74-slide74/text.txt": [
        {
            "question": "What method is used to evaluate the goodness of clustering by checking whether the nearest neighbor of each element is in the same cluster?",
            "reference-answers": [
                "The method used to evaluate the goodness of clustering by checking whether the nearest neighbor of each element is in the same cluster is called the \"nearest neighbor clustering evaluation.\" This method assesses the clustering quality by ensuring that each element's nearest neighbor is within the same cluster, indicating compact and well-defined clusters."
            ]
        },
        {
            "question": "To evaluate the goodness of clustering, one method involves checking if each element's nearest neighbor resides within the same cluster. This process involves two main steps: first, identifying the nearest neighbor for each element, and second, verifying whether this neighbor is part of the same cluster. If this condition holds true for most or all elements, it suggests that the clusters are compact and well-structured, indicating high-quality clustering. This approach helps assess the internal cohesion of clusters, ensuring that data points are closely related within their respective groups.",
            "reference-answers": [
                "To evaluate the goodness of clustering, one method involves checking if each element's nearest neighbor resides within the same cluster. This process involves two main steps: first, identifying the nearest neighbor for each element, and second, verifying whether this neighbor is part of the same cluster. If this condition holds true for most or all elements, it suggests that the clusters are compact and well-structured, indicating high-quality clustering. This approach helps assess the internal cohesion of clusters, ensuring that data points are closely related within their respective groups."
            ]
        },
        {
            "question": "To evaluate the goodness of clustering, we assess whether each data point's nearest neighbor is within the same cluster. If most nearest neighbors are in the same cluster, the clustering is compact and well-defined, indicating high quality. Conversely, if nearest neighbors often lie in different clusters, it suggests potential issues with cluster separation or cohesion.",
            "reference-answers": [
                "To evaluate the goodness of clustering, we assess whether each data point's nearest neighbor is within the same cluster. If most nearest neighbors are in the same cluster, the clustering is compact and well-defined, indicating high quality. Conversely, if nearest neighbors often lie in different clusters, it suggests potential issues with cluster separation or cohesion."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen75-slide75/text.txt": [
        {
            "question": "Question:  \nWhat method is described for evaluating the classifier in the text?",
            "reference-answers": [
                "The method described for evaluating the classifier is using a test set created by removing one or more points from the original data, training the classifier on the remaining points, and then checking if the classifier correctly predicts the classes of the removed test points."
            ]
        },
        {
            "question": "Another method for evaluating a classifier involves removing specific data points from the dataset, using the remaining data to train the classifier, and then testing the classifier's performance on the removed points to assess its accuracy.",
            "reference-answers": [
                "Another method for evaluating a classifier involves removing specific data points from the dataset, using the remaining data to train the classifier, and then testing the classifier's performance on the removed points to assess its accuracy."
            ]
        },
        {
            "question": "What method is mentioned for evaluating a classification task in the text?",
            "reference-answers": [
                "The method mentioned is splitting the data into training and test sets, training the classifier on the training set, and evaluating it on the test set."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen76-slide76/text.txt": [
        {
            "question": "What is the main issue with using traditional methods like concatenation for sentence representation in models with a bidirectional encoder and attention mechanism?",
            "reference-answers": [
                "The main issue is that traditional methods like concatenation cannot be used because the model generates different vectors at each decoding step, and these vectors vary in length, making it impossible to create a consistent sentence representation."
            ]
        },
        {
            "question": "How does the use of attention mechanisms in sequence-to-sequence models with a bidirectional encoder impact the ability to generate a single sentence embedding for semantic processing tasks?",
            "reference-answers": [
                "The attention mechanisms in sequence-to-sequence models with a bidirectional encoder generate different vectors at each step of the decoding process, making it difficult to produce a single sentence embedding. Additionally, the varying lengths of sentences prevent the simple concatenation of these vectors. As a result, methods like Centeval cannot be used to create a single, unified sentence representation for semantic processing tasks."
            ]
        },
        {
            "question": "Why is concatenation of vectors from each decoding step not feasible in a sequence-to-sequence model with bidirectional encoders and attention mechanisms?",
            "reference-answers": [
                "Because the sentence length varies, concatenating vectors from each decoding step would result in inconsistent vector lengths, making it unsuitable for tasks that require a uniform sentence representation."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen77-slide77/text.txt": [
        {
            "question": "What is one option mentioned for creating fixed-size representations while preserving attachments in the model?",
            "reference-answers": [
                "One option mentioned is to not use attention."
            ]
        },
        {
            "question": "How can we create fixed-size representations and preserve attachments in the model without using attention?",
            "reference-answers": [
                "One option is to not use attention. Instead, you can use other mechanisms like fixed positional encodings or simple feedforward layers to create fixed-size representations and preserve information in the model."
            ]
        },
        {
            "question": "What is one option for creating fixed-size representations while preserving attachments in the model?",
            "reference-answers": [
                "One option is to not use attention."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen78-slide78/text.txt": [
        {
            "question": "What aspect of encoding directions was used in the early practice of NullMT?",
            "reference-answers": [
                "The early practice of NullMT used the final state of each of these directions of encoding."
            ]
        },
        {
            "question": "What did the early practice of NullMT involve regarding encoding directions?",
            "reference-answers": [
                "The early practice of NullMT involved using just the final state of each of these directions of encoding."
            ]
        },
        {
            "question": "Question: What was the early practice of NullMT according to the text?",
            "reference-answers": [
                "The early practice of NullMT was to use just the final state of each direction of encoding."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen79-slide79/text.txt": [
        {
            "question": "What technique is described to ensure that the resulting vector for the sentence representation is always of the same size regardless of the sentence length?",
            "reference-answers": [
                "The technique described is using max pooling or average pooling over all the states. This ensures that the resulting vector for the sentence representation is always of the same size regardless of the sentence length."
            ]
        },
        {
            "question": "What is the purpose of applying max pooling or average pooling over all the states in the context of sentence representation?",
            "reference-answers": [
                "The purpose of applying max pooling or average pooling over all the states is to ensure that the resulting vector for the sentence representation is always of the same size, regardless of the sentence length. This is achieved by summing over all the vectors, which allows the model to handle sentences of varying lengths uniformly."
            ]
        },
        {
            "question": "What technique is used to ensure that the resulting vector for sentence representation is always of the same size regardless of the sentence length?",
            "reference-answers": [
                "The technique used is max pooling or average pooling over all the states. This ensures that the resulting vector for the sentence representation is always of the same size regardless of the sentence length."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen80-slide80/text.txt": [
        {
            "question": "How is inner attention used in the transformer architecture, and what is the significance of using a fixed number of attention views before the decoder starts processing?",
            "reference-answers": [
                "Inner attention in the transformer architecture is used to enable the system to examine various parts of the input sentence by employing multiple attention heads. Each head generates a distinct view of the sentence, and the number of these views is fixed before the decoder begins processing. This static approach ensures that the system has predefined perspectives on the input, which the decoder can then utilize efficiently to process the data."
            ]
        },
        {
            "question": "What is the role of multiple attention heads in a transformer model, and how are they utilized in a static manner before the decoder starts processing the input?",
            "reference-answers": [
                "Multiple attention heads in a transformer model allow the system to look at various parts of the input sentence, providing flexibility. They create different static views of the sentence before the decoder starts processing. The number of these views is fixed upfront, and each view is calculated separately using its own matrices."
            ]
        },
        {
            "question": "What is the purpose of using multiple attention heads in the transformer architecture?",
            "reference-answers": [
                "The purpose of using multiple attention heads in the transformer architecture is to provide flexibility by allowing the system to examine various parts of the input sentence. Each head creates a different view of the sentence, enabling the model to capture diverse aspects of the input through fixed, predetermined perspectives."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen81-slide81/text.txt": [
        {
            "question": "How does the decoder access the attentional representations from the encoder?",
            "reference-answers": [
                "The decoder accesses the attentional representations from the encoder by either using the concatenated vector of the encoder's attentional context as part of its input or by employing compound attention, allowing it to dynamically attend to different parts of the sentence based on the encoder's attention heads."
            ]
        },
        {
            "question": "What does the decoder have access to in order to attend to various parts of the sentence?",
            "reference-answers": [
                "The decoder has access to the attention heads from the encoder, which allows it to attend to various parts of the sentence."
            ]
        },
        {
            "question": "What capability does the decoder gain by attending to the encoder's attention summaries?",
            "reference-answers": [
                "The decoder gains the capability to flexibly attend to various parts of the sentence through compound attention, using a single vector to represent the sentence meaning."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen82-slide82/text.txt": [
        {
            "question": "What is the key assumption about the output sentence when using fixed-length heads for attention in a transformer-style model?",
            "reference-answers": [
                "The key assumption is that the output sentence has a fixed number of words, such as four or eight words."
            ]
        },
        {
            "question": "What assumption does the text make about the output sentence in the context of using a transformer-style encoder and decoder?",
            "reference-answers": [
                "The text assumes that the output sentence has a fixed number of words, such as four or eight words, when using a transformer-style encoder and decoder."
            ]
        },
        {
            "question": "What assumption do transformer-style models make about the output sentence length, and what do they refer to as \"heads\" in this context?",
            "reference-answers": [
                "Transformer-style models assume that the output sentence has a fixed number of words, such as four or eight. In this context, \"heads\" refer to the separate attention heads that the model uses to process the input."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen83-slide83/text.txt": [
        {
            "question": "What dataset was used for English-to-German translation, and how does it differ from the dataset used for English-to-Czech translation in terms of size and domain diversity?",
            "reference-answers": [
                "The dataset used for English-to-German translation was the Multi-30k dataset, which focuses on image captions. This dataset is smaller and more specialized in domain diversity compared to the larger and more diverse dataset used for English-to-Czech translation."
            ]
        },
        {
            "question": "What is one difference in the datasets used for translating English to Czech and English to German?",
            "reference-answers": [
                "The dataset for English to Czech was larger and more diverse, while the English to German translation used only the Multi-30k dataset, which is based on image captions."
            ]
        },
        {
            "question": "What were the differences in the datasets used for training the English-to-Czech and English-to-German translation systems, and how did these differences impact the results?",
            "reference-answers": [
                "The English-to-Czech translation system was trained on a large corpus with diverse domains, while the English-to-German system used only the Multi-30k dataset, which focuses on image captions. These differences impacted the results, as the larger, more diverse dataset likely contributed to better performance for Czech, whereas the limited domain of image captions may have affected the German system's ability to handle broader topics."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen84-slide84/text.txt": [
        {
            "question": "Which model achieved the best performance in terms of BLEU score according to the results?",
            "reference-answers": [
                "The standard normal attention model achieved the best performance in terms of BLEU score."
            ]
        },
        {
            "question": "What model achieved the highest BLEU score in the probes?",
            "reference-answers": [
                "The standard normal attention model achieved the highest BLEU score in the probes."
            ]
        },
        {
            "question": "Which model achieved the best performance in terms of BLEU score?",
            "reference-answers": [
                "The standard normal attention model achieved the best performance in terms of BLEU score."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen85-slide85/text.txt": [
        {
            "question": "Question: Why can we trust BLEU scores for evaluating translations in English?",
            "reference-answers": [
                "We can trust BLEU scores for evaluating English translations because they were double-checked and found to be consistent with human evaluations."
            ]
        },
        {
            "question": "Question: Can we trust BLEU scores for English to check evaluation based on the provided information?",
            "reference-answers": [
                "Yes, we can trust BLEU scores for English evaluation since the BLEU scores were consistent with human evaluation after double-checking."
            ]
        },
        {
            "question": "What did they double-check to ensure the consistency of translation quality evaluation?",
            "reference-answers": [
                "They double-checked the BLEU scores to ensure they were consistent with human evaluations."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen86-slide86/text.txt": [
        {
            "question": "Question: Why did setups using inner attention or compound attention perform better?",
            "reference-answers": [
                "Setups using inner attention or compound attention performed better because they restrict the attention mechanism to a fixed number of points that do not change throughout the output sentence, which proved to be useful."
            ]
        },
        {
            "question": "The text suggests that attention mechanisms, particularly inner or compound attention, are effective and useful, even when restricted to a fixed number of points and not allowed to change throughout the output sentence.",
            "reference-answers": [
                "The text suggests that attention mechanisms, particularly inner or compound attention, are effective and useful, even when restricted to a fixed number of points and not allowed to change throughout the output sentence."
            ]
        },
        {
            "question": "The setups using inner attention or compound attention perform better because they effectively capture relevant information even when the attention points are fixed and do not change throughout the output sentence.",
            "reference-answers": [
                "The setups using inner attention or compound attention perform better because they effectively capture relevant information even when the attention points are fixed and do not change throughout the output sentence."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen87-slide87/text.txt": [
        {
            "question": "What happens to the score when more heads are added to the attention mechanism?",
            "reference-answers": [
                "The score improves when more heads are added to the attention mechanism."
            ]
        },
        {
            "question": "What does the text suggest about increasing the number of heads in the attention mechanism?",
            "reference-answers": [
                "Increasing the number of heads in the attention mechanism improves the score."
            ]
        },
        {
            "question": "What did we observe about the relationship between the number of heads in the attention mechanism and the model's score?",
            "reference-answers": [
                "The more heads in the attention mechanism, the better the model's score."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen88-slide88/text.txt": [
        {
            "question": "What is the purpose of the semantic evaluation mentioned in the context of the English to Czech machine translation system?",
            "reference-answers": [
                "The purpose of the semantic evaluation is to assess how well the English to Czech machine translation system understands and preserves the meaning of the text during translation."
            ]
        },
        {
            "question": "What is the teacher evaluating in the context of the machine translation system?",
            "reference-answers": [
                "The teacher is evaluating the semantic evaluation of the representations learned by the English to Czech machine translation system."
            ]
        },
        {
            "question": "What is the purpose of the semantic evaluation mentioned in the context of the English to Czech machine translation system?",
            "reference-answers": [
                "The purpose of the semantic evaluation is to assess how well the English to Czech machine translation system understands and preserves the meaning of the text during translation."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen89-slide89/text.txt": [
        {
            "question": "According to the text, how does the performance of bag of words with GloVe embeddings compare to that of neural networks in representing sentences?",
            "reference-answers": [
                "The bag of words method using GloVe embeddings is very good, but neural networks (like infercent) are excellent, so neural networks perform better in representing sentences."
            ]
        },
        {
            "question": "Which methods are highlighted as effective for representing sentences, and what are they compared to?",
            "reference-answers": [
                "The methods highlighted as effective for representing sentences are bag of words (using GloVe embeddings) and the representations that emerge automatically when training neural network systems (e.g., Infercent). These methods are compared to each other, with both showing effectiveness in various tasks."
            ]
        },
        {
            "question": "Question: Based on the text, how does the effectiveness of the bag of words method compare to the Infercent method in sentence representation tasks?",
            "reference-answers": [
                "The bag of words method is very effective in sentence representation tasks and is particularly strong when using GloVe embeddings. It is noted to be a very good way of representing sentences. However, Infercent is highlighted as excellent in these tasks. Both methods are effective, with Infercent being superior according to the provided measures."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen90-slide90/text.txt": [
        {
            "question": "What recommendation is made regarding the use of attention in the encoder-decoder model?",
            "reference-answers": [
                "The recommendation is to avoid using attention in the encoder-decoder model and instead use just the final state of the encoder and decoder, as this approach serves better across many evaluation tasks."
            ]
        },
        {
            "question": "According to the text, what approach is suggested for better performance across evaluation tasks?",
            "reference-answers": [
                "The approach suggested for better performance across evaluation tasks is to use just the final state of the encoder and decoder."
            ]
        },
        {
            "question": "What does the text suggest is better for performance across evaluation tasks?",
            "reference-answers": [
                "The text suggests that using just the final state of the encoder and decoder is better for performance across evaluation tasks."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen91-slide91/text.txt": [
        {
            "question": "How does increasing the number of heads affect performance in semantics tasks compared to translation tasks, based on the information provided?",
            "reference-answers": [
                "Increasing the number of heads worsens performance in semantics tasks but improves performance in translation tasks."
            ]
        },
        {
            "question": "What effect does increasing the number of heads have on the performance in semantics tasks compared to translation tasks?",
            "reference-answers": [
                "Increasing the number of heads worsens performance in semantics tasks but improves performance in translation tasks."
            ]
        },
        {
            "question": "How does increasing the number of attention heads affect performance in semantics tasks compared to translation tasks?",
            "reference-answers": [
                "Increasing the number of attention heads worsens performance in semantics tasks but improves performance in translation tasks."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen92-slide92/text.txt": [
        {
            "question": "What is the relationship between the BLEU score and semantic evaluations as described in the text?",
            "reference-answers": [
                "The relationship between the BLEU score and semantic evaluations is characterized by a negative correlation. This means that when the BLEU score is high (indicating good translation quality), the semantic evaluations tend to be lower, and vice versa. Semantic evaluations, however, show a positive correlation with each other, indicating consistency among them. This suggests that while BLEU measures translation accuracy based on surface-level features like word alignment, semantic evaluations focus on deeper meaning and context, which are not prioritized in the translation task."
            ]
        },
        {
            "question": "What is the relationship between the BLEU score and semantic measures when evaluating machine translation models, as described in the text?",
            "reference-answers": [
                "The relationship between the BLEU score and semantic measures, as described in the text, is characterized by a negative correlation. When evaluating machine translation models across various test sets and setups, a higher BLEU score typically corresponds to a lower performance in semantic tasks, such as paraphrasing, image-based paraphrasing, and sentiment prediction. This inverse relationship suggests that models optimized for BLEU scores may prioritize surface-level accuracy (e.g., word-for-word translation) over deeper semantic understanding. Conversely, when the BLEU score is disregarded, semantic evaluations show a positive correlation, indicating that semantic measures are more aligned with each other but less so with translation quality metrics like BLEU. Additionally, the text highlights that translation models often operate at a shallow level, focusing on producing accurate word sequences rather than capturing the semantic meaning of sentences."
            ]
        },
        {
            "question": "What does the negative correlation between BLEU score and semantic tasks suggest about the nature of machine translation models?",
            "reference-answers": [
                "The negative correlation between BLEU score and semantic tasks suggests that machine translation models optimized for BLEU focus on surface-level accuracy, such as word-for-word translation, rather than deep semantic understanding. High BLEU scores indicate good lexical matching but may not capture the meaning of sentences, leading to poor performance in tasks requiring semantic comprehension. This implies that translation models prioritize accurate word alignment over understanding the actual message or context of the text."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen93-slide93/text.txt": [
        {
            "question": "What effect does removing the transformer model have on the correlation between BLEU scores and other metrics in the evaluation setup described?",
            "reference-answers": [
                "Removing the transformer model results in a clearer correlation pattern where the BLEU score becomes more negatively correlated, and other metrics show more positive correlations with each other."
            ]
        },
        {
            "question": "What effect does removing the transformer setup have on the correlation between BLEU and other metrics in the English to German evaluation?",
            "reference-answers": [
                "Removing the transformer setup in the English to German evaluation makes the correlation between BLEU and other metrics clearer, with BLEU becoming more negatively correlated and the other metrics showing more positive correlations with each other."
            ]
        },
        {
            "question": "What effect did removing the transformer model have on the correlation metrics, and what does the author suggest about deeper neural networks?",
            "reference-answers": [
                "Removing the transformer model made the correlation metrics clearer, with BLEU becoming more negatively correlated and other metrics showing more positive correlations. The author suggests that deeper neural networks, such as deeper recurrent neural networks, might perform better in semantic scoring and should be re-evaluated to confirm this."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen94-slide94/text.txt": [
        {
            "question": "What is the attention-attention model described in the text, and what does it show about the eight heads when the network produces an output word?",
            "reference-answers": [
                "The attention-attention model described in the text refers to a compound attention mechanism with eight heads. It shows that when the network produces an output word, each of the eight heads focuses on different parts of the sentence, indicating where the attention is directed for that particular output."
            ]
        },
        {
            "question": "What does the text explain about the attention-attention model with eight heads?",
            "reference-answers": [
                "The text explains that the attention-attention model with eight heads refers to a model where each of the eight attention heads focuses on different parts of the input sentence when producing an output word, demonstrating how each head directs its attention to specific locations in the input."
            ]
        },
        {
            "question": "How many attention heads are there, and what do they do when the model produces an output word?",
            "reference-answers": [
                "There are eight attention heads, and when the model produces an output word, each head focuses on different parts of the input sentence."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen95-slide95/text.txt": [
        {
            "question": "In what order were the yellow and red hats used in the sentences?",
            "reference-answers": [
                "The yellow hat was used first, followed by the red hat."
            ]
        },
        {
            "question": "In what order were the yellow and red hats used in the sentences?",
            "reference-answers": [
                "The yellow hat was used first, followed by the red hat. \n\nANSWER: The yellow hat was used before the red hat."
            ]
        },
        {
            "question": "What order was observed for the use of the yellow and red hats in the sentences?",
            "reference-answers": [
                "The yellow hat was always used at the beginning of the sentence, followed by the red hat."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen96-slide96/text.txt": [
        {
            "question": "What does the model divide the sentence into, according to the text?",
            "reference-answers": [
                "The model divides the sentence into eight equal parts, referred to as eighths."
            ]
        },
        {
            "question": "How does the recurrent neural model distribute its attention when translating sentences?",
            "reference-answers": [
                "The recurrent neural model divides the sentence into eight equal parts, with each part having its own head. The attention is distributed equidistantly across these segments, rather than focusing on specific grammatical heads like verbs, punctuation, or numbers."
            ]
        },
        {
            "question": "Where is the most attention put when translating, according to the text?",
            "reference-answers": [
                "The most attention is put on equal segments of the sentence, divided into eight equal parts, rather than focusing on specific grammatical elements like verbs, punctuation, or numbers."
            ]
        }
    ],
    "nmt-class/lecture09-word-and-sent-reprs/screen97-slide97/text.txt": [
        {
            "question": "Why does having more attention heads result in worse performance on polarity check tasks according to the text?",
            "reference-answers": [
                "Having more attention heads results in worse performance on polarity check tasks because the heads divide the sentence into parts, making it harder to search for sentiment-related information across the divided sections. With fewer heads, the entire sentence is processed together, making it easier to capture the overall sentiment."
            ]
        },
        {
            "question": "How does distributing information across multiple heads affect the system's performance in polarity check tasks?",
            "reference-answers": [
                "Distributing information across multiple heads makes it harder for the system to perform polarity check tasks because the relevant sentiment information is spread across different parts of the sentence, requiring more effort to search across the heads. Having fewer heads simplifies the process by keeping the information more centralized."
            ]
        },
        {
            "question": "How does the distribution of sentence parts among multiple heads affect the system's ability to perform polarity check tasks?",
            "reference-answers": [
                "The distribution of sentence parts among multiple heads leads to a shallow processing approach, which makes it difficult for the system to perform polarity check tasks effectively. Since the information about the polarity (e.g., whether the user was happy or the watcher was happy with the movie) is spread across different heads, the system has to search for this information across all heads, which becomes increasingly challenging as the number of heads increases. In contrast, with fewer heads or no attention mechanism, the entire sentence is processed at the same level, making it easier to access the necessary information for polarity checks without extensive searching."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen01-slide01/text.txt": [
        {
            "question": "Why is multilingual machine translation considered helpful even though machine translation by itself already involves at least two languages?",
            "reference-answers": [
                "Multilingual machine translation is considered helpful because it allows handling multiple language pairs more efficiently, potentially improving performance and resource sharing compared to working with each language pair individually."
            ]
        },
        {
            "question": "Why would multilingual machine translation be helpful?",
            "reference-answers": [
                "Multilingual machine translation is helpful because it allows the sharing of translation knowledge across multiple languages, which can improve the efficiency and effectiveness of the translation process. This approach is particularly useful for languages with limited resources or data, as it enables the leveraging of data from multiple language pairs to enhance the overall performance of the translation model."
            ]
        },
        {
            "question": "Why would more than two languages be helpful in the context of machine translation?",
            "reference-answers": [
                "More than two languages can be helpful because it allows for resource sharing and data efficiency, enabling translations between any pair without separate models and leveraging a larger dataset for improved accuracy."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen02-slide02/text.txt": [
        {
            "question": "What are the two main setups discussed in the lecture regarding multilingual systems?",
            "reference-answers": [
                "The two main setups discussed in the lecture are transfer learning and truly multilingual systems."
            ]
        },
        {
            "question": "What are the two main types of language setups discussed in the lecture, and what is the role of other languages in the transfer setup?",
            "reference-answers": [
                "The two main types of language setups discussed are transfer learning and truly multilingual systems. In the transfer setup, other languages serve as auxiliary data used only during the preparation (training) phase, while the system focuses on a single language pair during operation."
            ]
        },
        {
            "question": "What is the final letter of the stars mentioned in the text?",
            "reference-answers": [
                "The final letter of the stars mentioned in the text is B."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen03-slide03/text.txt": [
        {
            "question": "What are the two main motivations for developing multilingual machine translation systems, as discussed in the text?",
            "reference-answers": [
                "The two main motivations for developing multilingual machine translation systems are: (1) the need to handle low-resource languages, where there is insufficient parallel data for training dedicated translation systems, and (2) the desire to exploit language similarities, such as shared grammatical structures and vocabulary, to improve translation quality and efficiency."
            ]
        },
        {
            "question": "What are the two main motivations for multilingual machine translation, and what examples are provided to illustrate the need for it?",
            "reference-answers": [
                "The two main motivations for multilingual machine translation are the scarcity of parallel data for many languages and the leveraging of language similarities to improve translation quality. Examples include the challenges faced in translating between low-resource languages like ODEA and Marathi, where parallel texts are limited, and the practical need for efficient translation systems in multilingual environments such as the United Nations, European Union, and EuroSci, where building independent models for each language would be resource-intensive."
            ]
        },
        {
            "question": "What are the two main motivations for developing multilingual machine translation systems as discussed in the text?",
            "reference-answers": [
                "The two main motivations are the need to address the scarcity of parallel data for many low-resource languages and the desire to improve translation quality by leveraging additional information from multilingual resources."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen04-slide04/text.txt": [
        {
            "question": "What is the primary goal of transfer learning in the context of machine translation, and how does it utilize data from other languages?",
            "reference-answers": [
                "The primary goal of transfer learning in machine translation is to enhance the translation quality for a specific target or child language pair. This is achieved by reusing data from other languages, allowing the model to benefit from additional information and improve its performance in the target language."
            ]
        },
        {
            "question": "What is the primary purpose of transfer learning in machine translation?",
            "reference-answers": [
                "The primary purpose of transfer learning in machine translation is to improve the translation quality in a target language pair by reusing data from other languages."
            ]
        },
        {
            "question": "What is the primary purpose of transfer learning in machine translation?",
            "reference-answers": [
                "The primary purpose of transfer learning in machine translation is to improve the translation quality in one target language pair by reusing data from other languages."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen05-slide05/text.txt": [
        {
            "question": "What reason is given for the lack of overfitting in the training curve for neural machine translation systems as described in the text?",
            "reference-answers": [
                "The lack of overfitting is attributed to the model being well-regularized and having sufficient training data, which prevents performance degradation and keeps it stable."
            ]
        },
        {
            "question": "What characteristic is observed in the training curve for neural machine translation systems using transfer learning?",
            "reference-answers": [
                "The characteristic observed in the training curve is the absence of overfitting, with performance on the development set improving and then stabilizing, indicating a well-regulated model with sufficient training data."
            ]
        },
        {
            "question": "The characteristic of the training curve for neural machine translation systems when using transfer learning is that it typically shows no overfitting, attributed to the model being well-regularized and having sufficient training data. As a result, performance remains stable, with the development set's performance increasing and then leveling off.",
            "reference-answers": [
                "The characteristic of the training curve for neural machine translation systems when using transfer learning is that it typically shows no overfitting, attributed to the model being well-regularized and having sufficient training data. As a result, performance remains stable, with the development set's performance increasing and then leveling off."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen06-slide06/text.txt": [
        {
            "question": "What would happen if additional data from another language is incorporated into the model?",
            "reference-answers": [
                "Incorporating additional data from another language into the model would lead to an improved performance, as it would enable transfer learning, resulting in a better curve."
            ]
        },
        {
            "question": "What is the expected outcome when additional data from another task or language is incorporated into transfer learning?",
            "reference-answers": [
                "The expected outcome is an improved learning curve when incorporating additional data from another task or language into transfer learning."
            ]
        },
        {
            "question": "What is the effect of incorporating additional data from another task or language on the learning curve in transfer learning, as suggested by the text?",
            "reference-answers": [
                "Incorporating additional data from another task or language can lead to a better learning curve in transfer learning, as it aims to improve the model's performance through clearer integration of this data."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen07-slide07/text.txt": [
        {
            "question": "What are the three places where improvements can appear based on the experiments with transfer of neural machine translation system?",
            "reference-answers": [
                "The improvements can appear in three places: at the beginning (initial performance could be better), it could learn faster, and the final performance should be better."
            ]
        },
        {
            "question": "What are the three places where improvements can appear, as mentioned in the text?",
            "reference-answers": [
                "The three places where improvements can appear are the initial performance, learning faster, and the final performance."
            ]
        },
        {
            "question": "According to the text, identify the three places where improvements can appear.",
            "reference-answers": [
                "The three places where improvements can appear are: the initial performance, learning faster, and the final performance."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen08-slide08/text.txt": [
        {
            "question": "What is the next step after training the system on the parent corpus until convergence?",
            "reference-answers": [
                "The next step is to apply the system to the language pair of interest."
            ]
        },
        {
            "question": "What is the first step in achieving transfer learning as described in the text?",
            "reference-answers": [
                "Train your system on the parent corpus or parent language pair until convergence."
            ]
        },
        {
            "question": "What is the next step after training the system on the parent corpus until convergence?",
            "reference-answers": [
                "The next step is to apply the system to the language pair of interest."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen09-slide09/text.txt": [
        {
            "question": "What happens to the child model's performance when it is initialized with the parent model's weights instead of random weights?",
            "reference-answers": [
                "The child model's performance improves because it avoids overfitting and benefits from the parent model's pre-trained weights, resulting in better performance on the held-out set."
            ]
        },
        {
            "question": "How does initializing the child model with weights from a parent model trained on a related language pair help mitigate overfitting, and what is the outcome compared to training the child model from scratch with random weights?",
            "reference-answers": [
                "Initializing the child model with weights from a parent model trained on a related language pair helps mitigate overfitting because it provides a pre-trained foundation, reducing the need to start from scratch with random weights. This transfer learning allows the child model to build upon the parent's learned features and patterns, preventing overfitting to the smaller training data. The outcome is better performance on the held-out set compared to training the child model from scratch, as the transferred weights help generalize better and avoid memorization of the training data."
            ]
        },
        {
            "question": "What is the result of applying transfer learning when training the child model on a small dataset?",
            "reference-answers": [
                "The result of applying transfer learning when training the child model on a small dataset is better performance on the held-out set, as it avoids the degradation due to overfitting."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen10-slide10/text.txt": [
        {
            "question": "What phenomenon was observed when attempting to train a machine translation model using a curriculum-style approach, and how does it impact the model's ability to retain previously learned information?",
            "reference-answers": [
                "The phenomenon observed was catastrophic forgetting. This occurs when neural networks, particularly during training, rapidly forget previously learned information when exposed to new training data, significantly impacting their ability to retain knowledge accumulated over time."
            ]
        },
        {
            "question": "What phenomenon is described where neural networks forget previously learned information when exposed to new training data?",
            "reference-answers": [
                "The phenomenon described is catastrophic forgetting."
            ]
        },
        {
            "question": "**Question:**  \nWhat phenomenon is illustrated when a neural network model, trained on a corpus of English-Czech sentences in a curriculum-style approach, demonstrates poor performance due to forgetting previously learned information when exposed to new data?",
            "reference-answers": [
                "The phenomenon illustrated is catastrophic forgetting."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen11-slide11/text.txt": [
        {
            "question": "Why is training a neural network with a sorted corpus problematic, and what alternative approach is suggested?",
            "reference-answers": [
                "Training a neural network with a sorted corpus is problematic because it leads to repetitive and similar sentences being grouped together, causing the network to fail in learning effectively. The suggested alternative is to randomize the data to provide a diverse and varied training experience, which helps the network learn better by exposing it to different examples."
            ]
        },
        {
            "question": "What is the negative impact of sorting the corpus when training a model, as discussed in the text?",
            "reference-answers": [
                "The negative impact is that sorting the corpus leads to repetitive and similar sentences during training, causing the network to fail to learn effectively as it doesn't expose the model to a diverse range of data necessary for proper learning and adaptation."
            ]
        },
        {
            "question": "Why is training a neural network on a sorted corpus a bad idea?",
            "reference-answers": [
                "Training a neural network on a sorted corpus is a bad idea because it leads to repetitive and non-random data, which can cause the model to fail to learn effectively in the final task. When the data is sorted in any way, such as by sentence length, the network starts with very similar or \"boring\" sentences, which limits its ability to generalize and adapt during training. Instead of learning diverse patterns, the model becomes focused on these repetitive examples, making it less effective in handling the actual task. Randomizing the data is crucial to ensure that each batch provides enough variation to guide the network toward optimal parameter adjustments."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen12-slide12/text.txt": [
        {
            "question": "What two main strategies were employed in organizing the training data to improve the model's learning?",
            "reference-answers": [
                "1. Sentences were organized into batches (or big chunks) with increasing sentence lengths, starting from very short sentences and gradually allowing longer ones up to all lengths, using a curriculum learning approach.\n2. The order of these sorted batches was flipped, reversing the sequence of the training data."
            ]
        },
        {
            "question": "What method was used by Tom Cosme to change the order of the batches after they were organized by sentence length?",
            "reference-answers": [
                "Tom Cosme used the method of flipping the order of the sorted batches to change their sequence."
            ]
        },
        {
            "question": "How was the model's training corpus organized before Tom Cosme flipped its order?",
            "reference-answers": [
                "The model's training corpus was organized into batches of sentences, each batch limited to a specific maximum sentence length. Initially, sentences were up to five words, then up to ten, then up to fifteen, and finally sentences of all lengths. These batches were diverse and structured to gradually increase sentence length."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen13-slide13/text.txt": [
        {
            "question": "What are the key observations regarding the system's learning and forgetting behavior as it was trained on different sentence lengths?",
            "reference-answers": [
                "The key observations are that the system initially performed better due to oversampling longer sentences, which provided more data early in training. However, as the model was trained to prohibit longer sentences (catastrophic forgetting), it lost the ability to produce long sentences, only generating sentences as long as the last batch it was trained on. The model did not learn the relationship between source and target sentence lengths, leading to this forgetting behavior."
            ]
        },
        {
            "question": "What phenomenon was observed when the system was trained with prohibitions on producing sentences beyond certain lengths, leading to a degradation in its ability to generate long sentences?",
            "reference-answers": [
                "The phenomenon observed was catastrophic forgetting, where the model unlearned its ability to produce long sentences after prohibiting sentences beyond certain lengths."
            ]
        },
        {
            "question": "**Question:**  \nWhat phenomenon was observed when restricting the maximum sentence length during training, as mentioned in the text?",
            "reference-answers": [
                "Catastrophic forgetting was observed when restricting the maximum sentence length during training, as the model unlearned its ability to produce long sentences."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen14-slide14/text.txt": [
        {
            "question": "What approach was used in early transfer learning experiments when dealing with language pairs?",
            "reference-answers": [
                "In early transfer learning experiments, the approach involved training on a parent language pair and then applying the same model to a child language pair without changing any parameters or hyperparameters, a method referred to as \"trivial transfer.\""
            ]
        },
        {
            "question": "What was the idea behind \"trivial transfer\" in the context of the transfer learning task described?",
            "reference-answers": [
                "The idea behind \"trivial transfer\" was to simply train the model on a parent language pair and then apply it directly to a child language pair without changing any parameters or hyperparameters."
            ]
        },
        {
            "question": "The setup in early transfer learning experiments involved using English as a common language and focusing on language pairs that were related. The approach used was \"trivial transfer,\" where the model was trained on a parent language pair and then directly applied to a child language pair without altering the model's parameters.",
            "reference-answers": [
                "The setup in early transfer learning experiments involved using English as a common language and focusing on language pairs that were related. The approach used was \"trivial transfer,\" where the model was trained on a parent language pair and then directly applied to a child language pair without altering the model's parameters or hyperparameters."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen15-slide15/text.txt": [
        {
            "question": "Why does combining the parent and child corpora result in a subword vocabulary that is overfitted to the parent?",
            "reference-answers": [
                "Combining the parent and child corpora results in a subword vocabulary that is overfitted to the parent because the parent corpus is typically much larger, giving it a stronger influence on the subword dictionary. This causes the subword dictionary to be heavily biased towards the patterns and vocabulary of the parent corpus, at the expense of the smaller child corpus."
            ]
        },
        {
            "question": "What is a potential issue when extracting subword vocabularies from both the parent and child corpora together?",
            "reference-answers": [
                "A potential issue is that the subword vocabulary may be heavily overfitted to the parent corpus, as it is typically larger, leading to the subword dictionary reflecting the parent's characteristics more than the child's."
            ]
        },
        {
            "question": "What is a potential issue when extracting a subword dictionary from both a parent corpus and a child corpus together?",
            "reference-answers": [
                "A potential issue is that the subword dictionary becomes heavily overfitted to the parent corpus."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen16-slide16/text.txt": [
        {
            "question": "What method did they use to balance the vocabulary in the corpora?",
            "reference-answers": [
                "They balanced the vocabulary by subsampling from the parent corpus to ensure the corpora were of the same sizes."
            ]
        },
        {
            "question": "What method was used to balance the vocabulary in the described approach?",
            "reference-answers": [
                "The method used to balance the vocabulary was subsampling from the parent corpus to have corpora of the same sizes."
            ]
        },
        {
            "question": "What method was used to balance the vocabulary in the described approach?",
            "reference-answers": [
                "Subsampling from the parent corpus to have corpora of the same sizes."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen17-slide17/text.txt": [
        {
            "question": "What potential issue was noted regarding the English language in the vocabulary study?",
            "reference-answers": [
                "English was oversampled because it appeared twice in both parent and child."
            ]
        },
        {
            "question": "Why was English oversampled in the subword vocabulary study?",
            "reference-answers": [
                "English was oversampled because it appeared twice in both the parent and child languages, whereas Czech and Estonian were equally represented in their respective categories."
            ]
        },
        {
            "question": "Why was English oversampled in the vocabulary study mentioned?",
            "reference-answers": [
                "English was oversampled because it appeared twice in both the parent and child languages, whereas Czech and Estonian, which were the child languages in this case, were equally represented."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen18-slide18/text.txt": [
        {
            "question": "What is the ratio of the corpus size difference between English-Czech and English-Slovak translations, and which languages does this comparison refer to?",
            "reference-answers": [
                "The ratio of the corpus size difference between English-Czech and English-Slovak translations is ninefold, meaning the English-Czech corpus is nine times larger than the English-Slovak corpus. This comparison refers to the languages English, Czech, and Slovak."
            ]
        },
        {
            "question": "What is the ratio of the corpus size for Czech-English compared to English-Slovak?",
            "reference-answers": [
                "The ratio of the corpus size for Czech-English compared to English-Slovak is ninefold, meaning Czech-English has nine times more data.\n\nANSWER: Ninefold."
            ]
        },
        {
            "question": "What is the ratio of the corpus size between Czech-English and English-Slovak data?",
            "reference-answers": [
                "The ratio of the corpus size between Czech-English and English-Slovak data is ninefold, meaning the Czech-English corpus is nine times larger."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen19-slide19/text.txt": [
        {
            "question": "What is the difference in the BLEU course when the model is trained on the child language pair only versus first the parent and then the child, and why are the improvements larger when translating into English?",
            "reference-answers": [
                "The difference in the BLEU score is observed when the model is trained on the child language pair only versus first training on the parent and then the child. When translating into English, the improvements are larger because the additional English data directly benefits the language model, particularly the decoder, which gains from the parent training data. In contrast, when translating from English, the encoder's knowledge of English doesn't help as much in producing Slovak, as Slovak differs significantly from Czech."
            ]
        },
        {
            "question": "The question is:  \n**\"What is the difference in the BLEU course when the model is trained on the child language pair only versus being first trained on the parent and then continuing on the child, and why is there a bigger improvement when translating into English compared to translating from English?\"**",
            "reference-answers": [
                "The difference in the BLEU score occurs when the model is trained on the child language pair only versus being first trained on the parent and then continuing on the child. When translating from English, some improvement is observed, but when translating into English, a bigger improvement is seen. The reason is that the additional English data directly helps the language model, particularly the decoder, which benefits directly from the parent training data. In the setup where English is the source, the encoder knows English better, but Slovak, being different from Czech, does not see as much improvement."
            ]
        },
        {
            "question": "Why is there a larger improvement in translation when translating into English compared to translation from English?",
            "reference-answers": [
                "The larger improvement in translation when translating into English is likely because the additional English data directly benefits the decoder in the language model. This direct enhancement in the decoder's ability to produce English translations is more impactful compared to the encoder's improved understanding of English when translating from English, which doesn't significantly help in producing translations into a different language like Slovak."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen20-slide20/text.txt": [
        {
            "question": "What were the parent languages used in the experiment with Estonian, and how did the corpus size of Finnish compare to Estonian's?",
            "reference-answers": [
                "The parent languages used in the experiment with Estonian were Finnish, Russian, and Czech. Finnish had a corpus size that was three to three and a half times larger than Estonian's."
            ]
        },
        {
            "question": "Question: Which of the parent languages is closely related to Estonian?",
            "reference-answers": [
                "The parent language closely related to Estonian is Finnish. \n\nANSWER: Finnish"
            ]
        },
        {
            "question": "What is the relationship between Finnish and Estonian, and how does the size of the Finnish corpus compare to Estonian's?",
            "reference-answers": [
                "Finnish and Estonian are closely related. The Finnish corpus is three times, three and a half times bigger than Estonian's."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen21-slide21/text.txt": [
        {
            "question": "Which language provided the biggest improvement in scores, and why?",
            "reference-answers": [
                "The biggest improvement in scores came from Czech, which is unrelated to English."
            ]
        },
        {
            "question": "Question: Which language provided the biggest improvement according to the text?",
            "reference-answers": [
                "The biggest improvement came from Czech."
            ]
        },
        {
            "question": "Which language provided the biggest improvement in scores?",
            "reference-answers": [
                "Czech"
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen22-slide22/text.txt": [
        {
            "question": "What conclusion was drawn from the experiment regarding the importance of the relationship between the parent and child language?",
            "reference-answers": [
                "The conclusion drawn from the experiment is that the relationship between the parent and child language is not too important for the outcome."
            ]
        },
        {
            "question": "What conclusion was drawn about the importance of the relationship between the parent and child language in the experiment?",
            "reference-answers": [
                "The conclusion drawn was that the relationship between the parent and child language is not too important."
            ]
        },
        {
            "question": "What conclusion was drawn about the relationship between the parent and child languages based on the experiment involving Russian?",
            "reference-answers": [
                "The conclusion drawn was that the relationship between the parent and child languages is not too important."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen23-slide23/text.txt": [
        {
            "question": "What does the text refer to when discussing the transfer of knowledge?",
            "reference-answers": [
                "The text refers to the transfer of knowledge in the context of machine learning, specifically moving knowledge from a model pre-trained on Estonian and English to a child model focused on Finnish and English. This involves techniques like pre-training or fine-tuning to facilitate learning in a new language pair."
            ]
        },
        {
            "question": "Which languages were the focus of the experiment involving the child with the three-wheel transfer?",
            "reference-answers": [
                "The focus of the experiment was on Finnish and English."
            ]
        },
        {
            "question": "Which languages were used for pre-training and testing in the study?",
            "reference-answers": [
                "The pre-training was done on Estonian and English, while the testing was conducted on Finnish and English."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen24-slide24/text.txt": [
        {
            "question": "What did the experiments show about the importance of the parent language pair's size in training data for translation quality?",
            "reference-answers": [
                "The experiments showed that the size of the parent language pair's training data is the most important factor for translation quality. A smaller parent language pair negatively impacts the performance of the child, making pre-training not advisable."
            ]
        },
        {
            "question": "What did the experiments confirm as the most important factor affecting the child's translation performance?",
            "reference-answers": [
                "The experiments confirmed that the size of the training data for the parent language pair is the most important factor affecting the child's translation performance."
            ]
        },
        {
            "question": "What was identified as the most important aspect in the experiments regarding the parent language pair?",
            "reference-answers": [
                "The size of the training data for the parent language pair was identified as the most important aspect in the experiments."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen25-slide25/text.txt": [
        {
            "question": "What was the impact of swapping the position of English in the parent and child models on the child's performance, according to the experiments?",
            "reference-answers": [
                "Swapping the position of English in the parent and child models still resulted in improvements to the child's performance, though the gains were smaller compared to when English was on the correct side. The shared vocabulary contributed to this positive impact, and the child model still outperformed the baseline even with English on the wrong side."
            ]
        },
        {
            "question": "What was the impact on model performance when the position of English was swapped between the parent and child models in the experiments?",
            "reference-answers": [
                "The model performance still improved when the position of English was swapped between the parent and child models, though the gains were larger when English was on the correct side for each model."
            ]
        },
        {
            "question": "What was the impact of swapping the position of English in the parent and child models during the experiments?",
            "reference-answers": [
                "Swapping the position of English in the parent and child models still resulted in improvements over the baseline, even though the gains were smaller when English was on the \"wrong\" side compared to when it was on the \"correct\" side. The shared vocabulary in English contributed to these improvements, demonstrating that English's presence in the models was beneficial regardless of its position."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen26-slide26/text.txt": [
        {
            "question": "What was the observed outcome when training the translation model on unrelated language pairs like Arabic-Russian or Spanish-French for Estonian-English translation?",
            "reference-answers": [
                "The translation model showed some improvement when trained on unrelated language pairs like Arabic-Russian or Spanish-French, despite the lack of a common language and shared vocabulary with the target Estonian-English translation."
            ]
        },
        {
            "question": "\"Based on the text, what is the observed impact on translation performance when training a model to translate Estonian into English using corpora from unrelated language pairs (e.g., Arabic-Russian, Spanish-French)?\"",
            "reference-answers": [
                "The translation performance shows some improvement when training on corpora from unrelated language pairs, though the enhancement is less significant compared to when there is a common language. This improvement occurs even though there is no shared vocabulary between the source (Estonian) and target (English) languages or the training corpora."
            ]
        },
        {
            "question": "What improvement was observed when training the translation model on language pairs with no common language?",
            "reference-answers": [
                "There was some improvement observed when training the translation model on language pairs with no common language, though it was not as significant as in cases where a common language existed."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen27-slide27/text.txt": [
        {
            "question": "Question: When pre-training on a language pair with non-overlapping scripts, what type of improvement was observed?",
            "reference-answers": [
                "A non-significant improvement was observed."
            ]
        },
        {
            "question": "What was the outcome of pre-training on a language pair with no shared script (Arabic to Cyrillic) in terms of effectiveness for other language setups?",
            "reference-answers": [
                "Pre-training on the Arabic-Cyrillic language pair, which doesn't share a script, resulted in a non-significant improvement for other language setups."
            ]
        },
        {
            "question": "Which language pairs showed a non-significant improvement and which showed significant improvements?",
            "reference-answers": [
                "The language pair that showed a non-significant improvement was Arabic to Cyrillic. The significant improvements were observed in other setups, such as Latin-Estonian to Latin-English."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen28-slide28/text.txt": [
        {
            "question": "What is the optimal strategy for transitioning training from the parent model to the child model to ensure the best performance?",
            "reference-answers": [
                "The optimal strategy is to wait until the parent model is fully converged before transitioning to the child model. This ensures the best performance as the child starts from a fully trained parent."
            ]
        },
        {
            "question": "What is the best time to start training the child model according to the text?",
            "reference-answers": [
                "The best time to start training the child model is when the parent model is fully converged."
            ]
        },
        {
            "question": "What is the implication of training the parent until it is fully converged before starting the child?",
            "reference-answers": [
                "Training the parent until it is fully converged before starting the child ensures the best performance for the child, as the child begins with the parent's fully optimized model."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen29-slide29/text.txt": [
        {
            "question": "What is the impact of using a pre-trained parent model for transfer learning when the child model has only 10,000 sentence pairs in a low-resource setup?",
            "reference-answers": [
                "Using a pre-trained parent model for transfer learning significantly improves the child model's performance when the child has limited data. In this case, the child model with 10,000 sentence pairs saw its performance increase by 10 points (from 2 to 12) when the parent model, already well-trained on English-Finnish, was used for transfer learning. This demonstrates that transfer learning is particularly effective when the child dataset is much smaller than the parent dataset."
            ]
        },
        {
            "question": "What is the performance gain when using the parent model with 10,000 English-Estonian sentence pairs?",
            "reference-answers": [
                "The performance gain when using the parent model with 10,000 English-Estonian sentence pairs is a 10-point increase."
            ]
        },
        {
            "question": "What performance score did the baseline model achieve with 10,000 English-Estonian sentence pairs?",
            "reference-answers": [
                "The baseline model achieved a performance score close to two points with 10,000 English-Estonian sentence pairs."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen30-slide30/text.txt": [
        {
            "question": "What improvements were observed in the child's language production after the analysis?",
            "reference-answers": [
                "The improvements observed included longer outputs, which benefited the BLEU score due to its length penalty, and the use of better-quality words. The child produced regular Estonian words rather than relying on copied parent language tokens, indicating an enhancement in both sentence structure and vocabulary."
            ]
        },
        {
            "question": "What were the reasons behind the improved translation quality in the child's output, and what kind of new words were learned in the process?",
            "reference-answers": [
                "The improved translation quality in the child's output was due to the child producing longer sentences, which benefited the BLEU score, and using better words. The new words learned were regular Estonian words, not named entities, indicating an improvement in overall sentence structure."
            ]
        },
        {
            "question": "What was the source of the better words that the child produced in the study?",
            "reference-answers": [
                "The better words the child produced were Estonian words, specifically regular Estonian words, not names or numbers that might have been copied from the parent."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen31-slide31/text.txt": [
        {
            "question": "Why were there no subword units in common between the English and Estonian vocabulary in the child model?",
            "reference-answers": [
                "The absence of common subword units between English and Estonian in the child model is due to the different writing scripts used: Russian (Cyrillic) and Estonian (Latin), which have no overlapping subword units."
            ]
        },
        {
            "question": "What caused the child model to have no overlapping subword units between English-Russian and English-Estonian translations?",
            "reference-answers": [
                "The different script systems used by Russian (Cyrillic) and Estonian (Latin) caused the child model to have no overlapping subword units between English-Russian and English-Estonian translations."
            ]
        },
        {
            "question": "Why are there no subword units in the child model when translating from English to Estonian?",
            "reference-answers": [
                "Because Russian is written in Cyrillic and Estonian in Latin, there are no overlapping subword units, so the child model has none."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen32-slide32/text.txt": [
        {
            "question": "What effect does training a parent model on sentences of different lengths have on the BLEU score and the length of the translated outputs?",
            "reference-answers": [
                "Training the parent model on sentences of different lengths significantly impacts both the BLEU score and the length of the translated outputs. When the parent model is trained only on short sentences (e.g., under 20 words), the BLEU score is very low, and the outputs are also very short, averaging around 10 words. Conversely, training on longer sentences (20-40 words) results in a lower BLEU score as well, with outputs becoming longer than necessary, averaging around 15 words. The model struggles to produce appropriate outputs for sentences outside the trained range, leading to penalties for brevity or n-gram precision. However, training on a balanced range of sentence lengths yields the best BLEU score and appropriate output lengths, demonstrating the importance of diverse training data for optimal performance."
            ]
        },
        {
            "question": "How does training the parent model on sentences of varying lengths affect its translation performance, as measured by BLEU score and output length?",
            "reference-answers": [
                "Training the parent model on sentences of varying lengths generally improves translation performance, as measured by the BLEU score and output length. When the parent model is trained on a diverse range of sentence lengths, it achieves the best BLEU score and optimal average output length of 15 words. However, training the model on only short sentences results in poor performance, with a low BLEU score and very short outputs averaging 10 words. Conversely, training on longer sentences (20–40 words) also leads to a significant drop in BLEU score, as the model produces overly long outputs averaging five to six words beyond optimal length, leading to lower precision in n-grams and further penalization in the BLEU score. Thus, the model benefits most from training on a balanced mix of sentence lengths, allowing it to generalize better across different sentence lengths."
            ]
        },
        {
            "question": "When the parent model was trained on sentences with 20 to 40 words, the BLEU score was significantly lower compared to training on a balanced range of sentence lengths. Additionally, the output sentences were longer than the average length of 15 words, leading to a decrease in BLEU score due to precision issues with n-grams and brevity penalties.",
            "reference-answers": [
                "When the parent model was trained on sentences with 20 to 40 words, the BLEU score was significantly lower compared to training on a balanced range of sentence lengths. Additionally, the output sentences were longer than the average length of 15 words, leading to a decrease in BLEU score due to precision issues with n-grams and brevity penalties."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen33-slide33/text.txt": [
        {
            "question": "What is the impact of using constrained versus unconstrained parents on the child model's performance in terms of BLEU score and average sentence length?",
            "reference-answers": [
                "Using unconstrained parents resulted in a higher BLEU score of 19, indicating better transfer learning. In contrast, constrained parents showed less improvement, leading to a degraded BLEU score. However, both constrained and unconstrained parents yielded a similar average sentence length of 15.4, as the child model effectively unlearned the length limitations."
            ]
        },
        {
            "question": "Why does the use of constrained parents result in less improvement in the BLEU score compared to the unconstrained parent, even though the average sentence length is still acceptable?",
            "reference-answers": [
                "The constrained parents result in less improvement in the BLEU score because their restrictions limit the model's ability to transfer learning effectively, even though the average sentence length remains acceptable."
            ]
        },
        {
            "question": "What impact do constrained parent models have on transfer learning performance compared to unconstrained ones?",
            "reference-answers": [
                "Constrained parent models result in degraded transfer learning performance compared to unconstrained ones, as they provide less improvement in BLEU score despite maintaining acceptable average sentence lengths."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen34-slide34/text.txt": [
        {
            "question": "What two types of transfer were discussed in the text?",
            "reference-answers": [
                "The two types of transfer discussed were baseline transfer and trivial transfer."
            ]
        },
        {
            "question": "What does \"trivial transfer\" refer to, as discussed?",
            "reference-answers": [
                "\"Trivial transfer\" refers to a straightforward or basic form of transfer, as discussed, likely contrasting with a more complex or higher level of transfer."
            ]
        },
        {
            "question": "What was the purpose of highlighting the trivial transfer mentioned in the text?",
            "reference-answers": [
                "The purpose was to highlight the trivial transfer as the baseline transfer they discussed."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen35-slide35/text.txt": [
        {
            "question": "What was the next topic discussed after the previous content?",
            "reference-answers": [
                "The next topic discussed after the previous content was multilingual machine translation."
            ]
        },
        {
            "question": "What did the teacher move to next after discussing the previous text?",
            "reference-answers": [
                "The teacher moved to multilingual machine translation next."
            ]
        },
        {
            "question": "What is the next topic discussed after the previous content?",
            "reference-answers": [
                "Multilingual machine translation"
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen36-slide36/text.txt": [
        {
            "question": "**Question:**  \nWhat is zero-shot training, and how does it relate to multilingual machine translation systems?",
            "reference-answers": [
                "Zero-shot training refers to a system's ability to translate between language pairs that were not explicitly included together in its training data. In the context of multilingual machine translation systems, which can handle multiple source and target languages, zero-shot training enables the system to translate between any pair of languages it has been exposed to as sources or targets, even if that specific pair was not part of the training data. This capability leverages the system's multilingual knowledge to handle unseen language pairs by utilizing the existing language resources it was trained on."
            ]
        },
        {
            "question": "**Question:**  \nWhat is pivot translation, and how does it benefit translation between low-resource languages? Additionally, explain the concepts of multilingual source and target systems and describe the idea behind zero-shot training.",
            "reference-answers": [
                "**Answer:**\n\nPivot translation, also known as cascading, is a method where translation is done through an intermediate or pivot language. For instance, translating between Czech and German can be achieved by first translating from Czech to English and then from English to German, using English as the pivot. This approach is particularly useful for low-resource languages because it allows translation through a high-resource language with ample data, making the process more feasible and efficient. It's also employed in interpreting, where one interpreter translates into an intermediate language, and another interpreter then translates further into the target language.\n\nMultilingual source systems are designed to accept multiple source languages and translate them into a common target language, such as English. This setup is efficient for handling various source languages. Conversely, multilingual target systems can produce translations in multiple target languages. A system can also be both multilingual source and target, enabling it to handle multiple sources and targets simultaneously, thus being versatile for various translation needs.\n\nZero-shot training refers to a system's ability to translate between language pairs that it hasn't been explicitly trained on. If the system has been exposed to both languages individually during training, it can perform translations between them without prior specific pairing. Beyond this, systems can also be trained to translate into or from languages they haven't encountered, enhancing their flexibility and capability in handling diverse translation tasks."
            ]
        },
        {
            "question": "What is zero-shot training and how does it work in the context of machine translation systems?",
            "reference-answers": [
                "Zero-shot training refers to a machine translation system's ability to translate between language pairs that it has not been explicitly trained on. In this context, the system is trained on multiple source and target languages, but the specific pair of languages it translates between may not have been part of the training data. The system can therefore handle translations for language pairs that it has never seen before, as long as the source and target languages involved were included in the training data in some form. Beyond zero-shot training, a system might also be expected to translate into or from languages it has never encountered at all."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen37-slide37/text.txt": [
        {
            "question": "What is the example provided in the text to illustrate the advantage of using an intermediate language in multi-source translation?",
            "reference-answers": [
                "The example provided is translating the German word \"Schloss\" to French. The German word \"Schloss\" is ambiguous, meaning it can refer to either \"castle\" or \"lock.\" By first translating \"Schloss\" into English, where it becomes \"castle,\" the ambiguity is resolved, and the correct French translation \"château\" (meaning castle) is chosen. This demonstrates how using an intermediate language (English) in multi-source translation can help disambiguate and select the appropriate translation."
            ]
        },
        {
            "question": "Question: How does the text suggest multi-target focus benefits a multinational institution, and what is the key question regarding the number of languages handled by the model?",
            "reference-answers": [
                "The text suggests that multi-target focus benefits a multinational institution by enabling efficient translation into multiple languages simultaneously through a single system, which is cost-effective and resource-efficient. The key question is: How many target languages can you include in a model without significantly reducing its performance?"
            ]
        },
        {
            "question": "What is the maximum number of target languages that can be integrated into a model without significantly impacting its performance?",
            "reference-answers": [
                "The text does not provide a specific maximum number of target languages that can be integrated into a model without significantly impacting its performance. It discusses the benefits of multi-target translation and the idea of handling multiple languages but does not specify an exact figure."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen38-slide38/text.txt": [
        {
            "question": "What is described as the ideal setup for a machine translation system in the text?",
            "reference-answers": [
                "The ideal setup is a flexible multilingual machine translation system that uses one neural network to translate from multiple source languages into multiple target languages at once."
            ]
        },
        {
            "question": "What is the ideal setup for the machine translation system described?",
            "reference-answers": [
                "The ideal setup is a flexible multilingual machine translation system that uses one neural network to translate from multiple source languages into multiple target languages simultaneously, allowing for various input and output configurations."
            ]
        },
        {
            "question": "**Question:** What is the main characteristic of the ideal setup for a flexible multilingual machine translation system?",
            "reference-answers": [
                "The main characteristic is that it is a single neural network capable of translating from multiple sources into multiple targets simultaneously, making it flexible for various language combinations."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen39-slide39/text.txt": [
        {
            "question": "What are the key characteristics of multisource translation mentioned in the text?",
            "reference-answers": [
                "The key characteristics of multisource translation mentioned in the text are:\n- It is an old idea.\n- It uses word-based and phrase-based systems.\n- It has been tested."
            ]
        },
        {
            "question": "What does multisource translation involve using?",
            "reference-answers": [
                "Multisource translation involves using word-based and phrase-based systems."
            ]
        },
        {
            "question": "What does the text suggest about the age and history of multisource translation?",
            "reference-answers": [
                "The text suggests that multisource translation is pretty old and has a history that includes testing with word-based and phrase-based systems."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen40-slide40/text.txt": [
        {
            "question": "What is the underlying principle of neural machine translation, and how does it differ from traditional methods in handling the conditioning of source sentences?",
            "reference-answers": [
                "The underlying principle of neural machine translation is that it functions as a conditioned language model, where the system is conditioned on the source sentence as a whole. This differs from traditional methods, which often treated each source language separately and struggled with reusing knowledge due to strong independence assumptions. Neural machine translation, by conditioning on multiple source sentences collectively, handles them more efficiently and effectively."
            ]
        },
        {
            "question": "In neural machine translation, what is the system conditioned on?",
            "reference-answers": [
                "The neural machine translation system is conditioned on the source sentence as a whole."
            ]
        },
        {
            "question": "What is the underlying principle of neural machine translation systems?",
            "reference-answers": [
                "The underlying principle of neural machine translation systems is that they function as conditioned language models, where the system is conditioned on the source sentence as a whole."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen41-slide41/text.txt": [
        {
            "question": "What is the main drawback of the setup described in the text and why is it considered a limitation?",
            "reference-answers": [
                "The main drawback of the setup is that it requires a multiparl corpus, meaning both source languages (French and German) are needed to feed the decoder. This is considered a limitation because it restricts the model to situations where such a corpus is available, making it less versatile for scenarios with only one source language."
            ]
        },
        {
            "question": "What is the main drawback of the setup described by Zoffa and Knight?",
            "reference-answers": [
                "The main drawback of the setup described by Zoffa and Knight is that it requires a multiparl corpus, as it needs to process two source languages and combine their information."
            ]
        },
        {
            "question": "What is the main drawback of the setup described for translating English from French and German sources?",
            "reference-answers": [
                "The main drawback of the setup is that you really need a multiparl corpus."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen42-slide42/text.txt": [
        {
            "question": "What was the main question addressed by Orhan Ferrat's 2016 experiments with bilingual corpora?",
            "reference-answers": [
                "What to do with attention when training N encoders and N decoders with multiple source and target sites using bilingual corpora."
            ]
        },
        {
            "question": "What was the main focus of Orhan Ferrat's 2016 experiments with corpora?",
            "reference-answers": [
                "The main focus of Orhan Ferrat's 2016 experiments was exploring how to handle attention in a setup using only bilingual corpora across many language pairs, training multiple encoders and decoders to manage this complexity."
            ]
        },
        {
            "question": "What was the main focus of Orhan Ferrat's 2016 experiment when working with multiple bilingual corpora and training N encoders and decoders?",
            "reference-answers": [
                "The main focus of Orhan Ferrat's 2016 experiment was on attention mechanisms when working with multiple bilingual corpora, specifically training N encoders and N decoders to handle multiple source and target language pairs."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen43-slide43/text.txt": [
        {
            "question": "What does the attention do as you produce the target output?",
            "reference-answers": [
                "The attention moves across the positions in the source language as you produce each word of the target output."
            ]
        },
        {
            "question": "How does attention move across the source language when producing each word of the target output?",
            "reference-answers": [
                "As each word of the target output is produced, attention moves across the positions in the source language."
            ]
        },
        {
            "question": "What is the movement of attention during the production of target output?",
            "reference-answers": [
                "The attention moves across the positions in the source language as you produce the target output, one word at a time."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen44-slide44/text.txt": [
        {
            "question": "How is the attention mechanism handled in the model when dealing with different language pairs?",
            "reference-answers": [
                "The attention mechanism is the same across all encoders and decoders. Depending on the language pair, the respective encoder and decoder are trained, but the attention mechanism remains consistent across all languages."
            ]
        },
        {
            "question": "The key idea is that the attention mechanism remains consistent and unchanged across all language pairs, allowing each encoder-decoder pair to be trained separately while sharing the same attention mechanism.",
            "reference-answers": [
                "The key idea is that the attention mechanism remains consistent and unchanged across all language pairs, allowing each encoder-decoder pair to be trained separately while sharing the same attention mechanism."
            ]
        },
        {
            "question": "Does the attention mechanism change when training with different language pairs, and why or why not?",
            "reference-answers": [
                "The attention mechanism does not change when training with different language pairs because it is the same across all encoders and decoders, regardless of the language pair used."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen45-slide45/text.txt": [
        {
            "question": "What was the impact of testing the system in a simulated low-resource setting, particularly in terms of performance compared to the baseline and the number of languages used?",
            "reference-answers": [
                "Testing the system in a simulated low-resource setting showed improvements compared to the baseline, particularly when the training data was small. The impact was more pronounced with a higher number of languages used."
            ]
        },
        {
            "question": "How did the system perform in a simulated low-resource setting when tested with high-resource languages and reduced data sizes, particularly in terms of improvements compared to the baseline?",
            "reference-answers": [
                "The system showed improvements in a simulated low-resource setting when tested with high-resource languages and reduced data sizes, particularly when the training data was small. The more languages used, the more noticeable the improvements compared to the baseline."
            ]
        },
        {
            "question": "What improvements were observed when the system was tested in a simulated low resource setting, and how did the number of languages affect these results?",
            "reference-answers": [
                "Improvements were observed when the system was tested in a simulated low resource setting, particularly when the training data was small. The more languages included, the greater the improvements compared to the baseline."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen46-slide46/text.txt": [
        {
            "question": "In which year did the Roman Empire split into the Western and Eastern Roman Empires?",
            "reference-answers": [
                "The Roman Empire split into the Western and Eastern Roman Empires in 395 AD."
            ]
        },
        {
            "question": "What activity is the teacher engaged in for the exam?",
            "reference-answers": [
                "I'm unable to answer the question as no text has been provided."
            ]
        },
        {
            "question": "What instruction does the user give to the assistant in the provided text?",
            "reference-answers": [
                "The instruction is to write a question and answer pair about the photosynthesis process."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen47-slide45/text.txt": [
        {
            "question": "What did the experiments generally involve?",
            "reference-answers": [
                "The experiments generally involved... [However, the provided text does not contain enough information to specify what the experiments involved.]"
            ]
        },
        {
            "question": "What did the experiments generally involve?",
            "reference-answers": [
                "The text does not provide enough information to determine what the experiments generally involved."
            ]
        },
        {
            "question": "What did the experiments generally focus on?",
            "reference-answers": [
                "The text does not provide specific information about what the experiments focused on."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen48-slide46/text.txt": [
        {
            "question": "Which model type shows better performance when translating into English versus translating from English into other languages, such as French or German, and why?",
            "reference-answers": [
                "The multilingual model shows better performance when translating **into English**, while single paired models perform better when translating **from English into other languages** like French or German. This is because the multilingual model benefits from additional knowledge in the decoder for other language data when translating into English, but this advantage does not carry over when producing translations in other languages."
            ]
        },
        {
            "question": "Question: In which scenarios is the multilingual model more effective, and why? Explain the performance difference based on the test set BLEU scores.",
            "reference-answers": [
                "The multilingual model is more effective in scenarios where translations are into English. This is because the decoder in the multilingual model benefits from the additional knowledge of other languages, which improves performance when translating into English. However, when translating from English into other languages, the single paired models perform better as the multilingual model does not derive the same advantage in these directions."
            ]
        },
        {
            "question": "The multilingual model demonstrates superior performance when translating into English, whereas single paired models are more effective when translating from English into other languages such as French, Czech, German, Russian, or Finnish. This highlights that the multilingual model's advantage is specifically when the target language is English, likely due to its broader training data, while single paired models excel in other translation directions.",
            "reference-answers": [
                "The multilingual model demonstrates superior performance when translating into English, whereas single paired models are more effective when translating from English into other languages such as French, Czech, German, Russian, or Finnish. This highlights that the multilingual model's advantage is specifically when the target language is English, likely due to its broader training data, while single paired models excel in other translation directions."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen49-slide48/text.txt": [
        {
            "question": "What is the student's favorite subject?",
            "reference-answers": [
                "The information provided in the text does not explicitly state the student's favorite subject."
            ]
        },
        {
            "question": "What is the speaker confirming they prefer over English?",
            "reference-answers": [
                "The speaker is confirming that they prefer French over English. \n\nANSWER: French"
            ]
        },
        {
            "question": "Which subject is mentioned as being easier than English?",
            "reference-answers": [
                "Spanish"
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen50-slide47/text.txt": [
        {
            "question": "What methods can be used to process a sentence available in multiple languages using encoders and decoders?",
            "reference-answers": [
                "To process a sentence available in multiple languages using encoders and decoders, you can use the following methods:\n\n1. **Combine Context Vectors**: Use two encoders simultaneously to process the input and combine their context vectors. This combined vector can then be used by the decoder to generate the output, either by averaging the vectors or letting the decoder process the joint knowledge.\n\n2. **Run Separate Decoders with Voting**: Process the input independently through two decoders, each generating words for their respective languages. The final word is chosen by voting between the two independent setups, which is implemented via the softmax layer."
            ]
        },
        {
            "question": "What approach was used when processing input that includes multiple sources in different languages, and how does the decoder handle the combined knowledge from these sources?",
            "reference-answers": [
                "The approach involves using independent encoders and decoders for each language pair. When processing input with multiple sources in different languages, the encoders generate context vectors for each source, which are then combined. The decoder processes this joint knowledge by either averaging the context vectors or using them to modify its state. Additionally, the decoder can run separately for each source language, with the final word predicted through voting between the two independent setups."
            ]
        },
        {
            "question": "How did the experiment combine encoders and decoders to process inputs with multiple sources and languages, and what methods were used to integrate their outputs?",
            "reference-answers": [
                "The experiment combined encoders and decoders by using two encoders simultaneously to process inputs from multiple sources and languages. The context vectors from these encoders were either averaged or used together in the decoder. For decoding, the decoder could process the joint context vector, modify its state, or generate outputs by running separate decoders and voting on the results through the softmax layer."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen51-slide49/text.txt": [
        {
            "question": "How does the addition of other language data affect the performance of a model translating from Spanish to English compared to translating from English to Spanish?",
            "reference-answers": [
                "The addition of other language data improves the performance of a model translating from Spanish to English but does not enhance performance when translating from English to Spanish. Specifically, the model trained with other language data outperforms the baseline when translating into English, whereas the baseline, which only uses English-Spanish data, performs better when translating from English to Spanish."
            ]
        },
        {
            "question": "Why does the multi-language model perform better when translating into English but not when translating from English into another language?",
            "reference-answers": [
                "The multi-language model performs better when translating into English because it has been exposed to data from other languages, which helps improve translation into English. However, when translating from English to another language, such as Spanish, the additional data from other languages does not provide the same benefit, so the baseline model relying solely on English-Spanish data performs better."
            ]
        },
        {
            "question": "What is the effect of training a model with additional language data on its performance when translating into English versus translating into Spanish?",
            "reference-answers": [
                "Training a model with additional language data improves its performance when translating into English but not when translating into Spanish."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen52-slide50/text.txt": [
        {
            "question": "What is the comparative performance of early averaging and late averaging methods, and how does combining both affect the translation task?",
            "reference-answers": [
                "Early averaging alone performs worse than the baseline, while late averaging performs slightly better. Combining both methods, by using information from the source languages at the same time, results in the best performance for the translation task."
            ]
        },
        {
            "question": "What was the best setup for the translation task described?",
            "reference-answers": [
                "The best setup was using both early and late averaging of the source language knowledge together."
            ]
        },
        {
            "question": "Which combination of methods yields the best performance in the machine translation task described?",
            "reference-answers": [
                "The combination of using both early averaging and late averaging yields the best performance."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen53-slide52/text.txt": [
        {
            "question": "What approach was suggested to improve translation between a language pair?",
            "reference-answers": [
                "The approach suggested was to include additional languages to improve translation between a language pair."
            ]
        },
        {
            "question": "What was the approach mentioned for improving translation between a language pair?",
            "reference-answers": [
                "The approach mentioned was to improve translation between a language pair by including additional languages."
            ]
        },
        {
            "question": "What idea was presented to improve translation into a language pair?",
            "reference-answers": [
                "The idea presented was to improve translation into a language pair by including additional languages."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen54-slide50/text.txt": [
        {
            "question": "What is the relationship between the performance of multilingual systems and whether the target languages are shared or distinct in the context of transfer learning?",
            "reference-answers": [
                "In transfer learning, multilingual systems maintain good performance on all language pairs of interest. When target languages are shared, these systems show performance gains. However, when target languages are distinct, training independent systems is more effective, even though multilingual systems are still considered interesting and valuable."
            ]
        },
        {
            "question": "What is the impact of having shared versus distinct target languages on the performance of multilingual systems in the context of transfer learning?",
            "reference-answers": [
                "The impact of shared versus distinct target languages on the performance of multilingual systems in transfer learning is that shared target languages lead to performance gains, while distinct target languages do not, making independent training more beneficial in the latter case."
            ]
        },
        {
            "question": "The complication arises because when target languages are distinct, maintaining performance across multiple languages becomes challenging. Fine-tuning a model for one language can negatively affect its performance on others, making independent training more effective in such cases.",
            "reference-answers": [
                "The complication arises because when target languages are distinct, maintaining performance across multiple languages becomes challenging. Fine-tuning a model for one language can negatively affect its performance on others, making independent training more effective in such cases."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen55-slide52/text.txt": [
        {
            "question": "What is the main difference between pivot translation and zero-shot translation, and what is the advantage of using zero-shot translation in this context?",
            "reference-answers": [
                "The main difference is that pivot translation involves using an intermediate language (English) in two steps, while zero-shot translation directly translates between the source and target languages in a single step. The advantage of zero-shot translation is its efficiency and reduced potential for information loss or ambiguity, as it eliminates the need for intermediate steps."
            ]
        },
        {
            "question": "**Question:** What is the main advantage of zero-shot translation compared to pivot translation as described in the text?",
            "reference-answers": [
                "The main advantage of zero-shot translation compared to pivot translation is that it allows direct translation between languages without going through an intermediate step, making it faster and more efficient."
            ]
        },
        {
            "question": "How does Zero-shot translation differ from the pivot translation method, and what is the structure of the multi-way encoder-decoder system used in Zero-shot translation?",
            "reference-answers": [
                "Zero-shot translation differs from pivot translation as it avoids the intermediate step of translating through a third language like English. Instead, it directly translates from Spanish to French using a multi-way encoder-decoder system. This system is structured with a single model trained on multiple languages, enabling direct translation without the need for pivoting."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen56-slide53/text.txt": [
        {
            "question": "**Question:** Why is pivoting necessary for the neural network's performance according to the text?",
            "reference-answers": [
                "Pivoting is necessary because without it, the neural network's components operate independently, leading to poor performance. Pivoting allows the network to effectively combine its trained paths, enabling better overall performance by leveraging the shared structure and trained interactions between encoders and decoders."
            ]
        },
        {
            "question": "What issue arises when using the attention mechanism without pivoting in the neural network model described?",
            "reference-answers": [
                "The issue is that without pivoting, the attention mechanism cannot effectively combine different parts of the network, leading to poor performance because the tasks are treated as mutually independent and unable to share information."
            ]
        },
        {
            "question": "**Question:**  \nWhy does the performance of the network suffer when pivoting without explicit training?",
            "reference-answers": [
                "The performance of the network suffers when pivoting without explicit training because the tasks are mutually independent, and the network lacks the ability to share information effectively between its components. The network was trained with specific encoder-decoder pairs (e.g., English encoder with French decoder), and attempting to use it in a different configuration, such as a pivot through an unused path, results in poor performance. Without explicit training for the pivot, the network cannot adapt or combine its unused components to handle the new task, leading to inadequate results."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen57-slide54/text.txt": [
        {
            "question": "Is it possible to fine-tune the network to add missing links by using a small amount of parallel data between Spanish and French?",
            "reference-answers": [
                "Yes, it is possible to fine-tune the network to add missing links by using a small amount of parallel data between Spanish and French."
            ]
        },
        {
            "question": "Is it feasible to train the missing parts of a neural network by fine-tuning on a small amount of parallel data between Spanish and French?",
            "reference-answers": [
                "Yes, it is feasible to train the missing parts of the neural network by fine-tuning on a small amount of parallel data between Spanish and French, provided that you can obtain such data."
            ]
        },
        {
            "question": "**Question:** Where would the data come from for fine-tuning the network between Spanish and French?",
            "reference-answers": [
                "The data would come from a source that provides parallel data between Spanish and French."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen58-slide55/text.txt": [
        {
            "question": "What language does the system proceed to use after training with the Spanish-English and English-French corpora?",
            "reference-answers": [
                "The system proceeds to use English."
            ]
        },
        {
            "question": "Which corpora were used to train the multi-way system, and what was the purpose of using them?",
            "reference-answers": [
                "The Spanish-English and English-French corpora were used to train the multi-way system. The purpose was to enable multi-directional translation paths in a system that handles multiple sources and multiple targets."
            ]
        },
        {
            "question": "What does the text suggest about the use of English in the multi-way system?",
            "reference-answers": [
                "The text suggests that English serves as an intermediate or bridge language in the multi-way system, connecting different language pairs such as Spanish-English and English-French."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen59-slide56/text.txt": [
        {
            "question": "Question:  \nIs it possible to use the Spanish-English system to produce synthetic Spanish?",
            "reference-answers": [
                "Yes, it is possible to use the Spanish-English system to produce synthetic Spanish."
            ]
        },
        {
            "question": "What can be done using the Spanish-English system mentioned in the text?",
            "reference-answers": [
                "The Spanish-English system can be used to produce synthetic Spanish."
            ]
        },
        {
            "question": "What system is mentioned for producing synthetic Spanish?",
            "reference-answers": [
                "The system mentioned for producing synthetic Spanish is the Spanish-English system."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen60-slide57/text.txt": [
        {
            "question": "What was the network fine-tuned with during the described training process?",
            "reference-answers": [
                "The network was fine-tuned with pairs of sentences that go from Spanish into French."
            ]
        },
        {
            "question": "What type of data is used to fine-tune the network in the described scenario?",
            "reference-answers": [
                "The type of data used to fine-tune the network is pairs of sentences in Spanish and French."
            ]
        },
        {
            "question": "What data is used to fine-tune the network after initial training?",
            "reference-answers": [
                "The data used to fine-tune the network after initial training consists of pairs of sentences in Spanish and French. Specifically, the fine-tuning is done with data that goes from Spanish into French."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen61-slide58/text.txt": [
        {
            "question": "Which method resulted in a higher BLEU score when comparing pivoting without fine-tuning to using a synthetic parallel corpus?",
            "reference-answers": [
                "The pivoting without fine-tuning resulted in a higher BLEU score."
            ]
        },
        {
            "question": "What BLEU scores were obtained using pivoting and synthetic parallel corpus methods, and which method performed better?",
            "reference-answers": [
                "Using pivoting without fine-tuning resulted in a BLEU score of around 20, while the synthetic parallel corpus method achieved a BLEU score of up to 17. Pivoting performed better as it had a higher BLEU score."
            ]
        },
        {
            "question": "What is the BLEU score when using a synthetic parallel corpus without fine-tuning?",
            "reference-answers": [
                "The BLEU score when using a synthetic parallel corpus without fine-tuning is **17**."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen62-slide59/text.txt": [
        {
            "question": "What is the main conclusion regarding the impact of using a true parallel corpus compared to single pair models in terms of performance?",
            "reference-answers": [
                "The main conclusion is that using a true parallel corpus can reach the performance of explicit pivoting setups, but single pair models with sufficient data (like one million sentences) can perform better, even with sampling. This shows that relying on separate encoders and decoders without a large true parallel corpus doesn't yield better results, and trivial transfer learning provides similar improvements."
            ]
        },
        {
            "question": "**Question:** Does using a true parallel corpus allow us to achieve performance comparable to or slightly below that of an explicit pivoting setup, and what is the implication of using single pair baseline models with one million sentences in terms of performance?",
            "reference-answers": [
                "Using a true parallel corpus allows us to achieve performance comparable to or slightly below that of an explicit pivoting setup. However, single pair baseline models with one million sentences, even with sampling, perform better. This implies that the amount of data in single pair setups can lead to better performance compared to true parallel corpora, especially when the latter is relatively small."
            ]
        },
        {
            "question": "How does the use of trivial transfer learning compare to using a true parallel corpus in terms of performance improvement?",
            "reference-answers": [
                "The use of trivial transfer learning provides an equally significant improvement compared to using a true parallel corpus, as both approaches yield notable performance gains."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen63-slide60/text.txt": [
        {
            "question": "What is the key method described for training a multilingual translation model using a single Encoder-Decoder structure?",
            "reference-answers": [
                "The key method is using a single Encoder-Decoder model where the encoder processes multiple source languages by mixing them in the training data, and a language token is added to the source sentence to specify the target language, enabling the model to translate into the desired language without altering its structure."
            ]
        },
        {
            "question": "What is the main method described for training a model to handle multiple languages without increasing the number of parameters?",
            "reference-answers": [
                "The main method described is using a single encoder and decoder model with a language token added to specify the target language, allowing the model to handle multiple languages without increasing the number of parameters."
            ]
        },
        {
            "question": "What is used to indicate the desired target language in the model during training?",
            "reference-answers": [
                "A language token is used to indicate the desired target language in the model during training."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen64-slide61/text.txt": [
        {
            "question": "What was the main advantage of the multilingual system used in the Google experiment from 2016?",
            "reference-answers": [
                "The main advantage of the multilingual system used in the Google experiment from 2016 was hardware cost-saving by having one model for multiple languages, which allowed translation into English from different languages."
            ]
        },
        {
            "question": "What was the main benefit of the 2016 Google multilingual translation system experiment?",
            "reference-answers": [
                "The main benefit of the 2016 Google multilingual translation system experiment was hardware cost-saving, as it allowed one model to handle multiple languages, and it improved translation accuracy into English, particularly when translating from other languages."
            ]
        },
        {
            "question": "What improvement was noted when translating into English with the multilingual system?",
            "reference-answers": [
                "Improvement was noted in the blind scores when translating into English with the multilingual system."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen65-slide62/text.txt": [
        {
            "question": "What was the outcome of using English-German training data for English-French translation in their experiments?",
            "reference-answers": [
                "The outcome was a significant decrease in performance, as the system tried to produce German words in French, leading to a big loss in translation quality. Additionally, the imbalance in corpus sizes between French and German datasets contributed to this issue, causing a loss in either language's performance."
            ]
        },
        {
            "question": "What issue arises when using English-German training data for English-French translation, and how does the imbalance in corpus sizes influence the system's performance?",
            "reference-answers": [
                "When using English-German training data for English-French translation, the system may attempt to produce German words in French, which significantly decreases performance. Additionally, the imbalance in corpus sizes between French and German corpora can skew the distribution, leading to a loss in performance in one of the languages."
            ]
        },
        {
            "question": "What is the impact of using multilingual training data on translation performance, and how does it affect the outcomes when the source and target languages are different?",
            "reference-answers": [
                "Using multilingual training data for translation can sometimes help but often significantly decreases performance, especially when the source and target languages are different. For example, using English-German training data for English-French translation can lead to poor performance because the model may produce German words in French. Additionally, the sizes of the corpora for different languages can cause imbalances, skewing the distribution and negatively impacting the performance of one language. Overall, these experiments have not been very promising, as they often result in a loss in performance for at least one language."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen66-slide61/text.txt": [
        {
            "question": "What is the capability of a model when handling multiple source languages while maintaining the same target language?",
            "reference-answers": [
                "The model can handle multiple source languages while maintaining the same target language without losing translation quality."
            ]
        },
        {
            "question": "Question: Can a teacher feed multiple source languages into the same model as long as the target language remains the same without losing translation quality?",
            "reference-answers": [
                "Yes, you can feed multiple source languages into the same model as long as the target language remains the same, without losing translation quality."
            ]
        },
        {
            "question": "Does a translation model maintain quality when fed multiple source languages with the same target?",
            "reference-answers": [
                "Yes, a translation model maintains quality when fed multiple source languages with the same target, as long as the target remains consistent."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen67-slide62/text.txt": [
        {
            "question": "What is the reason for loss when feeding different language pairs into the same model, and what role does the first token of the input sentence play in this context?",
            "reference-answers": [
                "The loss occurs when feeding different language pairs into the same model because the model is likely not aware of which target language to predict for each input. The first token of the input sentence plays a crucial role by indicating which language the model should translate into, thereby helping to address this issue. Without this token, the model would struggle to handle multiple language pairs effectively."
            ]
        },
        {
            "question": "What does the first token of the input sentence indicate in the context of the model?",
            "reference-answers": [
                "The first token of the input sentence indicates which language we want to translate into."
            ]
        },
        {
            "question": "What does the first token of the input sentence indicate in the context of language translation?",
            "reference-answers": [
                "The first token of the input sentence indicates which language we want to translate into."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen68-slide63/text.txt": [
        {
            "question": "What was the system trained on?",
            "reference-answers": [
                "The system was trained on combinations of each of these languages with English."
            ]
        },
        {
            "question": "Who developed the recurrent neural network system in 2018 that was trained on Bible translations in nearly 1000 languages?",
            "reference-answers": [
                "Jörg Tiedemann"
            ]
        },
        {
            "question": "What was the primary characteristic of the dataset used in Jörg Tiedemann's 2018 recurrent neural network system for Bible translation?",
            "reference-answers": [
                "The primary characteristic of the dataset was that it consisted of translations of the Bible into nearly 1000 languages, specifically 900 languages, making it very limited in content but extensive in language coverage."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen69-slide64/text.txt": [
        {
            "question": "What method was used to visualize the language embeddings, and which language families formed distinct clusters?",
            "reference-answers": [
                "The method used to visualize the language embeddings was T-SNE. The distinct clusters formed include Indo-European, Austronesian, Afro-Zero-Quechua, Creole, and Quechua."
            ]
        },
        {
            "question": "What does the text indicate about the use of T-SNE in relation to language families?",
            "reference-answers": [
                "The text indicates that T-SNE is used to project language embeddings into a two-dimensional vector space, which helps visualize and identify clusters corresponding to language families. This method empirically reveals the grouping of languages into their respective families, such as Indo-European, Niger-Congo, Austronesian, Afro-Zero-Quechua, and Creole, with Quechua forming a distinct cluster."
            ]
        },
        {
            "question": "What does the text suggest about the ability of T-SNE to identify language family clusters?",
            "reference-answers": [
                "The text suggests that T-SNE is able to identify language family clusters, as the embeddings projected into a two-dimensional vector space using T-SNE show distinct groupings corresponding to language families such as Indo-European, Niger-Congo, Austronesian, Afro-Zero-Quechua, and Creole languages, with Quechua forming a specific cluster."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen70-slide63/text.txt": [
        {
            "question": "What is the model's ability to cluster languages, given that it does not translate well but can do so in an unsupervised manner?",
            "reference-answers": [
                "The model has the ability to cluster languages in an unsupervised manner, even though it does not translate well."
            ]
        },
        {
            "question": "What does the text suggest about the ability of the described method to cluster languages without supervision?",
            "reference-answers": [
                "The text suggests that the method can successfully cluster languages in an unsupervised manner, even though it does not translate well."
            ]
        },
        {
            "question": "Question: Can the model cluster languages in an unsupervised way despite its limited translation ability?",
            "reference-answers": [
                "Yes, the model can cluster languages in an unsupervised way despite its limited translation ability."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen71-slide64/text.txt": [
        {
            "question": "Question: In what way are linguists' methods similar to those of another field, as described in the text?",
            "reference-answers": [
                "Linguists' methods are similar to those of biologists in their approach to studying complex systems, using empirical analysis and systematic observation."
            ]
        },
        {
            "question": "In what way are linguists' methods similar to those of another field?",
            "reference-answers": [
                "Linguists use methods similar to those of biologists when studying systems and patterns."
            ]
        },
        {
            "question": "What method do linguists use to analyze language structures, as described in the text?",
            "reference-answers": [
                "The provided text is incomplete and does not contain enough information to determine the method linguists use to analyze language structures."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen72-slide65/text.txt": [
        {
            "question": "What did Google find about using high-resource language data to improve multilingual systems for low-resource languages?",
            "reference-answers": [
                "Google found that using high-resource language data improved multilingual systems specifically for low-resource languages, while high-resource languages in the mix did not benefit."
            ]
        },
        {
            "question": "What did Google's recent research reveal about the effectiveness of multilingual systems, particularly regarding the use of data from high-resource languages to improve low-resource languages?",
            "reference-answers": [
                "Google's recent research revealed that while combining high-resource languages in multilingual systems did not yield any improvements, using data from high-resource languages effectively improved the performance of low-resource languages."
            ]
        },
        {
            "question": "What method did Google use to improve the performance of low-resource languages in their multilingual systems by leveraging data from high-resource languages?",
            "reference-answers": [
                "Google used the data from high-resource languages to improve the performance of low-resource languages in their multilingual systems."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen73-slide66/text.txt": [
        {
            "question": "What is the range of data sizes used in the experiment conducted by Orhan Ferrat and his colleagues, considering the parallel corpora for 102 languages?",
            "reference-answers": [
                "The range of data sizes used in the experiment is from billions of word pairs to just dozens of thousands of sentence pairs."
            ]
        },
        {
            "question": "What did Orhan Ferrat and his team experiment with in terms of data distribution across languages?",
            "reference-answers": [
                "Orhan Ferrat and his team experimented with a distribution of parallel corpora across 102 languages, where the data sizes vary significantly, ranging from billions of word pairs to just tens of thousands of sentence pairs."
            ]
        },
        {
            "question": "What is the range of data sizes for the parallel corpora used by Orhan Ferrat and his colleagues for 102 languages?",
            "reference-answers": [
                "The range of data sizes for the parallel corpora used by Orhan Ferrat and his colleagues for 102 languages is from tens of thousands to billions of word pairs."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen74-slide67/text.txt": [
        {
            "question": "How does the translation quality compare between high-resource and low-resource languages according to the text?",
            "reference-answers": [
                "The translation quality is better for high-resource languages than for low-resource languages."
            ]
        },
        {
            "question": "What does the text suggest about the comparison of translation quality between high-resource and low-resource languages?",
            "reference-answers": [
                "The text suggests that high-resource languages generally have better translation quality compared to low-resource languages, likely due to human judgments used in measuring the quality."
            ]
        },
        {
            "question": "Question: Why is it expected that translation quality is better for high-resource languages than for low-resource languages?",
            "reference-answers": [
                "It is expected that translation quality is better for high-resource languages than for low-resource languages because high-resource languages typically have more data and better tools available, which generally lead to improved translation outcomes. Additionally, BLEU scores, which are often used to measure translation quality, are not directly comparable across languages, suggesting that human evaluations likely play a role in assessing these differences."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen75-slide68/text.txt": [
        {
            "question": "What are the two main components of each encoder and decoder layer in the transformer setup?",
            "reference-answers": [
                "The two main components of each encoder and decoder layer in the transformer setup are:  \n1. **Multihead attention** (in the encoder) or **self-attention and encoder-decoder attention** (in the decoder).  \n2. **Feedforward network**."
            ]
        },
        {
            "question": "How many sub-layers do the encoder and decoder layers have, and what do these sub-layers consist of?",
            "reference-answers": [
                "The encoder and decoder layers each consist of two sub-layers. In the encoder, the sub-layers are multi-head attention and a feedforward network. In the decoder, the sub-layers are self-attention and encoder-decoder attention, with the feedforward network also being part of the processing."
            ]
        },
        {
            "question": "Question: How is the transformer model applied in multi-language setups, and what are its key components and advantages over recurrent neural networks in this context?",
            "reference-answers": [
                "The transformer model is applied in multi-language setups by utilizing its encoder-decoder structure, which processes input sequences through multiple layers of attention mechanisms and feedforward networks. The key components include:\n\n1. **Encoder**: Consists of multiple layers with multi-head attention, allowing the model to consider multiple perspectives of the input. Each layer also includes a feedforward network that processes and refines the representations.\n\n2. **Decoder**: Features self-attention to capture dependencies within the target sequence and encoder-decoder attention to utilize the source sequence information. Each decoder layer also has a feedforward network for enhancing representations.\n\nThe transformer's advantages over recurrent neural networks (RNNs) in multi-language setups include:\n\n- **Efficient Processing**: The transformer's parallel processing capabilities allow it to handle long-range dependencies more effectively than RNNs, which process sequentially and can struggle with long sequences.\n\n- **Scalability**: The model can be scaled to handle multiple languages more efficiently, making it suitable for scenarios with diverse or limited resources.\n\n- **Resource Utilization**: Unlike RNNs, which require significant computational resources and sequential processing, the transformer's structure allows for more efficient use of resources, particularly in handling high-resource languages and multiple language pairs."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen76-slide69/text.txt": [
        {
            "question": "What is the key difference between a \"wide\" and a \"deep\" transformer network, and why does a deep network generally perform better on low-resource languages?",
            "reference-answers": [
                "The key difference between a \"wide\" and a \"deep\" transformer network lies in how their parameters are allocated. A \"wide\" network increases the number of parallel processes (heads) and the dimensionality of the feedforward layers, resulting in a broader architecture. In contrast, a \"deep\" network increases the number of layers, creating a more extended and deeper structure. \n\nA deep network generally performs better on low-resource languages because its increased depth allows it more layers to generalize and construct more robust representations. This deeper structure enables the model to better capture patterns and generalize across languages, especially when data is scarce, leading to superior performance on low-resource languages."
            ]
        },
        {
            "question": "What is the largest transformer setup discussed in the text, and what are its key characteristics in terms of the number of layers, heads, and parameters?",
            "reference-answers": [
                "The largest transformer setup discussed in the text has 64 layers in both the encoder and decoder, totaling 128 layers, 32 attention heads, and contains 6 billion parameters."
            ]
        },
        {
            "question": "What is the reason that deep transformer networks outperform wide ones, especially on low-resource languages?",
            "reference-answers": [
                "Deep transformer networks outperform wide ones, especially on low-resource languages, because they have more layers, allowing the network more time to generalize and create a more generic representation, which is particularly beneficial when data is limited."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen77-slide70/text.txt": [
        {
            "question": "\"According to the text, what is the benefit of using a multilingual model compared to pairwise systems, and at what point is it better to use multilingual models instead of separate pairwise models?\"",
            "reference-answers": [
                "The benefit of using a multilingual model compared to pairwise systems is that it provides better performance for low-resource languages. Multilingual models are advantageous when the languages have limited data, as they outperform the pairwise systems in such cases. It is better to use multilingual models instead of separate pairwise models when the languages become low-resource compared to the average, meaning when they have insufficient data relative to others."
            ]
        },
        {
            "question": "What is the impact of using a multilingual model compared to a pairwise model for languages with limited resources?",
            "reference-answers": [
                "The multilingual model outperforms the pairwise model for languages with limited resources, as shown by the improved BLUE scores. High-resource languages, however, may not benefit and could even perform worse compared to the baseline pairwise models."
            ]
        },
        {
            "question": "At what point does the switch from using separate pairwise models to a multilingual model become beneficial, particularly in terms of data resource levels?",
            "reference-answers": [
                "The switch from using separate pairwise models to a multilingual model becomes beneficial when the languages are low resource compared to the average."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen78-slide71/text.txt": [
        {
            "question": "What effect does increasing the number of parameters have on the performance of the model, particularly on high-resource language pairs?",
            "reference-answers": [
                "Increasing the number of parameters improves the model's performance, particularly on high-resource language pairs. With more parameters, the model maintains or enhances its performance, as seen when scaling from 400 million to 50 billion parameters, achieving better results on high-resource language pairs."
            ]
        },
        {
            "question": "What performance improvements are observed on high-resource language pairs when the model size is increased from 400 million parameters to 50 billion parameters, and what role do 'further tricks' play in this?",
            "reference-answers": [
                "When the model size is increased from 400 million parameters to 50 billion parameters, performance on high-resource language pairs improves significantly. Specifically, moving to 6 billion parameters avoids performance loss compared to the standard 400 million parameter model. Further tricks, such as optimization techniques or architectural adjustments, play a crucial role in achieving even greater gains, enabling the model to reach five black points on high-resource language pairs when scaled up to 50 billion parameters."
            ]
        },
        {
            "question": "What parameter size allows the model to avoid loss on high-resource languages according to the text?",
            "reference-answers": [
                "6hM"
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen79-slide72/text.txt": [
        {
            "question": "The main bottleneck when using a large model with multiple target languages is that if you try to include too many target languages without increasing the model's capacity, the high-resource languages will lose performance compared to when they are trained individually.",
            "reference-answers": [
                "The main bottleneck when using a large model with multiple target languages is that if you try to include too many target languages without increasing the model's capacity, the high-resource languages will lose performance compared to when they are trained individually."
            ]
        },
        {
            "question": "What is the advantage of using a gating network with experts in a large transformer model?",
            "reference-answers": [
                "The advantage of using a gating network with experts in a large transformer model is that it allows for efficient use of parameters by only utilizing a subset of experts for each input, reducing computational load and making the model more manageable on GPUs. This approach enables the model to scale up significantly, such as reaching 50 billion parameters, while allowing experts to specialize and share the workload effectively, especially when handling multiple languages. This specialization can improve performance without overwhelming the model, even when dealing with low-resource languages."
            ]
        },
        {
            "question": "The trade-off when using a large model with a mixture of experts for multiple languages involves balancing model capacity against the number of languages to maintain performance. While increasing the model size can improve performance, adding too many languages without scaling the model capacity can lead to degraded performance, particularly for high-resource languages that may require more focused training. Low-resource languages might not fully utilize the model's capacity due to limited data. Thus, the trade-off involves optimizing the model's size and the number of languages to ensure effective performance across all included languages.",
            "reference-answers": [
                "The trade-off involves balancing model capacity and the number of languages to maintain performance. Increasing the model size can improve performance, but adding too many languages without scaling the model capacity can degrade performance, particularly for high-resource languages that require more focused training. Low-resource languages might not fully utilize the model's capacity due to limited data. Thus, the trade-off involves optimizing the model's size and the number of languages to ensure effective performance across all included languages."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen80-slide73/text.txt": [
        {
            "question": "Which technique involves using a main pre-trained network with additional tunable sublayers (adapters) specifically trained for different language pairs to reduce model size and enable efficient translation?",
            "reference-answers": [
                "The technique involves using a main pre-trained network with additional tunable sublayers (adapters) specifically trained for different language pairs to reduce model size and enable efficient translation."
            ]
        },
        {
            "question": "Orhan Ferden and his colleagues reduced the model size by introducing tunable sublayers, which act as language-specific experts. These sublayers, or adapter layers, are trained in a supervised manner for particular language pairs, while the majority of the network remains pre-trained on a large mixed corpus of main languages. This approach allows the model to efficiently handle different language pairs during runtime without significantly increasing the overall model size.",
            "reference-answers": [
                "Orhan Ferden and his colleagues reduced the model size by introducing tunable sublayers, which act as language-specific experts. These sublayers, or adapter layers, are trained in a supervised manner for particular language pairs, while the majority of the network remains pre-trained on a large mixed corpus of main languages. This approach allows the model to efficiently handle different language pairs during runtime without significantly increasing the overall model size."
            ]
        },
        {
            "question": "What is the role of the tunable sublayers in the model?",
            "reference-answers": [
                "The tunable sublayers serve as adapter layers that allow the model to adapt to specific language pairs during fine-tuning and runtime. Most of the network remains pre-trained on a large corpus, while these sublayers are trained for particular language pairs, enabling efficient translation without significantly changing the main network."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen81-slide74/text.txt": [
        {
            "question": "Question: Why does the multilingual system underperform compared to the bilingual baselines, and how do adapters help in this situation?",
            "reference-answers": [
                "The multilingual system underperforms compared to the bilingual baselines because the high-resource languages take up most of the model's capacity, leaving less for the low-resource languages. Adapters help by allowing the model to adjust to each language without changing its core parameters, thus retaining performance on low-resource languages."
            ]
        },
        {
            "question": "Why does the multilingual system underperform compared to the bilingual baselines, and how do adapters address this issue?",
            "reference-answers": [
                "The multilingual system underperforms compared to the bilingual baselines because the high-resource languages dominate the model's capacity, leaving insufficient space for low-resource languages. Adapters address this by allowing the model to adapt to each language more effectively, retaining performance on low-resource languages without altering the model's core structure."
            ]
        },
        {
            "question": "**Question:**  \nWhy does the multilingual system underperform compared to the bilingual baselines, and how do adapters help reduce this performance gap when translating into English?",
            "reference-answers": [
                "The multilingual system underperforms compared to the bilingual baselines because the high-resource languages exceed the model's capacity. Adapters help reduce the performance gap by allowing the model to maintain its effectiveness on low-resource languages when translating into English."
            ]
        }
    ],
    "nmt-class/lecture10-multilingual-mt/screen82-slide75/text.txt": [
        {
            "question": "What is the significance of adapters in the context of translating out of English?",
            "reference-answers": [
                "Adapters are significant in translating out of English because they help bridge the gap between the shared source language (English) and the different target languages, ensuring the translation is more accurate and suitable for each specific target."
            ]
        },
        {
            "question": "What is the importance of adapters when translating out of English?",
            "reference-answers": [
                "Adapters are more prominent when translating out of English because they help bridge the gap between the source language (English) and the target language, accounting for linguistic and cultural differences."
            ]
        },
        {
            "question": "What is mentioned as being more prominent when translating out of English?",
            "reference-answers": [
                "The importance of the adapters is more prominent."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen01-slide01/text.txt": [
        {
            "question": "What was the main focus of the last lecture on statistical machine translation?",
            "reference-answers": [
                "The main focus of the last lecture on statistical machine translation was on multimodal machine translation, which includes speech and vision as additional aspects of the translation."
            ]
        },
        {
            "question": "What subject was the last lecture of the semester focused on?",
            "reference-answers": [
                "The last lecture of the semester was focused on statistical machine translation, specifically its multimodal aspects, which include speech and vision."
            ]
        },
        {
            "question": "What is the main topic of the lecture?",
            "reference-answers": [
                "The main topic of the lecture is statistical machine translation, specifically focusing on multimodal aspects that include speech and vision."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen02-slide02/text.txt": [
        {
            "question": "What are the two main approaches discussed in the lecture for speech translation?",
            "reference-answers": [
                "The two main approaches discussed in the lecture for speech translation are:\n\n1. The integration of two independent systems: speech recognition and machine translation.\n2. End-to-end approaches using a single deep neural network to model the entire task from source language speech to target language text."
            ]
        },
        {
            "question": "What are the two main approaches discussed in the lecture for speech translation, and what role does the survey by Sulubacic and colleagues play in this context?",
            "reference-answers": [
                "The two main approaches discussed in the lecture for speech translation are: (1) the traditional approach of integrating independent speech recognition and machine translation systems, and (2) recent end-to-end approaches where the entire task of translating source language speech to target language text is modeled in one step using deep neural networks. The survey by Sulubacic and colleagues plays a key role as it provides the most recent overview of multimodal approaches to machine translation, serving as a valuable resource for understanding the field, particularly regarding data considerations, even though the lecture does not cover all its aspects."
            ]
        },
        {
            "question": "The recent survey on multimodal approaches to machine translation mentioned in the lecture is by Sulubacic and colleagues.",
            "reference-answers": [
                "The recent survey on multimodal approaches to machine translation mentioned in the lecture is by Sulubacic and colleagues."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen03-slide03/text.txt": [
        {
            "question": "What method has been used so far to translate the text into the target language?",
            "reference-answers": [
                "Machine translation has been used so far to translate the text into the target language."
            ]
        },
        {
            "question": "What method have we been using to translate text from the source language to the target language?",
            "reference-answers": [
                "We have been using machine translation to translate text from the source language to the target language."
            ]
        },
        {
            "question": "What method has been used to translate the text into the target language?",
            "reference-answers": [
                "Machine translation has been used to translate the text into the target language."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen04-slide04/text.txt": [
        {
            "question": "What technology is required to convert speech into text for the machine translation system when speech is added to the source side?",
            "reference-answers": [
                "Speech-to-text technology"
            ]
        },
        {
            "question": "What is the primary function of the technology mentioned in the text?",
            "reference-answers": [
                "The primary function is to convert speech into text for the machine translation system."
            ]
        },
        {
            "question": "What is the purpose of adding speech on the source side in the machine translation system?",
            "reference-answers": [
                "The purpose is to convert speech into text so the machine translation system can process it."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen05-slide05/text.txt": [
        {
            "question": "What type of translation system is described in the text, where a device worn in the ear acts as a Babel fish, converting speech directly into another language without intermediate text?",
            "reference-answers": [
                "The type of translation system described is a speech-to-speech translation system."
            ]
        },
        {
            "question": "What analogy is used in the text to describe the system that converts text back into speech for translation?",
            "reference-answers": [
                "The analogy used is the Babel fish."
            ]
        },
        {
            "question": "What is the next step people may want to do after converting speech to text?",
            "reference-answers": [
                "The next step people may want to do after converting speech to text is converting it back into speech, which would be a speech-to-speech translation."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen06-slide06/text.txt": [
        {
            "question": "What direct method have researchers explored to address the issues with the cascade in speech-to-text translation?",
            "reference-answers": [
                "Researchers have explored a direct method referred to as spoken language translation, where the process goes from speech in the source language directly to text in the target language without using explicit transcription in the middle step."
            ]
        },
        {
            "question": "What issue does the cascade method bring, and what alternative method is proposed to avoid it?",
            "reference-answers": [
                "The cascade method brings the issue of information loss at the boundaries of the cascade. The proposed alternative method to avoid this is spoken language translation, which directly translates speech from the source language to text in the target language without using explicit transcription in the middle step."
            ]
        },
        {
            "question": "What direct method is mentioned for avoiding the loss of information at the boundaries of the cascade in speech translation?",
            "reference-answers": [
                "The direct method mentioned for avoiding the loss of information at the boundaries of the cascade in speech translation is **spoken language translation**."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen07-slide07/text.txt": [
        {
            "question": "What is the author contrasting with machine translation in the context of spoken language translation?",
            "reference-answers": [
                "The author is contrasting the combination of ASR and MT with a direct speech-to-speech system in the context of spoken language translation."
            ]
        },
        {
            "question": "What system is the teacher contrasting with machine translation in their discussion about spoken language translation?",
            "reference-answers": [
                "The teacher is contrasting machine translation (MT) with a combination of ASR (Automatic Speech Recognition) and MT, specifically a speech-to-speech system."
            ]
        },
        {
            "question": "What is the teacher's main focus when discussing spoken language translation, and how does it compare to the combination of ASR and MT?",
            "reference-answers": [
                "The teacher's main focus is on machine translation (MT) and contrasting it with the combination of ASR (Automatic Speech Recognition) and MT in the context of spoken language translation. The teacher emphasizes MT while acknowledging the role of ASR-MT systems, but they do not delve into ASR or text-to-speech technologies."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen08-slide08/text.txt": [
        {
            "question": "What is the purpose of the image captioning task as described in the text?",
            "reference-answers": [
                "The purpose of the image captioning task is to automatically describe what is in the image with text in the source language, making it a monolingual task."
            ]
        },
        {
            "question": "What are the two ways images can be used in language tasks, and what types of tasks do they involve according to the text?",
            "reference-answers": [
                "The two ways images can be used in language tasks are:  \n1. **Image Captioning**: Automatically describing what is in the image with text in the source language, which is a monolingual task.  \n2. **Image-Guided Translation**: Using the image to assist in translation tasks."
            ]
        },
        {
            "question": "What are the two main ways images can be used in relation to the source text?",
            "reference-answers": [
                "The two main ways images can be used in relation to the source text are image captioning and image-guided translation."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen09-slide09/text.txt": [
        {
            "question": "What does visual guidance offer during the translation process according to the text?",
            "reference-answers": [
                "Visual guidance offers the inclusion of related visual information during the translation process, which helps in better choosing the target language content."
            ]
        },
        {
            "question": "What emerging technology related to translation is mentioned, and what is its current state?",
            "reference-answers": [
                "The emerging technology mentioned is visual or video-guided translation, which involves using visual information such as speech from a video or a sequence of related images to aid in the translation process. Currently, this technology is in its early stages, with only initial attempts seen so far, and it is expected to develop further in the coming years."
            ]
        },
        {
            "question": "What is the purpose of incorporating visual or video guidance in translation as discussed?",
            "reference-answers": [
                "The purpose of incorporating visual or video guidance in translation is to better choose the target by including additional context from related visual or video content, thereby enhancing the translation process."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen10-slide11/text.txt": [
        {
            "question": "What is the typical input for machine translation in this context?",
            "reference-answers": [
                "The typical input for machine translation in this context is grammatically correct individual sentences."
            ]
        },
        {
            "question": "What is the typical input for machine translation in the context of spoken language translation?",
            "reference-answers": [
                "The typical input for machine translation in the context of spoken language translation is usually grammatically correct individual sentences."
            ]
        },
        {
            "question": "What type of input is typically used for machine translation in this context?",
            "reference-answers": [
                "The input is typically grammatically correct individual sentences, and sometimes whole documents."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen11-slide12/text.txt": [
        {
            "question": "What is the primary reason incremental machine translation aims to provide stable output, and how does it address the issue of standard machine translation reordering words that could lead to instability?",
            "reference-answers": [
                "The primary reason incremental machine translation aims to provide stable output is to avoid the instability caused by standard machine translation reordering words in any way, which can lead to sudden and significant changes in the translation as new words arrive. Incremental machine translation addresses this by processing input incrementally, allowing it to decide whether to wait for more words or emit the current translation, thus maintaining a stable output that does not drastically change once partial results are shown to the user."
            ]
        },
        {
            "question": "**Question:**  \nWhat is the main aim of incremental machine translation?",
            "reference-answers": [
                "The main aim of incremental machine translation is to provide a stable output, ensuring that the translation remains consistent as new words are added, preventing sudden and distracting changes."
            ]
        },
        {
            "question": "What is the main aim of incremental machine translation?",
            "reference-answers": [
                "The main aim of incremental machine translation is to provide the stability of the output, ensuring it does not change in a distracting way as new information comes in."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen12-slide13/text.txt": [
        {
            "question": "What is the primary advantage of direct speech-to-speech translation as discussed in the text?",
            "reference-answers": [
                "The primary advantage of direct speech-to-speech translation is that it allows for the direct modeling of speech and the preservation of voice, tone, and prosody, which would be difficult to maintain through text-based translation followed by voice synthesis."
            ]
        },
        {
            "question": "What is the main advantage of using a direct modeling approach with a deep neural network in speech-to-speech translation?",
            "reference-answers": [
                "The main advantage of using a direct modeling approach with a deep neural network in speech-to-speech translation is that it preserves voice, tone, and prosodic features, which are crucial for accurate and natural translation. This approach allows the model to handle all language differences and commonalities directly, without the need to separately manage non-verbal aspects through text-based methods."
            ]
        },
        {
            "question": "What is the main advantage of speech-to-speech translation over speech-to-text followed by text-to-speech?",
            "reference-answers": [
                "The main advantage is that speech-to-speech translation directly models the conversion from source to target language, preserving voice, tone, and prosody without losing these features through intermediate text processing."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen13-slide14/text.txt": [
        {
            "question": "What are the two components connected in the cascading machine spoken language translation system?",
            "reference-answers": [
                "The two components connected in the cascading machine spoken language translation system are speech recognition and machine translation."
            ]
        },
        {
            "question": "What are the two main components used in the cascading machine spoken language translation system?",
            "reference-answers": [
                "- Speech recognition  \n- Machine translation"
            ]
        },
        {
            "question": "What are the two components connected in the cascading machine spoken language translation system?",
            "reference-answers": [
                "The two components connected in the cascading machine spoken language translation system are speech recognition and machine translation."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen14-slide15/text.txt": [
        {
            "question": "What milestone did speech recognition systems achieve around 2017 after the introduction of deep neural networks and subsequent developments?",
            "reference-answers": [
                "Speech recognition systems achieved human-level performance around 2017 after the introduction of deep neural networks and subsequent developments."
            ]
        },
        {
            "question": "What key developments led to speech recognition achieving human-level performance around 2017?",
            "reference-answers": [
                "The key developments that led to speech recognition achieving human-level performance around 2017 were the introduction of deep neural networks, along with the use of more data and new algorithms."
            ]
        },
        {
            "question": "The standard measure for evaluating the performance of speech recognition systems, as mentioned in the text, is the word error rate.",
            "reference-answers": [
                "The standard measure for evaluating the performance of speech recognition systems, as mentioned in the text, is the word error rate."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen15-slide16/text.txt": [
        {
            "question": "What is the main idea discussed about machine translation in the text?",
            "reference-answers": [
                "The main idea is that machine translation is becoming comparable to human translation in certain settings and evaluation methods."
            ]
        },
        {
            "question": "What has the discussion highlighted about machine translation in certain settings and evaluation methods?",
            "reference-answers": [
                "The discussion has highlighted that machine translation is getting on par with humans in some settings and in some evaluation ways."
            ]
        },
        {
            "question": "The text suggests that machine translation is capable of matching human performance in certain settings and evaluation methods. When considering KVS in evaluation, it implies that integrating this framework can further enhance the capabilities of machine translation in these contexts.",
            "reference-answers": [
                "The text suggests that machine translation is capable of matching human performance in certain settings and evaluation methods. When considering KVS in evaluation, it implies that integrating this framework can further enhance the capabilities of machine translation in these contexts."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen16-slide17/text.txt": [
        {
            "question": "What are the key steps mentioned for creating a great speech translation system?",
            "reference-answers": [
                "The key steps mentioned for creating a great speech translation system are running ASR (Automatic Speech Recognition) on the sound, followed by running machine translation, and then performing superhuman interpretation of the translated content."
            ]
        },
        {
            "question": "What does the speech translation system involve?",
            "reference-answers": [
                "The speech translation system involves running ASR (Automatic Speech Recognition) on the sound to be translated, followed by machine translation, with a focus on superhuman interpretation of the spoken content."
            ]
        },
        {
            "question": "What is the key component mentioned for the speech translation system in the text?",
            "reference-answers": [
                "ASR"
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen17-slide18/text.txt": [
        {
            "question": "What is the main challenge in using ASR systems and machine translation for translating sequences of words into correct and clear sentences?",
            "reference-answers": [
                "The main challenge is that while ASR systems produce lowercase words and machine translation focuses on individual sentences, converting a sequence of words into correct and clear sentences remains a complex and non-straightforward step."
            ]
        },
        {
            "question": "What is the main challenge mentioned in the text regarding the process of converting a sequence of words into correct and clear sentences in the context of machine translation?",
            "reference-answers": [
                "The main challenge is the difficulty in converting the sequence of words into correct and clear sentences, despite the focus of ASR systems and machine translation on individual sentences."
            ]
        },
        {
            "question": "What is one issue discussed regarding the transition from a sequence of words to correct, clear sentences in the context of machine translation?",
            "reference-answers": [
                "The issue discussed is that while ASR systems and machine translation focus on producing sequences of words and translating individual sentences, respectively, the transition from these sequences to correct, clear, and well-formed sentences remains a challenging and non-straightforward step."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen18-slide19/text.txt": [
        {
            "question": "What action is required to process the flow of words as described?",
            "reference-answers": [
                "segmentation"
            ]
        },
        {
            "question": "What does one need to do with the flow of words?",
            "reference-answers": [
                "Segment the flow of words."
            ]
        },
        {
            "question": "What action must the teacher take regarding the flow of words?",
            "reference-answers": [
                "The teacher must segment the flow of words."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen19-slide20/text.txt": [
        {
            "question": "What example is given in the text to illustrate how ambiguity in ASR outputs can lead to translation errors?",
            "reference-answers": [
                "The example given is that the ASR might misinterpret the word \"ice cream\" as either referring to the delicious cold treat eaten in summer or as indicating that someone is screaming because they are afraid. This ambiguity can lead to translation errors if the ASR outputs the wrong interpretation, which is then used as the basis for translation."
            ]
        },
        {
            "question": "What is the potential issue when ASR outputs uncertain results, and how can it affect machine translation?",
            "reference-answers": [
                "When ASR outputs uncertain results, it can lead to ambiguity in understanding the input. For example, the word \"ice cream\" could be misinterpreted as \"I scream.\" This uncertainty forces the ASR system to guess the correct output without access to further context. Machine translation then relies on this potentially incorrect candidate, translating it into the target language. If the ASR outputs the wrong candidate, the translation will be misleading or confusing. If the ASR could provide multiple uncertain outputs (like an NBEST list), the risk of the user receiving a completely confusing message would be lower."
            ]
        },
        {
            "question": "What is the risk of the ASR system guessing an ambiguous sound without access to further context?",
            "reference-answers": [
                "The risk is that without access to further context, the ASR system might guess the wrong interpretation of an ambiguous sound, leading to an incorrect translation and potentially confusing the user."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen20-slide21/text.txt": [
        {
            "question": "What are the two main issues at the beginning of the pipeline when deploying the system, and why is it important to present stable and correct output to the user?",
            "reference-answers": [
                "The two main issues at the beginning of the pipeline when deploying the system are: (1) acquiring the sound and (2) presenting the output to the user. It is important to present stable and correct output to the user because exposing them to all possible candidates the system is considering can overload or confuse them. Providing reliable and accurate results as soon as possible ensures a better user experience and avoids overwhelming the user with too much information."
            ]
        },
        {
            "question": "What is a key issue in deploying the system, and how does it involve the acquisition of sound and the presentation of output?",
            "reference-answers": [
                "A key issue in deploying the system is balancing the acquisition of sound and the presentation of output to avoid overwhelming the user. The system must efficiently acquire sound and present stable, correct output without exposing the user to all possible candidates, ensuring a smooth and correct user experience."
            ]
        },
        {
            "question": "What challenge is mentioned in the text regarding the presentation of output to the user when deploying the system?",
            "reference-answers": [
                "The challenge mentioned is presenting a stable and correct output to the user without exposing them to all possible candidates the system is considering."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen21-slide22/text.txt": [
        {
            "question": "What are the main challenges mentioned in the text when integrating systems in the ELITER project?",
            "reference-answers": [
                "The main challenges mentioned include the difficulty of integrating systems, the complexity of the pipeline, handling uncertainties and ambiguities in input, and potential errors introduced during integration."
            ]
        },
        {
            "question": "What is the main challenge mentioned when deploying the system in the European project called ELITER?",
            "reference-answers": [
                "The main challenge mentioned when deploying the system in the European project called ELITER is the difficulty of integrating the components together, which can introduce further errors."
            ]
        },
        {
            "question": "What is the purpose of sending NBEST lists in the described system?",
            "reference-answers": [
                "To handle the uncertainty or ambiguity in the input, we need to provide multiple possible transcriptions (NBEST lists) to the next component in the pipeline, ensuring that the translation can account for potential transcription errors or ambiguities."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen22-slide23/text.txt": [
        {
            "question": "What is the main challenge mentioned in the text when subtitling online sessions in another language in real time?",
            "reference-answers": [
                "The main challenge is subtitling online sessions in another language in real time because the online aspect makes it difficult."
            ]
        },
        {
            "question": "What challenge is presented by subtitling live online sessions in another language?",
            "reference-answers": [
                "The challenge presented is the difficulty of subtitling live online sessions in another language, as the online aspect introduces specific complexities."
            ]
        },
        {
            "question": "The difficulty in subtitling live online sessions in another language is primarily due to the real-time nature of the process and the additional challenge of translating subtitles into a different language.",
            "reference-answers": [
                "The difficulty in subtitling live online sessions in another language is primarily due to the real-time nature of the process and the additional challenge of translating subtitles into a different language."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen23-slide24/text.txt": [
        {
            "question": "What is the role of the mediator in the described architecture?",
            "reference-answers": [
                "The mediator acts as a central hub, connecting all the different workers and coordinating the processing pipeline. It receives the sound data from the client, distributes it to the appropriate systems like the ASR, and ensures the web interface updates as processing occurs, handling data flow and partial outputs."
            ]
        },
        {
            "question": "What type of errors is the described architecture resilient to?",
            "reference-answers": [
                "The described architecture is resilient to errors related to partial input processing and online data handling, such as partial data loss or delays."
            ]
        },
        {
            "question": "What is the role of the mediator in the described architecture?",
            "reference-answers": [
                "The mediator acts as a central hub, connecting all the different workers and coordinating the processing pipeline. It receives the sound data from the client, distributes it to the appropriate systems like the ASR, and ensures the web interface updates as processing occurs, handling data flow and partial outputs."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen24-slide25/text.txt": [
        {
            "question": "What are the possible issues that were encountered in the tests?",
            "reference-answers": [
                "The possible issues are the actual issues that we have encountered in our tests."
            ]
        },
        {
            "question": "What does the speaker say about the issues they have encountered in their tests?",
            "reference-answers": [
                "The speaker refers to the issues they will discuss as true issues that they have encountered in their tests, not just hypothetical or possible ones."
            ]
        },
        {
            "question": "What are the true issues that were found during the tests?",
            "reference-answers": [
                "The true issues that were found during the tests are the possible issues that we have actually ran into."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen25-slide26/text.txt": [
        {
            "question": "What was the primary reason for the unexpected delay during the live subtitling?",
            "reference-answers": [
                "The primary reason for the unexpected delay during the live subtitling was that reconstruction was happening at the institute, which caused an internet connectivity problem at the ASR partner's end, leading to a slow pipeline and an unknown delay."
            ]
        },
        {
            "question": "What was the primary reason for the unexpected delay experienced during the live subtitling?",
            "reference-answers": [
                "The primary reason for the unexpected delay during the live subtitling was that there was some reconstruction happening at the institute, which caused an internet connectivity problem at the ASR partner, leading to a slowdown in the pipeline."
            ]
        },
        {
            "question": "What was the primary issue that led to the delay during the live subtitling?",
            "reference-answers": [
                "The primary issue that led to the delay during the live subtitling was an internet connectivity problem at the ASR partner, which caused the pipeline to slow down."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen26-slide27/text.txt": [
        {
            "question": "What problem occurs with the sound quality if the first cable is not properly connected?",
            "reference-answers": [
                "The sound quality is bad."
            ]
        },
        {
            "question": "What happens to the sound quality if the first cable is badly plugged in?",
            "reference-answers": [
                "The sound quality becomes bad."
            ]
        },
        {
            "question": "What is one issue that can arise with the sound input?",
            "reference-answers": [
                "One issue that can arise with the sound input is that if the first cable is badly plugged in, the sound quality is bad and nothing can recover from it."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen27-slide28/text.txt": [
        {
            "question": "What are the main challenges in subtitle presentation during live streaming, as discussed in the text?",
            "reference-answers": [
                "The main challenges in subtitle presentation during live streaming include delays caused by communication bottlenecks between the presentation worker and the web, as well as potential overloading of end-user devices, which can result in subtitles not being displayed in a timely manner."
            ]
        },
        {
            "question": "What are two main issues that can cause delays in subtitle presentation during a live stream?",
            "reference-answers": [
                "The two main issues causing delays in subtitle presentation during a live stream are communication bottlenecks between the presentation worker and the web, leading to subtitle delays, and end-user devices being overwhelmed with data, resulting in delayed subtitle display."
            ]
        },
        {
            "question": "What are two issues that can cause subtitle delays for end users in the described system?",
            "reference-answers": [
                "- A communication bottleneck between the presentation worker and the web, causing delays in subtitle delivery.  \n- Overloaded end-user devices that struggle to handle the data, resulting in delayed subtitle presentation."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen28-slide29/text.txt": [
        {
            "question": "What was the reason for the delay in the pipeline caused by the misconfiguration?",
            "reference-answers": [
                "The delay was caused by shipping the sound twice over the first-mile connection due to a misconfiguration, which overloaded it."
            ]
        },
        {
            "question": "What was the cause of the delay in the pipeline?",
            "reference-answers": [
                "The delay in the pipeline was caused by a typo in the configuration that resulted in the sound being sent twice through the mediator, overloading the initial connection."
            ]
        },
        {
            "question": "What was the reason for intentionally starting the ASR system locally?",
            "reference-answers": [
                "The ASR system was intentionally started locally because the models were only compiled on the particle machine, which directly acquired the sound."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen29-slide31/text.txt": [
        {
            "question": "What problem occurred due to the incorrect placement of the chest mic during the presentation?",
            "reference-answers": [
                "The sound volume and clarity of the sound decreased."
            ]
        },
        {
            "question": "What was the reason for the decreased sound volume and clarity during the presentation?",
            "reference-answers": [
                "The decreased sound volume and clarity were due to the chest mic being positioned on one side of the body while the speaker faced and spoke towards the opposite side, increasing the distance between the mic and the sound source."
            ]
        },
        {
            "question": "What was the result of the colleague attaching the chest mic close to one shoulder and talking to the opposite side during the presentation?",
            "reference-answers": [
                "The result was that the sound volume and clarity decreased due to the incorrect placement and direction of the chest mic."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen30-slide32/text.txt": [
        {
            "question": "What comparison was briefly made in one of the sessions, and what was the clear benefit of using one type of microphone over the other?",
            "reference-answers": [
                "A comparison was briefly made between two types of microphones: the chest mic and the headset microphone. The clear benefit of using the headset microphone is that the distance from the mouth does not change, providing consistent sound quality."
            ]
        },
        {
            "question": "What is the main advantage of using a headset microphone compared to a chest microphone, as mentioned in the text?",
            "reference-answers": [
                "The main advantage of using a headset microphone is that the distance from the mouth remains consistent, unlike the chest microphone."
            ]
        },
        {
            "question": "What is the primary advantage of using a headset microphone compared to a chest microphone, as mentioned in the text?",
            "reference-answers": [
                "The primary advantage of using a headset microphone is that it maintains a consistent distance from the mouth, which provides clearer audio."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen31-slide33/text.txt": [
        {
            "question": "What could people cover the microphone with when using a handheld microphone?",
            "reference-answers": [
                "hands, clothing, or other objects."
            ]
        },
        {
            "question": "What could people cover a handheld microphone with, according to the text?",
            "reference-answers": [
                "The text does not provide the answer."
            ]
        },
        {
            "question": "What is a common problem people face when using handheld microphones?",
            "reference-answers": [
                "People could cover the microphone with their hands or other objects, causing interference or poor audio quality."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen32-slide35/text.txt": [
        {
            "question": "What are the possible issues that could arise from incorrect positioning of something in relation to the mouth and hand?",
            "reference-answers": [
                "The possible issues are that the item could be positioned too close to the mouth, too far from the mouth, or too far below the hand."
            ]
        },
        {
            "question": "What are some possible mistakes someone could make when positioning something near their mouth or hand?",
            "reference-answers": [
                "They could put it too close to their mouth, too far from their mouth, or too far below their hand."
            ]
        },
        {
            "question": "What are two possible mistakes someone could make regarding the placement of the item near their mouth or hand?",
            "reference-answers": [
                "They could place the item too close to their mouth or too far from their mouth."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen33-slide36/text.txt": [
        {
            "question": "What potential issue can arise when someone stands in front of loudspeakers while using a microphone during an exam?",
            "reference-answers": [
                "The potential issue is that the double sound wave from the loudspeakers and microphone could damage the ASR quality, as the system is not trained on such scenarios."
            ]
        },
        {
            "question": "What could degrade ASR quality when someone stands in front of loudspeakers, causing the microphone to receive the same sound with a slight delay?",
            "reference-answers": [
                "The double sound wave caused by the slight delay in the same sound reaching the microphone could degrade ASR quality because the ASR system is not trained on such situations."
            ]
        },
        {
            "question": "What potential issue could arise with the ASR quality when someone stands in front of the loudspeakers, causing the microphone to receive the same sound with a delay?",
            "reference-answers": [
                "The potential issue is that the ASR quality could be damaged because it is not trained on such double sound waves, which could interfere with its ability to recognize the speech accurately."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen34-slide37/text.txt": [
        {
            "question": "What is the reason for ensuring that every intermediate component in the pipeline produces output in a reasonable range of volumes?",
            "reference-answers": [
                "To ensure that the signal is properly captured without being too quiet or too loud, which could result in losing the signal or overloading downstream components."
            ]
        },
        {
            "question": "What must be ensured when acquiring the sound to prevent issues in the pipeline?",
            "reference-answers": [
                "When acquiring the sound, it must be ensured that every intermediate component in the pipeline produces output within a reasonable volume range to prevent issues where the signal is either too quiet to be detected or too loud, causing problems downstream."
            ]
        },
        {
            "question": "What are the consequences of having intermediate components produce output at volumes that are either too quiet or too loud in the pipeline?",
            "reference-answers": [
                "If intermediate components produce output that is too quiet, you will not get the signal. If the output is too loud, the components further down the pipeline will fight against that."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen35-slide38/text.txt": [
        {
            "question": "What did the teacher discuss at the beginning of the lecture and what did they test?",
            "reference-answers": [
                "The teacher discussed the superhuman quality of speech recognition at the beginning of the lecture and tested that quality themselves."
            ]
        },
        {
            "question": "What happened to the ASR during the lecture that the teacher mentioned?",
            "reference-answers": [
                "The ASR stopped working during the lecture."
            ]
        },
        {
            "question": "What did the teacher discuss at the beginning of the lecture?",
            "reference-answers": [
                "The teacher discussed the superhuman quality of speech recognition at the beginning of the lecture."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen36-slide39/text.txt": [
        {
            "question": "What specific challenges did the ASR system face in recognizing the speaker's message during the described scenario?",
            "reference-answers": [
                "The ASR system faced several challenges in recognizing the speaker's message. First, the speaker's poor pronunciation, likely due to not being a native English speaker, caused words like \"bottle\" and \"weir\" to be misarticulated. Second, the presence of significant background noise overwhelmed the system, preventing it from recognizing any words. Additionally, the city name České Budějovice was not present in the ASR system's vocabulary, leading to failure in recognizing it."
            ]
        },
        {
            "question": "What challenges did the ASR system face in the described scenario, and how did these issues affect its ability to accurately recognize and understand the speaker's message?",
            "reference-answers": [
                "In the described scenario, the ASR system faced several challenges that hindered its ability to accurately recognize and understand the speaker's message. First, the speaker's pronunciation of certain words, such as \"bottle\" and \"weir,\" was incorrect due to not being a native English speaker. This mispronunciation confused the ASR system, leading to incorrect recognition of the intended words. Additionally, the ASR system struggled with the noise in the environment, which was so disruptive that it could not recognize any words at all. Furthermore, the city name České Budějovice was not in the vocabulary of the ASR system, causing it to fail in recognizing this specific reference. These issues collectively made it difficult for the ASR system to accurately capture and understand the speaker's message."
            ]
        },
        {
            "question": "What specific issues did the ASR system face in this realistic setup?",
            "reference-answers": [
                "The ASR system faced several specific issues:  \n1. **Mispronunciation**: The speaker's non-native English led to incorrect pronunciation of words like \"bottle\" and \"weir,\" which the ASR system struggled to recognize.  \n2. **Noise Interference**: The poor signal-to-noise ratio made it difficult for the ASR system to recognize any words, even if some words were misheard (e.g., \"bottle\" was misrecognized as \"somebody\").  \n3. **Vocabulary Limitation**: The ASR system did not recognize the city name \"České Budějovice\" because it was not included in its vocabulary, leading to failure in identifying that part of the speech."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen37-slide40/text.txt": [
        {
            "question": "What is the word error rate of the speech recognition system compared to the human level, and how does it perform on recordings where systems give up?",
            "reference-answers": [
                "The speech recognition system has a word error rate of 40%, which is 10 times worse than the human level of 4%. On recordings where systems give up, the word error rate is 100%."
            ]
        },
        {
            "question": "What is the word error rate of the speech recognition system described in the text, and how does it compare to human-level recognition quality?",
            "reference-answers": [
                "The speech recognition system has a word error rate of 40%, which is 10 times worse than human-level recognition quality of 4%."
            ]
        },
        {
            "question": "What is the word error rate of Google's system when it gives up on producing any output?",
            "reference-answers": [
                "The word error rate of Google's system when it gives up on producing any output is 100%."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen38-slide41/text.txt": [
        {
            "question": "Why did the speech recognition systems fail in the described scenario?",
            "reference-answers": [
                "The speech recognition systems failed because the high school students made grammatical errors and had pronunciation issues, which made it difficult for the systems to accurately recognize the speech. Additionally, the noisy input and lack of control over the recording environment further contributed to the systems' poor performance."
            ]
        },
        {
            "question": "What does the text suggest about the accuracy of speech recognition systems in real-world scenarios?",
            "reference-answers": [
                "The text suggests that speech recognition systems are significantly less accurate in real-world scenarios due to factors like grammatical and pronunciation errors, poor recording environments, and the spontaneity of speech, leading to much lower performance compared to ideal conditions."
            ]
        },
        {
            "question": "Why did the speech recognition systems perform poorly in the described scenario?",
            "reference-answers": [
                "The speech recognition systems performed poorly because the high school students made grammatical and pronunciation errors, and the noisy input combined with a lack of controlled recording environment made it difficult for the systems to accurately recognize and transcribe the speech."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen39-slide42/text.txt": [
        {
            "question": "What were the two reality checks mentioned for?",
            "reference-answers": [
                "The two reality checks were for speech recognition and machine translation."
            ]
        },
        {
            "question": "What were the two reality checks performed by the non-native person aimed at?",
            "reference-answers": [
                "The two reality checks were aimed at speech recognition and machine translation."
            ]
        },
        {
            "question": "What was the reality check for machine translation after discussing superhuman capabilities?",
            "reference-answers": [
                "A reality check for a machine translation."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen40-slide43/text.txt": [
        {
            "question": "What are the two main translation errors described in the text, and what do they demonstrate about the challenges in machine translation?",
            "reference-answers": [
                "The two main translation errors described are:\n\n1. **Mistranslation of specific words leading to meaning distortion**: For example, the word \"if\" was mistranslated into German as \"op,\" which corresponds to \"weather\" in English. This caused the German speaker to misunderstand the sentence, interpreting it as \"it is difficult to ask whether you understand or do not understand\" instead of \"it is much more difficult to ask if you do not have any clue.\" This demonstrates how specific word mistranslations can completely change the meaning of a sentence.\n\n2. **Failure to translate unknown or obscure vocabulary**: In another example, profanities were not recognized by the translation system, which instead translated them as \"professional things.\" This shows how unknown or unfamiliar words can lead to incorrect or nonsensical translations, as the system struggles to find appropriate replacements for words it does not recognize.\n\nThese errors demonstrate the challenges in machine translation, particularly the difficulty in handling lexical ambiguity and the limitations of translation systems when encountering unknown or obscure vocabulary. Such issues can lead to significant misunderstandings and highlight the need for more robust language processing capabilities."
            ]
        },
        {
            "question": "Explain the two examples of translation errors provided in the text and identify the type of error each illustrates.",
            "reference-answers": [
                "The text provides two examples of translation errors:\n\n1. **First Example (Mistranslation of \"if\"):**\n   - **Error:** The word \"if\" was mistranslated into German as \"op,\" which corresponds to the English word \"weather.\"\n   - **Issue:** This caused a misunderstanding where the German speaker interpreted the sentence as being about understanding or not understanding, rather than the difficulty of asking when you don't have a clue.\n   - **Type of Error:** This illustrates a **word substitution error**, where a word is incorrectly translated into another word with a different meaning.\n\n2. **Second Example (Profanity Misinterpretation):**\n   - **Error:** A profanity was not recognized by the translation system, and the subword units were misinterpreted as \"professional things.\"\n   - **Issue:** The system could not handle the profanity and produced an incorrect translation based on the subwords of the unknown word.\n   - **Type of Error:** This illustrates a **semantic distortion error**, where the intended meaning is lost due to the system's inability to recognize or translate certain words accurately."
            ]
        },
        {
            "question": "What are the two examples of translation errors discussed in the text?",
            "reference-answers": [
                "Two examples of translation errors discussed in the text are:\n\n1. The mistranslation of the English word \"if\" into German as \"op,\" which corresponds to \"weather\" in English, leading to a misunderstanding of the sentence's meaning.\n\n2. The mistranslation of a word involving \"profanities\" as \"professional things\" because the translation system did not recognize the word and instead used the subword units to form \"professional things.\""
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen41-slide44/text.txt": [
        {
            "question": "What does the author emphasize about the combination of speech recognition and machine translation systems?",
            "reference-answers": [
                "The author emphasizes that combining speech recognition and machine translation systems can lead to severe misunderstandings, as errors in one system can propagate and distort the original meaning significantly. They stress the importance of relying on a larger context to mitigate these errors, as isolated errors can mislead users and affect the overall message comprehension."
            ]
        },
        {
            "question": "What main problem does the text discuss regarding the combination of speech recognition and machine translation systems?",
            "reference-answers": [
                "The main problem discussed is that the combination of speech recognition and machine translation systems can lead to significant errors because the errors from one system compound the issues in the other, especially when there is insufficient context to recover from these errors."
            ]
        },
        {
            "question": "What is the main problem when speech recognition errors are combined with machine translation errors, as illustrated in the example?",
            "reference-answers": [
                "The main problem is that combining speech recognition errors with machine translation errors can lead to severe misunderstandings because the translation cannot recover from the initial errors without a larger context, resulting in a loss of the intended meaning."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen42-slide46/text.txt": [
        {
            "question": "How do errors in identifying sentence boundaries affect the performance of end-to-end spoken language translation systems, according to the text?",
            "reference-answers": [
                "Errors in identifying sentence boundaries lead to much larger errors in the performance of end-to-end spoken language translation systems, as the systems are trained and tested on datasets with isolated utterances. If sentence boundaries are not correctly identified, the translation errors will be significantly greater because the system assumes each utterance is a complete sentence."
            ]
        },
        {
            "question": "What is the impact of accurately identifying sentence boundaries on the performance of end-to-end spoken language translation systems?",
            "reference-answers": [
                "Accurately identifying sentence boundaries significantly impacts the performance of end-to-end spoken language translation systems. If the sentence boundaries are not correctly identified, the system's performance will be negatively affected, as the errors will be much larger than the translation system alone would suggest. This is because the current end-to-end spoken language translation systems are trained and tested on datasets that already come in isolated utterances, meaning they work best when the input is correctly segmented into individual utterances. If the system cannot identify where the utterances are, it will lead to much larger errors."
            ]
        },
        {
            "question": "What challenge arises when integrating ASR and machine translation without proper segmentation?",
            "reference-answers": [
                "The challenge is that without proper segmentation, the ASR outputs a continuous sequence of words without punctuation, which makes it difficult for the machine translation system to process them into correct sentences. This leads to larger errors in translation compared to if the ASR could predict punctuation directly or if the SLT system handled segmentation correctly."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen43-slide47/text.txt": [
        {
            "question": "What approaches are mentioned for inserting punctuation into a sequence of words, and how does sound information factor into these approaches?",
            "reference-answers": [
                "The approaches mentioned for inserting punctuation into a sequence of words include:\n\n1. **Language Model Scoring**: Comparing the probabilities of sequences with and without punctuation (e.g., adding a comma, full stop, or question mark) and selecting the most probable option.\n\n2. **Sequence Labeling Algorithms**: Using tags for each word to indicate the punctuation that should follow (e.g., nothing, full stop, comma, question mark, or capitalization) and employing models like Markov models, CRFs, or LSTMs.\n\n3. **Machine Translation Task**: Treating punctuation insertion as a translation task, where a system translates a sequence of unpunctuated words into a sequence with punctuation, using approaches like phrase-based or neural machine translation.\n\nRegarding sound information, it is noted that:\n\n- **Language Model Approach**: Does not incorporate sound information.\n- **Sequence Labeling**: Sound information can be included depending on the architecture, but it is not always straightforward.\n- **Machine Translation**: Sound information is not easily integrated, especially in phrase-based systems, which cannot incorporate it.\n\nSound is informative for prosody and intonation, which can indicate sentence boundaries and other linguistic aspects, but the discussed approaches do not easily integrate sound information."
            ]
        },
        {
            "question": "How do different approaches, such as language model scoring, sequence labeling, and machine translation, contribute to inserting punctuation in a sequence of words, and what role does sound information play in these models?",
            "reference-answers": [
                "Different approaches contribute to inserting punctuation in a sequence of words as follows:\n\n1. **Language Model Scoring**: This approach uses a language model to evaluate the probability of sequences of words. It compares the probability of the sequence without punctuation to the probability with inserted punctuation (e.g., comma, full stop, question mark). The model selects the most probable option.\n\n2. **Sequence Labeling**: This approach tags each word in the sequence with a label indicating the appropriate punctuation to follow. The tags can include options like nothing, full stop, comma, or question mark, along with capitalization for the next word. This method can be implemented using models like Markov models, Conditional Random Fields (CRFs), or recurrent neural networks such as LSTMs.\n\n3. **Machine Translation**: This approach treats punctuation insertion as a machine translation task. It translates a sequence of unpunctuated words into a sequence with punctuation. This can be done using phrase-based machine translation or neural machine translation techniques.\n\nRegarding the role of sound information, it is noted that sound is not typically incorporated in these approaches. Language model scoring does not use sound at all. In sequence labeling, whether sound is included depends on the architecture. In machine translation, sound is generally not part of the system. However, sound can be informative for punctuation insertion, such as indicating sentence boundaries through pauses or intonation changes."
            ]
        },
        {
            "question": "Which approach mentioned does not use sound information?",
            "reference-answers": [
                "The language model-based approach does not use sound information."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen44-slide48/text.txt": [
        {
            "question": "What is one severe consequence of misplacing punctuation marks in the context of machine translation?",
            "reference-answers": [
                "One severe consequence is that the machine translation system treats the incorrectly segmented sentences as trustworthy, using surrounding words that shouldn't belong together and shuffling them into prominent positions, leading to significant damage in the final output."
            ]
        },
        {
            "question": "What is the primary issue discussed regarding the impact of incorrect punctuation placement in the provided text?",
            "reference-answers": [
                "The primary issue is that incorrect punctuation placement can lead to severe translation errors, as the machine treats incorrectly segmented sentences as accurate and misuses the surrounding words."
            ]
        },
        {
            "question": "What is an example of how misplaced punctuation can cause problems in machine translation?",
            "reference-answers": [
                "An example of how misplaced punctuation can cause problems in machine translation is when the speaker used the phrase \"all too well\" during their speech. The segmenter incorrectly placed a full stop after \"all too,\" resulting in the sentence being split into \"this approach does not generalize all too\" and the next sentence starting with \"well.\" This misplacement caused the machine translation system to mistranslate the sentences, as it treated the incorrectly split parts as separate and trustworthy, leading to errors in the final output."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen45-slide49/text.txt": [
        {
            "question": "What does the text suggest moving towards in terms of translation?",
            "reference-answers": [
                "The text suggests moving towards end-to-end spoken language translation."
            ]
        },
        {
            "question": "What problem does the end-to-end spoken language translation not have?",
            "reference-answers": [
                "The end-to-end spoken language translation does not have the problems that previous methods had."
            ]
        },
        {
            "question": "What is the purpose of moving to end-to-end spoken language translation?",
            "reference-answers": [
                "The purpose of moving to end-to-end spoken language translation is to avoid the problems that existed in the previous approach."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen47-slide49/text.txt": [
        {
            "question": "What are systems trained and tested on according to the text?",
            "reference-answers": [
                "The systems are trained and tested on corpora which are utterance segmented by themselves."
            ]
        },
        {
            "question": "What does the text say about the training and testing of the systems?",
            "reference-answers": [
                "The systems are trained and tested on corpora which are utterance segmented by themselves."
            ]
        },
        {
            "question": "The TEXT mentions that the corpora used for training and testing are segmented into what?",
            "reference-answers": [
                "The corpora are segmented into utterances."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen48-slide50/text.txt": [
        {
            "question": "What computational and alignment challenges arise from the input being longer and non-segmented in direct spoken language translation?",
            "reference-answers": [
                "The computational and alignment challenges arise because the input sequences are much longer, making it computationally challenging to fit all the input into the GPU memory. Additionally, the alignment problem is more complicated since the input is often non-segmented, meaning it is not manually or correctly segmented into utterances, unlike text input."
            ]
        },
        {
            "question": "What are the main drawbacks of direct spoken language translation as discussed in the text?",
            "reference-answers": [
                "The main drawbacks of direct spoken language translation, as discussed in the text, are:\n\n- **Insufficient Training Data**: There is a lack of adequate data that combines speech in the source language with text in the target language. Researchers often resort to synthesizing data, which is not optimal.\n\n- **Longer Input Sequences**: Speech input is longer, making it computationally intensive and challenging to fit into GPU memory.\n\n- **Complex Alignment Problem**: Aligning sounds with words is more difficult than aligning text.\n\n- **Non-Segmented Input**: Speech is not naturally segmented, unlike text, which often requires manual segmentation."
            ]
        },
        {
            "question": "The main drawback mentioned for direct spoken language translation is the insufficient training data, particularly the lack of sufficient paired source language speech and target language text data.",
            "reference-answers": [
                "The main drawback mentioned for direct spoken language translation is the insufficient training data, particularly the lack of sufficient paired source language speech and target language text data."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen49-slide51/text.txt": [
        {
            "question": "What can the system do at runtime after being trained on triples of input, source transcript, and target transcript in the multitask learning approach?",
            "reference-answers": [
                "The system can produce both the transcript of the source language and the translation in the target language at runtime."
            ]
        },
        {
            "question": "What approaches are suggested when training data for speech-to-text systems is insufficient?",
            "reference-answers": [
                "1. **Pre-training the Speech Encoder**: Train the speech encoder on a larger corpus to produce the golden transcript, leveraging additional data for better performance.\n\n2. **Pre-training and Fine-tuning**: Pre-train the translation system using source text and target text, then reuse the speech encoder and decoder, fine-tuning them on the small SLT dataset.\n\n3. **Teacher-Student/Knowledge Distillation**: Use a teacher model to generate probability distributions, guiding the student model to match these distributions, transferring knowledge effectively.\n\n4. **Multitask Learning**: Train the system on a dataset containing input sound, source transcript, and target transcript simultaneously, allowing it to produce both transcripts at runtime by considering both during training."
            ]
        },
        {
            "question": "What is the primary issue with training a speech-to-text system, and what approaches can be used to address it according to the lecture?",
            "reference-answers": [
                "The primary issue with training a speech-to-text system is insufficient training data. To address this, the lecture suggests three approaches: pre-training components on larger datasets, using teacher-student knowledge distillation, and employing multitask learning to leverage additional data and improve alignment."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen50-slide52/text.txt": [
        {
            "question": "What type of corpus did Berard use for his end-to-end spoken language translation proof of concept?",
            "reference-answers": [
                "The corpus used by Berard was a synthetic corpus, specifically the BTEC (basic travel expressions) corpus."
            ]
        },
        {
            "question": "What type of corpus was used in Berard's 2016 proof of concept for end-to-end spoken language translation?",
            "reference-answers": [
                "The synthetic BTEC corpus was used."
            ]
        },
        {
            "question": "What type of corpus was used in Berard's 2016 proof of concept for end-to-end spoken language translation?",
            "reference-answers": [
                "The synthetic corpus, specifically the BTEC (Basic Travel Expressions) corpus, was used in Berard's 2016 proof of concept."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen51-slide53/text.txt": [
        {
            "question": "What decoding method was used in the 2018 experiment to predict English and French translations?",
            "reference-answers": [
                "character level decoding"
            ]
        },
        {
            "question": "What components were used in the 2018 study's approach to speech encoding?",
            "reference-answers": [
                "The components used in the 2018 study's approach to speech encoding include feedforward layers, convolution layers, parametric recurrent neural networks, and a bidirectional LSTM."
            ]
        },
        {
            "question": "What components and approach were used in the 2018 study for end-to-end speech-to-text translation, including the encoder, handling of the time dimension, attention mechanism, and decoding method?",
            "reference-answers": [
                "The 2018 study employed a speech encoder that utilized feedforward layers followed by convolutional layers to reduce the input length. To handle the time dimension, parametric recurrent neural networks (PRNNs) were used, where deeper layers processed the input at intervals (e.g., every three or five steps) to condense the temporal data. A bidirectional LSTM then digested this condensed input. The attention mechanism used was the standard attention discussed previously. Decoding was performed at the character level to predict the English transcription or French translation."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen52-slide54/text.txt": [
        {
            "question": "What is the pre-training approach described for the neural network components?",
            "reference-answers": [
                "The pre-training approach involves first training each component of the neural network separately on monolingual tasks. For example, the system is first trained on speech recognition to produce the source-side transcript, and for machine translation, it is pre-trained in a monolingual setting. After this initial training, the system is switched to a multilingual setting where it uses the target language transcript for tasks like speech recognition and translation. The components are pre-trained on independent and easier-to-obtain datasets before being integrated into a more complex multilingual model."
            ]
        },
        {
            "question": "What two main stages are involved in the training approach described in the text?",
            "reference-answers": [
                "The two main stages involved in the training approach are pre-training on a single task and then switching to a multilingual setting."
            ]
        },
        {
            "question": "What are the components of the neural network pre-trained on in the described approach?",
            "reference-answers": [
                "The components of the neural network are pre-trained on monolingual datasets."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen53-slide55/text.txt": [
        {
            "question": "What is the test score of the ensemble system, and how does it compare to the cascaded approach's score? Additionally, mention the number of parameters used in the ensemble system.",
            "reference-answers": [
                "The ensemble system achieved a test score of 15.5, which is slightly lower than the cascaded approach's score of 15.8. The ensemble system uses 9 million parameters."
            ]
        },
        {
            "question": "What is the comparison between the cascaded approach and the ensemble approach in terms of their test score performance, and how does the number of parameters used in the ensemble system compare to those in the ASR and translation components?",
            "reference-answers": [
                "The cascaded approach achieves a test score of 15.8, while the ensemble approach scores 15.5. The ensemble system uses 9 million parameters, which is fewer than the translation component (15-16 million) but more than the ASR-only system (6.3 million)."
            ]
        },
        {
            "question": "What is the relationship between the number of parameters and the performance of the cascaded approach compared to the ensembling system?",
            "reference-answers": [
                "The cascaded approach has fewer parameters (9 million) than the translation component but achieves a slightly better score (15.8 vs. 15.5). The ensembling system, which includes multiple subsystems, performs almost as well as the cascaded approach despite requiring more parameters in its components. Larger models with billions of parameters are needed for better performance in open domains."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen54-slide56/text.txt": [
        {
            "question": "What does the survey by Silvachak indicate about the effectiveness of various architectures, such as convolution networks with LSTMs, transformers, and others, in comparison to the cascaded system?",
            "reference-answers": [
                "The survey by Silvachak indicates that there has been significant improvement in performance over the last two years, with architectures such as convolution networks with LSTMs, transformers, and others getting close to or matching the cascaded system. However, it remains unclear which specific architecture is the most effective, as various approaches like convolution networks with transformers, Pyramid RNNs, knowledge distillation, pre-training, and multi-task setups are still being actively compared and tested."
            ]
        },
        {
            "question": "What architectural approaches are being explored to determine the most effective one, considering that the transformer's success is still uncertain?",
            "reference-answers": [
                "The architectural approaches being explored include convolutional networks with LSTMs, convolutional networks with transformers, pyramid recurrent neural networks, knowledge distillation (teacher-student approach), pre-training, and multi-task setups. The effectiveness of these approaches is still under investigation, with the success of the transformer alone remaining uncertain."
            ]
        },
        {
            "question": "What does the survey by Silvachak indicate about the current debate regarding the effectiveness of various architectures?",
            "reference-answers": [
                "The survey by Silvachak indicates that there is an ongoing debate regarding the effectiveness of various architectures, as it shows significant improvement over the past two years but remains unclear about the best approach. Different methods, such as convolution networks with LSTM, transformers, pyramid RNNs, knowledge distillation, pre-training, and multi-task setups, are being explored, and no single architecture has yet emerged as the most effective."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen55-slide57/text.txt": [
        {
            "question": "What are the two main modifications introduced by the Italian FBK team in their 2019 paper to the standard transformer architecture?",
            "reference-answers": [
                "The Italian FBK team introduced two main modifications in their 2019 paper: they added two layers of linear transformations to the combined input of frequency and positional encoding, and they used two levels of convolutional networks to reduce the input size before applying standard encoder layers."
            ]
        },
        {
            "question": "What modifications did the Italian FBK team make to the standard transformer architecture in their 2019 paper?",
            "reference-answers": [
                "The Italian FBK team made two main modifications to the standard transformer architecture. First, they added linear transformations, specifically two layers, to the combined input of frequency information and positional encoding. Second, they incorporated two levels of convolutional networks to reduce the input size before processing. Additionally, in their S-transformer variant, they delayed the addition of positional encoding until just before the self-attention phase, differing from the standard approach."
            ]
        },
        {
            "question": "What are the two main modifications made by the FBK team to the standard transformer architecture, and how do they differ from the standard approach?",
            "reference-answers": [
                "The FBK team made two main modifications to the standard transformer architecture. First, they added two layers of linear transformations to the combined input of the frequency and positional encoding, which is different from the standard approach that directly processes the input with positional encoding through self-attention. Second, they used convolutional networks to reduce the size of the input before applying the standard encoder layers, whereas the standard approach does not include such a step."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen56-slide58/text.txt": [
        {
            "question": "What is the name of the end-to-end speech-to-speech translation system mentioned in the text, and how does it preserve speaker characteristics during the translation process?",
            "reference-answers": [
                "The end-to-end speech-to-speech translation system mentioned is called **Translator-Tran**. It preserves speaker characteristics by concatenating the phoneme representation of the target language with the speaker information in a second decoder, allowing the translated speech to retain the original speaker's voice characteristics."
            ]
        },
        {
            "question": "What is the name of the end-to-end speech-to-speech translation system mentioned in the text, and how does it preserve speaker characteristics?",
            "reference-answers": [
                "The end-to-end speech-to-speech translation system mentioned in the text is called **Translator-Tran**. It preserves speaker characteristics by concatenating the phoneme representation with speaker information and using a second decoder to create spectrograms, thereby retaining the speaker's voice in the translated speech."
            ]
        },
        {
            "question": "The key components of the Translator-Tran system that allow it to preserve speaker characteristics include spectrogram conversion, concatenation of phoneme representation with speaker information, and the use of a second decoder to generate target spectrograms.",
            "reference-answers": [
                "The key components of the Translator-Tran system that allow it to preserve speaker characteristics include spectrogram conversion, the concatenation of phoneme representation with speaker information, and the use of a second decoder to generate target spectrograms."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen57-slide59/text.txt": [
        {
            "question": "What is the teacher planning to discuss in the presentation?",
            "reference-answers": [
                "The teacher is planning to discuss the presentation on speech-to-text translation."
            ]
        },
        {
            "question": "What is the focus of the presentation during the speech-to-text translation?",
            "reference-answers": [
                "The focus of the presentation during the speech-to-text translation is on the translation process of the presentation itself."
            ]
        },
        {
            "question": "What was the main focus of the presentation on speech-to-text translation?",
            "reference-answers": [
                "The main focus of the presentation was on speech-to-text translation."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen58-slide60/text.txt": [
        {
            "question": "What is the importance of subtitle timing and usability in ensuring effective communication?",
            "reference-answers": [
                "Subtitle timing and usability are crucial for effective communication because poor timing can make subtitles appear too quickly, preventing people from reading them and losing the message, even if the translations are perfect. Significant reordering in updates can also confuse viewers. Additionally, subtitles need to be in a suitable format, such as being appropriately sized, to ensure they are readable, even with perfect timing."
            ]
        },
        {
            "question": "What is a critical factor that can hinder the effectiveness of subtitle displays even if the translations are perfect?",
            "reference-answers": [
                "A critical factor is the timing and presentation of the subtitles, such as their speed and size, which can hinder effectiveness even with perfect translations."
            ]
        },
        {
            "question": "According to the text, why is timing important when displaying subtitles?",
            "reference-answers": [
                "Timing is important because subtitles that appear too quickly or are reordered in significant ways can make the message hard to follow, even if the translations are perfect. Poor timing can disrupt the flow, making it difficult for the audience to read and understand the subtitles effectively."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen59-slide61/text.txt": [
        {
            "question": "What alternative does the speaker propose for creating subtitles in multiple languages for a multilingual audience when space is limited?",
            "reference-answers": [
                "Okay, I need to answer the question: What alternative does the speaker propose for creating subtitles in multiple languages for a multilingual audience when space is limited? The text provided is about the project \"Eliter,\" which produces subtitles in many languages simultaneously. The issue is that with limited space for each language, it's difficult to include all the necessary information and keep it stable.\n\nSo, the speaker mentions that they're trying something new in their project, which involves producing subtitles in multiple languages at once. The challenge is the limited space for each language, making it hard to squeeze in information and keep it stable. The speaker then says, \"maybe a better option,\" which suggests they're hinting at an alternative solution.\n\nI need to figure out what that alternative is. The text doesn't explicitly state it, but from the context, the speaker is looking for a better way to handle the space issue. One common alternative when space is limited is to use a different format or method. Perhaps instead of having all languages on the screen at once, they could use a dual subtitle approach, where subtitles switch between languages. Or maybe a smaller font size, but that wasn't mentioned.\n\nAlternatively, the speaker might be suggesting using a single language subtitle with a language selection option for the audience. Or maybe using on-screen text with icons or flags indicating the language. Another possibility is using a sliding subtitle approach where each language appears one after another. However, the exact alternative isn't spelled out in the text, so I have to infer based on common practices.\n\nWait, the speaker says, \"if you want to show these many languages to the multilingual audience, then you have very limited space for each of the languages. And with this limited space, it is often difficult to squeeze in the information and to keep it stable. So maybe a better option.\" So the problem is that when trying to show multiple languages at once, each has limited space, making it hard. The speaker is suggesting a better option than that.\n\nPerhaps the alternative is not showing all languages at the same time but cycling through them. So instead of having all languages on the screen simultaneously, subtitles change depending on the language being spoken. Alternatively, maybe using a different layout or a smaller font. But without explicit info, I have to make an educated guess based on typical solutions.\n\nGiven that the text doesn't provide the alternative, but the speaker is suggesting a better option, perhaps the alternative is to use a single language subtitle but offer the option to switch languages, either through a menu or automatically based on the audience's settings. Or maybe using a format where each language's subtitle appears in a separate window or panel, but that might take more space.\n\nAnother thought: maybe using audio descriptions or voice-over in one language and on-screen text in another. But that's more for accessibility. Alternatively, perhaps the alternative is to focus on one language at a time, but that might not serve a multilingual audience well.\n\nWait, the project is called Eliter, and they're producing subtitles in many languages at once. The challenge is the limited space. So the alternative the speaker is hinting at is perhaps not trying to display all languages at the same time. Maybe instead of showing all subtitles simultaneously, they can cycle through them or have a way to switch between languages when needed.\n\nAlternatively, maybe the alternative is to use a more compact font or a smaller font size, but the speaker didn't mention that. They just mentioned the space issue. So perhaps the better option is to not try to display all languages on the screen at once but to find another way, like having a button or menu where the audience can select their preferred language, and the subtitles change accordingly.\n\nBut since the text doesn't specify, I have to base my answer on the context. The speaker is suggesting that the current method is difficult, so they're proposing a better option, which might be a different approach altogether, like using a single language with language selection, or perhaps using a different subtitle format that allows more efficient use of space.\n\nWait, maybe the alternative is to use a different type of subtitle format, like HTML or something that can dynamically adjust the space. But again, that's speculative.\n\nAlternatively, perhaps the alternative is to use a single subtitle stream with multiple languages, but that's not standard. Or maybe using a dual subtitle approach where two languages are displayed at a time, but that's still showing multiple languages, just not all at once.\n\nGiven that the text doesn't provide the exact alternative, but the speaker is suggesting a better option than the current method, the answer might be that the alternative is to use a different method, such as not displaying all languages simultaneously but instead providing a way to switch languages, perhaps through a menu or by selecting the preferred language.\n\nBut to be precise, since the text doesn't specify the alternative, maybe the answer is that the speaker proposes using a different method when space is limited, implying that instead of showing all languages at once, they might use a different approach, like cycling through languages or offering a language selection option.\n\nHowever, given that the speaker says \"maybe a better option,\" perhaps the alternative is not mentioned in the text, but I have to infer. Since I have to use only the information from the text, perhaps the answer is that the speaker suggests a better option, but it's not detailed in the text. However, the user provided an example answer, which is \"Maybe a better option,\" but that seems incomplete.\n\nWait, looking back, the user provided an example answer, which is just \"Maybe a better option.\" But that's not helpful. Alternatively, perhaps the answer is that the speaker proposes using a different method, such as a single language subtitle with language selection, but again, that's inferred.\n\nWait, perhaps the answer is that the speaker suggests using a different format or approach, such as not displaying all languages at once but instead providing a way to switch between languages. So, the answer would be that the alternative is to use a method where the subtitles can switch between languages based on the audience's preference or automatically.\n\nBut since the text doesn't specify, I can only say that the speaker proposes a better option, but I don't know exactly what it is. However, the user's example answer is just \"Maybe a better option,\" which seems insufficient.\n\nWait, perhaps the answer is that the speaker suggests a better option, but without more details, I can't specify. Alternatively, maybe the alternative is to use a single language subtitle with a note or menu to select the language, but that's speculative.\n\nWait, perhaps the answer is that the speaker suggests using a different approach, like a single subtitle track with multiple languages but controlled through a menu. But again, that's not in the text.\n\nAlternatively, perhaps the answer is that the speaker proposes using a single subtitle stream that can display the necessary information without the space constraints, but that's not specified.\n\nWait, perhaps the answer is that the speaker suggests using a different layout, such as smaller text or a different positioning, but that's not mentioned.\n\nHmm, maybe I should look again. The text says: \"We are producing subtitles in many languages at once. And if you want to show these many languages to the multilingual audience, then you have very limited space for each of the languages. And with this limited space, it is often difficult to squeeze in the information and to keep it stable. So maybe a better option.\"\n\nSo, the speaker is saying that producing subtitles in many languages at once is the current method, which has the problem of limited space. The speaker suggests a better option, but doesn't specify what it is. So, the answer is that the speaker proposes a better option, which isn't detailed in the text.\n\nBut the user's example answer is just \"Maybe a better option,\" which seems to be acknowledging that the speaker is suggesting something else without details.\n\nAlternatively, perhaps the answer is that the speaker suggests not displaying all languages at once but using a different method, such as dual subtitles or switching languages.\n\nBut since the text doesn't specify, perhaps the answer is that the speaker proposes an alternative method, which isn't detailed here, but the exact alternative isn't given.\n\nWait, but the user's instruction is to respond only with the answer, based on the text. The text doesn't provide the alternative, only that a better option is suggested.\n\nSo, perhaps the answer is that the speaker suggests a better option, but it isn't specified. Alternatively, maybe the answer is that the speaker proposes a different approach, such as using a single language with a language selector, but that's inferred.\n\nBut perhaps the intended answer is that the speaker proposes using a single language subtitle, but that's not stated. Alternatively, maybe the answer is that the speaker suggests a better option, but the exact alternative isn't mentioned.\n\nWait, perhaps the answer is that the speaker suggests using a different method, such as a single subtitle stream that can be adjusted for different languages, but again, that's inferred.\n\nAlternatively, perhaps the answer is that the speaker proposes using a different subtitle format that allows for more efficient space usage, but that's not mentioned.\n\nAlternatively, perhaps the answer is that the speaker suggests not showing all languages at once but instead providing a way to cycle through them or select the preferred language.\n\nBut since the text doesn't specify, I have to conclude that the speaker suggests a better option without providing details. Therefore, the answer is that the speaker proposes a better option for creating subtitles in multiple languages when space is limited, but the exact alternative isn't specified in the provided text.\n\nWait, but the example answer given by the user is just \"Maybe a better option.\" So perhaps the answer is that the speaker proposes a better option, but it's not detailed.\n\nAlternatively, maybe the answer is that the speaker suggests using a single language subtitle with a note to switch languages, but that's not in the text.\n\nWait, perhaps the answer is that the speaker proposes using a different method, such as a single subtitle track that can display all languages without the space constraint, but that's not specified.\n\nI think the best approach is to state that the speaker suggests a better option, but without more details, I can't specify what it is. However, the example answer is \"Maybe a better option,\" which seems to be the intended response.\n\nTherefore, the answer is: Maybe a better option.\n\nBut to be more precise, perhaps the alternative is to use a single language subtitle with language selection, but that's inferred.\n\nAlternatively, perhaps the answer is that the speaker proposes using a different subtitle format or method, such as dual subtitles or a language selector menu, but that's not in the text.\n\nI think the answer should be that the speaker suggests a"
            ]
        },
        {
            "question": "What challenge does the Eliter project face when creating subtitles for a multilingual audience?",
            "reference-answers": [
                "The Eliter project faces the challenge of limited space for each language when presenting subtitles to a multilingual audience, making it difficult to include sufficient information while keeping it stable."
            ]
        },
        {
            "question": "What challenge is faced when displaying subtitles in many languages to a multilingual audience in the project Eliter?",
            "reference-answers": [
                "The challenge is that limited space makes it difficult to squeeze in the information and keep it stable when displaying subtitles in many languages to a multilingual audience."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen60-slide62/text.txt": [
        {
            "question": "What is the purpose of the paragraphs described in the TEXT?",
            "reference-answers": [
                "The purpose is to provide a way for people to read back and easily track which sentences are completed, still being updated, or just incoming, allowing them to select what they want to read."
            ]
        },
        {
            "question": "What is the purpose of the paragraphs mentioned in the TEXT?",
            "reference-answers": [
                "The purpose of the paragraphs is to allow people to read back and easily indicate which sentences have been completed, which are still being updated, and which are just incoming, providing a mix of stable and unstable outputs for the user to select what they would like to read."
            ]
        },
        {
            "question": "What is the purpose of the paragraphs mentioned in the TEXT?",
            "reference-answers": [
                "The purpose of the paragraphs is to allow users to review information, indicating which sentences are completed, still being updated, or newly incoming, so they can select what they want to read."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen61-slide63/text.txt": [
        {
            "question": "What problem arises when subtitles are updated after initial display and how is it mitigated?",
            "reference-answers": [
                "When subtitles are updated after their initial display, it can cause confusion and an annoying user experience, as the subtitles may flicker or change after being shown. This happens because the ASR output and segmenter may adjust punctuation placement and machine translation results, leading to repeated updates. To mitigate this, the subtitler component is designed to move the subtitles to a new line once they are displayed, preventing flickering and ensuring the user does not see previously shown content being overwritten."
            ]
        },
        {
            "question": "What problem causes the subtitles to flicker in this system?",
            "reference-answers": [
                "The subtitles flicker because the system updates them after they've already been displayed. When the segmenter changes its punctuation or the translation system revises its output, the subtitles are modified, causing the previously shown text to change and appear flickering as it reappears in the subtitle space."
            ]
        },
        {
            "question": "What happens when the system has limited space for displaying subtitles and the segmenter changes its punctuation placement after initial translation?",
            "reference-answers": [
                "When the system has limited space for displaying subtitles and the segmenter changes its punctuation placement after initial translation, the subtitles may be updated after they were already displayed. This can cause confusion and an annoying experience for the user, as the subtitle would flicker or change after being shown."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen62-slide64/text.txt": [
        {
            "question": "How do users' preferences for subtitles differ based on their understanding of the source language?",
            "reference-answers": [
                "Users who understand the source language a little prefer subtitles in their mother tongue as hints, valuing simultaneity over precision, as they mainly follow the spoken language with subtitles aiding unfamiliar words. Conversely, users who do not understand the source language at all prioritize stability and precision, accepting a slight delay for accurate subtitles, as they rely entirely on the subtitles for comprehension."
            ]
        },
        {
            "question": "What is the difference in preferences between users who have some understanding of the source language and those who do not, regarding subtitle timing and translation accuracy?",
            "reference-answers": [
                "Users who have some understanding of the source language prefer simultaneity in subtitle timing over translation accuracy, as they primarily follow the spoken language and use subtitles as hints. In contrast, users who do not speak the source language at all prioritize stability and precision in translation accuracy over immediateness, as they rely solely on the subtitles for comprehension."
            ]
        },
        {
            "question": "What is the key consideration when designing subtitles for non-native speakers in terms of cognitive load and how does it vary between users with some understanding of the source language and those without?",
            "reference-answers": [
                "The key consideration when designing subtitles for non-native speakers in terms of cognitive load is the balance between simultaneity and precision. For users with some understanding of the source language, simultaneity is preferred as it allows them to follow the sound and use subtitles as hints for unclear words, even if the subtitles are not perfectly precise. However, for users who do not speak the source language at all, stability and precision are more important than immediateness, as they rely entirely on the subtitles for accurate translation and are willing to accept a slight delay in the subtitles to achieve this."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen63-slide65/text.txt": [
        {
            "question": "What decision did the speaker make regarding the evaluation of spoken language translation?",
            "reference-answers": [
                "The speaker decided to skip the evaluation of spoken language translation."
            ]
        },
        {
            "question": "What does the speaker plan to do?",
            "reference-answers": [
                "The speaker plans to skip the evaluation of spoken language translation."
            ]
        },
        {
            "question": "What aspect of language translation did the speaker decide to skip evaluating?",
            "reference-answers": [
                "spoken language translation"
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen64-slide66/text.txt": [
        {
            "question": "What additional factors are important when comparing the outputs of machine translation systems in spoken language beyond just the quality of the translation?",
            "reference-answers": [
                "In addition to the quality of the translation, when comparing machine translation systems in spoken language, it is important to consider the timing and synchronization of the translation output. Specifically, in an online setup, the system must ensure that the translation is produced in a timely manner to keep up with the spoken input, avoiding delays that could disrupt the flow of conversation. Additionally, factors such as the system's ability to handle real-time processing and minimize latency are crucial for ensuring a smooth and natural interaction."
            ]
        },
        {
            "question": "What additional aspects are considered when comparing machine translation systems in spoken language beyond translation quality, and how do factors like timing synchronization and visual elements such as flicker play a role?",
            "reference-answers": [
                "When comparing machine translation systems in spoken language, additional aspects beyond translation quality include timing synchronization and visual elements such as flicker. Timing synchronization ensures that the translation aligns with the speaker's utterance in real-time, maintaining the flow of the conversation. Visual elements like flicker, which may refer to screen artifacts or visual cues, can also impact the user experience and the perception of the translation quality in an online setup."
            ]
        },
        {
            "question": "What two factors besides translation quality should be considered when evaluating spoken language translation systems?",
            "reference-answers": [
                "The two factors are the delay or latency in the translation process and the synchronization of the translated output with the input, especially in real-time or online setups."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen65-slide67/text.txt": [
        {
            "question": "What problem arises when evaluating segmentation systems due to differences in how the input sound is segmented?",
            "reference-answers": [
                "The problem that arises is a mismatch between the segmentation used by the system and the golden segmentation, which can affect the evaluation of segmentation systems."
            ]
        },
        {
            "question": "What challenge arises in evaluation when there is a mismatch between the golden segmentation and the system's segmentation of input sound, and why is it important to address this issue?",
            "reference-answers": [
                "The challenge is that a mismatch between the golden segmentation and the system's segmentation of input sound can arise during evaluation, which needs to be addressed to handle this mismatch properly. This is important because it ensures accurate and fair assessment of the system's performance."
            ]
        },
        {
            "question": "What problem occurs during evaluation when the segmentation of the input sound differs from the golden segmentation?",
            "reference-answers": [
                "The problem is that the segmentation of the input sound differs from the golden segmentation, leading to a mismatch during evaluation."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen66-slide69/text.txt": [
        {
            "question": "What does the teacher mention about the last part of the lecture?",
            "reference-answers": [
                "The teacher mentions that the last part of the lecture will focus on visual information and will be a much shorter presentation."
            ]
        },
        {
            "question": "What was the topic of the last part of the lecture?",
            "reference-answers": [
                "Visual Information"
            ]
        },
        {
            "question": "What is the focus of the last part of the lecture and how is it presented?",
            "reference-answers": [
                "The focus of the last part of the lecture is on visual information, and it is presented in a much shorter format."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen67-slide70/text.txt": [
        {
            "question": "What is the translation of \"a tennis player is getting ready\" into Czech, based on the provided example?",
            "reference-answers": [
                "tenista se připravuje"
            ]
        },
        {
            "question": "What is the Czech translation of \"a tennis player is getting ready\"?",
            "reference-answers": [
                "tenista se připravuje"
            ]
        },
        {
            "question": "What is the Czech translation of the sentence \"A tennis player is getting ready\" based on the provided example?",
            "reference-answers": [
                "tenista se připravuje"
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen68-slide71/text.txt": [
        {
            "question": "What is the gender of the subject in each of the two sentences provided?",
            "reference-answers": [
                "The first sentence is male, and the second sentence is in the female or feminine gender."
            ]
        },
        {
            "question": "What is the grammatical gender of the word \"ka\" in the sentence \"Ka se připravuje.\"?",
            "reference-answers": [
                "The grammatical gender of the word \"ka\" in the sentence \"Ka se připravuje.\" is masculine."
            ]
        },
        {
            "question": "What are the genders assigned to the first and second Czech sentences discussed?",
            "reference-answers": [
                "The first Czech sentence is assigned the male gender, and the second sentence is assigned the female or feminine gender."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen69-slide72/text.txt": [
        {
            "question": "What can you do if there is an image next to the sentence about the queue?",
            "reference-answers": [
                "You can easily do whatever is mentioned in the text if there is an image next to the sentence about the queue."
            ]
        },
        {
            "question": "What can you easily do if there is an image next to a sentence in the queue?",
            "reference-answers": [
                "view or interpret the sentence with the image"
            ]
        },
        {
            "question": "What can you easily do if there is an image next to the sentence in the queue?",
            "reference-answers": [
                "understand or visualize the sentence"
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen70-slide73/text.txt": [
        {
            "question": "What issue did the teacher encounter in the example where the translation of \"Two lamps lying in the sun\" into Hindi was incorrect, and how did the image contribute to this challenge?",
            "reference-answers": [
                "The issue was that the translation of \"Two lamps lying in the sun\" into Hindi was incorrect, with \"lying\" being mistranslated. The image of two lamps lying in the sun was supposed to aid translation but didn't prevent the mistranslation, as the word \"lying\" can mean both lying down and telling lies, and the image only showed the physical act of lying."
            ]
        },
        {
            "question": "What challenge is highlighted in the example involving the translation of \"Two lamps lying in the sun\" into Hindi using the Visual Genome dataset?",
            "reference-answers": [
                "The challenge highlighted is that the visual context from the image, which shows two lamps lying in the sun, did not sufficiently assist in correctly translating the word \"lying.\" The translation incorrectly interpreted \"lying\" as \"telling lies\" instead of \"lying down,\" demonstrating that the image's context did not clearly disambiguate the meaning of the word, leading to a mistranslation."
            ]
        },
        {
            "question": "The example given is where the English sentence \"Two lamps lying in the sun\" was mistranslated into Hindi as \"two baby sheep are telling lies.\" The image of two lamps lying in the sun didn't assist in correcting the translation because \"lying\" could mean both lying in the sun and lying to each other, which the picture didn't depict.",
            "reference-answers": [
                "The example shows that the image of two lamps lying in the sun did not help correct the mistranslation of \"lying\" into Hindi, as the word can mean both lying in a place and lying to someone, which the image did not clarify."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen71-slide74/text.txt": [
        {
            "question": "How many ambiguous words were manually selected for the challenge test set?",
            "reference-answers": [
                "The manually selected number of ambiguous words for the challenge test set was 19. \n\nANSWER: 19"
            ]
        },
        {
            "question": "What example of an ambiguous word was identified in the challenge test set?",
            "reference-answers": [
                "The example of an ambiguous word identified in the challenge test set is \"mouse.\" This word can be translated as either the rodent or the computer mouse in Hindi, leading to different translations."
            ]
        },
        {
            "question": "How many ambiguous words were manually selected for the challenge test set?",
            "reference-answers": [
                "The manually selected number of ambiguous words for the challenge test set was 19. \n\nANSWER: 19"
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen72-slide75/text.txt": [
        {
            "question": "How does the surrounding context help determine the meaning of the word \"penalty\" in the given examples?",
            "reference-answers": [
                "The surrounding context, such as the images or words accompanying the word \"penalty,\" provides clear information about its meaning. For example, if \"penalty\" is associated with a street sign, it likely refers to a fine, while if it is paired with a soccer field image, it probably relates to the penalty box in a soccer game. The context helps determine whether \"penalty\" refers to a fine or a soccer-related concept."
            ]
        },
        {
            "question": "How does the surrounding context influence the meaning of the word \"penalty\" in the examples provided?",
            "reference-answers": [
                "The surrounding context, such as the images and words accompanying the word \"penalty,\" provides clear information about its meaning. In the examples, the left-hand picture relates to a fine (monetary penalty), while the right-hand picture connects to a soccer game's penalty box. This contextual information helps determine the specific meaning of \"penalty\" in each scenario."
            ]
        },
        {
            "question": "What is the penalty box most likely associated with, based on the information provided?",
            "reference-answers": [
                "The penalty box is most likely associated with soccer."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen73-slide76/text.txt": [
        {
            "question": "What component of the attention mechanism corresponds to different words in the input sentence?",
            "reference-answers": [
                "The component of the attention mechanism that corresponds to different words in the input sentence is the states. Specifically, after processing the input sentence with a bidirectional recurrent neural network, the attention mechanism attends to these states, each of which corresponds to different words in the sentence. \n\nANSWER: The states that correspond to different words in the sentence."
            ]
        },
        {
            "question": "What is the attention mechanism used for in the described attention architecture?",
            "reference-answers": [
                "The attention mechanism in the described architecture is used to focus on different parts of the input sentence as the output is being produced."
            ]
        },
        {
            "question": "What is the role of attention in the described model?",
            "reference-answers": [
                "The attention mechanism in the described model allows it to dynamically focus on different parts of the input sentence as it processes and generates the output, emphasizing relevant sections during each step."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen74-slide77/text.txt": [
        {
            "question": "What does the attention mechanism in the described image captioning model attend to during the decoding process?",
            "reference-answers": [
                "The attention mechanism attends to the various parts of the image during the decoding process."
            ]
        },
        {
            "question": "How is the attention mechanism applied in the image captioning task, and what is the role of the decoder in this process?",
            "reference-answers": [
                "In the image captioning task, the attention mechanism is applied over the 2D parts of the sentence, focusing on different regions of the image as the decoder generates each word of the output description. The decoder processes the image features and the context from previous steps to produce the next word in the sequence, effectively building the sentence by attending to relevant parts of the image."
            ]
        },
        {
            "question": "What are the inputs and outputs of the image captioning task described in the text?",
            "reference-answers": [
                "The input is an image, and the output is a description of that image."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen75-slide76/text.txt": [
        {
            "question": "What does the multi-encoder setup allow the system to do?",
            "reference-answers": [
                "The multi-encoder setup allows the system to attend to either the source words or other elements."
            ]
        },
        {
            "question": "What is the result of setups being put together?",
            "reference-answers": [
                "The result of setups being put together is a multi-encoder setup."
            ]
        },
        {
            "question": "What must be done first in order to have the multi-encoder setup?",
            "reference-answers": [
                "Setups together."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen76-slide77/text.txt": [
        {
            "question": "What two main components are used in the described setup to combine image information with an additional encoder and produce output words?",
            "reference-answers": [
                "The two main components used are an image encoder and a decoder."
            ]
        },
        {
            "question": "What aspect of the setup is the same as in the multilingual lecture from last week?",
            "reference-answers": [
                "The aspect that is the same as in the multilingual lecture from last week is the use of multiple encoders (one for each input, such as different languages or in this case, the image) that all feed into a single decoder."
            ]
        },
        {
            "question": "What are the two main components mentioned in the text and how are they combined to utilize image information in producing output words?",
            "reference-answers": [
                "The two main components mentioned are the image encoder and the decoder. These are combined by using the image encoder to process the image information and the decoder to generate the output words, similar to how multiple text inputs were handled in a previous lecture."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen77-slide78/text.txt": [
        {
            "question": "What causes the decoder's attention to the image to increase when processing the word \"green\"?",
            "reference-answers": [
                "The decoder's attention to the image increases when processing the word \"green\" because the green color in the image was very noticeable (salient) and informative, prompting the decoder to look at the visual information for better context."
            ]
        },
        {
            "question": "In the described hierarchical attention setup, how did the decoder's attention to the image behave when the word 'green' was mentioned in the output?",
            "reference-answers": [
                "When the word 'green' was mentioned in the output, the decoder's attention to the image was triggered. Specifically, the image attention line showed lighter spots, indicating that the decoder briefly looked at the visual information, particularly when the color 'green' was mentioned."
            ]
        },
        {
            "question": "When was the decoder most likely to attend to the visual information in the hierarchical attention setup?",
            "reference-answers": [
                "The decoder was most likely to attend to the visual information when processing the word \"green.\""
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen78-slide79/text.txt": [
        {
            "question": "What are some of the approaches mentioned in the text for visually supported translation tasks?",
            "reference-answers": [
                "The approaches mentioned include hierarchical attention, parallel attention, using imagination in the decoder, and a multi-task setup where the encoder is trained to produce both text and image representations."
            ]
        },
        {
            "question": "What approaches are mentioned in the text as being used in visually supported translation?",
            "reference-answers": [
                "The approaches mentioned in the text include hierarchical attention, parallel attention, using imagination (where the decoder imagines the image), and a multi-task setup where the encoder processes the input text and is trained to produce both the target text and image representation."
            ]
        },
        {
            "question": "What is one of the strategies used in the encoder-decoder setup for visually supported translation tasks as described in the text?",
            "reference-answers": [
                "One strategy is the encoder processing the input text and being trained to produce both the target text and the target image representation, effectively imagining what the image could be."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen79-slide80/text.txt": [
        {
            "question": "Which model's performance was significantly affected when the image did not match the input text?",
            "reference-answers": [
                "The hierarchical attention approach's performance was significantly affected when the image did not match the input text."
            ]
        },
        {
            "question": "Which model demonstrated sensitivity to non-matching images in Desmond Elliott's experiment?",
            "reference-answers": [
                "The hierarchical attention approach model demonstrated sensitivity to non-matching images in Desmond Elliott's experiment."
            ]
        },
        {
            "question": "Which model in Desmond Elliott's experiment was sensitive to the image discrepancies, and how did its performance differ when the image did not match the input text?",
            "reference-answers": [
                "The hierarchical attention approach was sensitive to image discrepancies. Its performance deteriorated when the image did not match the input text."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen80-slide81/text.txt": [
        {
            "question": "What does the speaker refer to when mentioning \"this last\" in the TEXT?",
            "reference-answers": [
                "the final point or concept mentioned in the context of exam preparation."
            ]
        },
        {
            "question": "What is mentioned there as well?",
            "reference-answers": [
                "The text does not provide enough information to answer the question."
            ]
        },
        {
            "question": "What is mentioned again in the text?",
            "reference-answers": [
                "The text is incomplete, so it's not possible to determine what is mentioned again. Please provide the full text for a proper response."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen81-slide80/text.txt": [
        {
            "question": "What issue does Ozan Raglan's paper identify regarding the dataset?",
            "reference-answers": [
                "The issue identified is that the dataset does not require the visual information to carry out the task, implying it lacks the necessary visual data."
            ]
        },
        {
            "question": "What issue does Ozan Raglan's paper identify regarding the dataset?",
            "reference-answers": [
                "The issue identified is that the dataset does not require the visual information to carry out the task, implying it lacks the necessary visual data."
            ]
        },
        {
            "question": "What issue does Ozan Raglan identify as the main problem in his paper regarding the use of visual information?",
            "reference-answers": [
                "The main problem Ozan Raglan identifies is that the dataset does not require the visual information to carry out the tasks."
            ]
        }
    ],
    "nmt-class/lecture11-multimodal-mt/screen82-slide81/text.txt": [
        {
            "question": "What was the key difference in performance between the NMT systems trained on text-only information and those trained on multimodal (text and visual) information when presented with degraded text inputs?",
            "reference-answers": [
                "The key difference was that multimodal systems, trained on both text and visual information, could recover details like gender and colors from the image, resulting in accurate translations. In contrast, text-only NMT systems produced incorrect outputs, such as guessing \"buoy\" (a boy) and misidentifying the color as blue, as they lacked access to visual context."
            ]
        },
        {
            "question": "What was the impact of incorporating image information on the translation systems' accuracy in the experiment described?",
            "reference-answers": [
                "Incorporating image information significantly improved the translation systems' accuracy. The multimodal systems, which utilized both text and visual data, accurately inferred details like gender and colors from the image, resulting in correct translations such as \"femme\" and the accurate color \"white.\" In contrast, non-multimodal systems, relying solely on text, produced incorrect translations like \"buoy\" and mentioned the wrong color \"blue.\" Thus, the image data补足了文本信息的不足，增强了翻译的准确性。"
            ]
        },
        {
            "question": "What did the experiment demonstrate about the effectiveness of using image information in translation systems compared to text-only systems?",
            "reference-answers": [
                "The experiment demonstrated that translation systems incorporating image information are more effective than text-only systems, as they can recover details like gender and color from the image, leading to more accurate translations."
            ]
        }
    ],
    "popular/audio-09/text.en.txt": [
        {
            "question": "What is one example Kilian Kirch-Gessner uses to illustrate the positive impact of EU membership on the Czech Republic?",
            "reference-answers": [
                "The Teppler Monastery in Western Bohemia is an example. EU funds totaling around 500 million Czech crowns (approximately 20 million euros) were invested to restore the monastery, which now hosts an academy, seminars, concerts, and other cultural activities, revitalizing the site and attracting visitors."
            ]
        },
        {
            "question": "What was the approximate amount in euros that the EU contributed to the Teppler Monastery in West Bohemia?",
            "reference-answers": [
                "The EU contributed approximately 20 million euros to the Teppler Monastery in West Bohemia."
            ]
        },
        {
            "question": "What role have EU subsidies played in the revitalization of the Teppler Monastery and the local brewery in Western Bohemia?",
            "reference-answers": [
                "EU subsidies have played a crucial role in the revitalization of both the Teppler Monastery and the local brewery in Western Bohemia. The Teppler Monastery received approximately 500 million Czech crowns (around 20 million euros) in EU funding, which facilitated its renovation and transformation into a vibrant cultural and educational center. This included the establishment of an academy hosting seminars, concerts, and other events, which has boosted tourism and community activity.\n\nSimilarly, the local brewery in Chodowa Plana benefited from EU support, contributing to its growth as a tourist attraction. The influx of visitors, including international tourists, has been partly attributed to increased EU funding post-2004, enhancing the brewery's facilities and solidifying its status as the largest employer in the region. Both instances demonstrate how EU subsidies have not only revitalized these sites but also supported local economies and cultural vibrancy in Western Bohemia."
            ]
        }
    ],
    "popular/audio-10/text.en.txt": [
        {
            "question": "What were the economic growth rates of the Czech Republic and Slovakia between 2009 and 2016, and how did the introduction of the euro in Slovakia affect the GDP per capita difference between the two countries?",
            "reference-answers": [
                "Between 2009 and 2016, the Czech Republic experienced an annual economic growth rate of 0.4%, while Slovakia's growth rate was 1.6% per year. The introduction of the euro in Slovakia led to a significant reduction in the GDP per capita difference between the two countries. By 2015, the gap decreased from 21% to 7%, a reduction of 14% over seven years."
            ]
        },
        {
            "question": "What evidence does Nielsen present to argue that Slovakia's euro adoption has led to better economic outcomes than the Czech Republic's approach?",
            "reference-answers": [
                "Nielsen presents evidence that Slovakia's adoption of the euro has led to better economic outcomes than the Czech Republic's approach by highlighting that Slovakia experienced higher annual economic growth (1.6%) compared to the Czech Republic's 0.4% between 2009 and 2016. Additionally, the GDP per capita gap between the two countries narrowed significantly, from 21% to 7% by 2015. Slovakia also showed more stable economic performance during the 2012-2013 crisis and benefited from a more stable monetary policy framework due to eurozone membership."
            ]
        },
        {
            "question": "What was the percentage reduction in the GDP per capita difference between the Czech Republic and Slovakia between 2009 and 2015, and what was the specific difference in their economic growth rates during that period?",
            "reference-answers": [
                "The percentage reduction in the GDP per capita difference between the Czech Republic and Slovakia between 2009 and 2015 was 14%, and the specific difference in their economic growth rates during that period was 1.2% annually."
            ]
        }
    ],
    "popular/audio-12/text.en.txt": [
        {
            "question": "What are the key points Marjina Dlabayowa emphasizes about the European Union, and how does she illustrate these through her personal experiences, such as her difficulties as a non-EU student and her views on the importance of EU programs and funding for the Czech Republic?",
            "reference-answers": [
                "Marjina Dlabayova emphasizes several key points about the European Union (EU) and illustrates them through her personal experiences:\n\n1. **The EU as a Home and Core Values**: She views the EU as a home rather than an impersonal structure, highlighting its values and importance as a foundation for unity and personal identity.\n\n2. **Evolution of Opportunities via Programs Like Erasmus**: She contrasts her own challenging experiences as a non-EU student with the current opportunities provided by EU programs like Erasmus, which have made studying abroad more accessible to thousands.\n\n3. **Challenges as a Non-EU Student and Entrepreneur**: Her personal struggles, such as difficulty obtaining scholarships and visas as a non-EU student and setting up a company in Italy, underscore the barriers faced before the EU's freedoms and the importance of maintaining these freedoms.\n\n4. **Economic Benefits and Funding**: She highlights the financial benefits to the Czech Republic from EU funds, which have contributed to economic prosperity, lower unemployment, and improved living standards.\n\n5. **Criticism of Barriers and Need for Unity**: She disapproves of current barriers being erected within the EU, advocating for the preservation of the internal market and EU unity.\n\n6. **Gratitude and Communication**: She stresses the need to recognize and communicate the EU's positive impact, urging against unwarranted skepticism and encouraging appreciation for the EU's role in personal and national development.\n\nThrough her experiences, Dlabayova illustrates the transformative and beneficial role of the EU in her life and the Czech Republic's development."
            ]
        },
        {
            "question": "What specific challenges did Marjina face as an extracommunity student before the Erasmus program existed?",
            "reference-answers": [
                "Marjina faced several specific challenges as an extracommunity student before the Erasmus program existed. She couldn't get a scholarship, a visa, or any support. She had to reapply for a visa every time and it took two years before she could fight for herself. These challenges made her experience difficult and required persistence to overcome."
            ]
        },
        {
            "question": "\"According to Marjina Dlabayova, what does she say about the Czech Republic's attitude towards the European Union?\"",
            "reference-answers": [
                "Marjina Dlabayova suggests that the Czech Republic is perceived as one of the biggest EU sceptics. She notes that this scepticism is evident in various statistics and public demonstrations, indicating a critical attitude towards the European Union."
            ]
        }
    ],
    "popular/audio-19/text.en.txt": [
        {
            "question": "How many people in Germany die without any relatives to take care of their burial each year, according to the TEXT?",
            "reference-answers": [
                "Approximately 10,000 people in Germany die without any relatives to take care of their burial each year."
            ]
        },
        {
            "question": "How often does the Tobias Brotherhood organize a funeral service for the deceased without relatives?",
            "reference-answers": [
                "The Tobias Brotherhood organizes a funeral service every three months for deceased individuals without relatives."
            ]
        },
        {
            "question": "How often does the Tobias Brotherhood organize funeral services for the deceased without family or friends?",
            "reference-answers": [
                "The Tobias Brotherhood organizes funeral services every three months."
            ]
        }
    ],
    "popular/audio-24/text.en.txt": [
        {
            "question": "What was the purpose of the Gedenkort Charité - Wissenschaft in Verantwortung project, and what components did it include?",
            "reference-answers": [
                "The purpose of the Gedenkort Charité - Wissenschaft in Verantwortung project was to commemorate the Jewish employees of Charité who were persecuted during the Nazi regime. It aimed to create a visible and educational tribute to their contributions and fates, using renowned artists to design memorial pillars. The project also included a museum section and an educational trail for young students and doctors to reflect on the ethical implications of their involvement in Nazi crimes. Components of the project were the memorial pillars, a museum, and an educational trail with historical stations."
            ]
        },
        {
            "question": "What is the purpose of the Gedenkort Charité project?",
            "reference-answers": [
                "The purpose of the Gedenkort Charité project is to commemorate the Jewish employees of the Charité hospital who were dismissed or persecuted under the Nazi regime. It serves as a memorial through the use of commemorative pillars, a museum section, and an educational trail, aiming to educate current and future doctors and students about the ethical implications of their profession during the Third Reich. The project seeks to remind healthcare professionals of the importance of maintaining ethical standards and to prevent similar atrocities by reflecting on how small ethical shifts can lead to significant historical consequences."
            ]
        },
        {
            "question": "What actions has the Charité taken to confront its past involvement with the Nazi regime, and what is the purpose of these actions?",
            "reference-answers": [
                "The Charité has taken several actions to confront its past involvement with the Nazi regime. These include launching the Gedenkort Charité - Wissenschaft in Verantwortung project in 2013, which features memorial pillars created by renowned artists to commemorate Jewish employees dismissed after 1933. The project also encompasses a museum section and an educational trail designed for young students and doctors to reflect on the ethical implications of their predecessors' involvement with the Nazi regime. The purpose of these actions is to educate future medical professionals about ethical responsibilities and to confront the hospital's past by acknowledging the victims and preventing similar ethical missteps in the future."
            ]
        }
    ],
    "popular/video-02/text.en.txt": [
        {
            "question": "What factors does the text suggest contribute to addiction beyond the chemical properties of heroin?",
            "reference-answers": [
                "The text suggests that factors contributing to addiction beyond the chemical properties of heroin include environmental stress, social isolation, lack of stimulation, and societal marginalization. It also highlights the importance of social bonds and a supportive environment in reducing addiction."
            ]
        },
        {
            "question": "How does the text explain the factors contributing to addiction beyond the use of heroin itself?",
            "reference-answers": [
                "The text explains that addiction is influenced by factors beyond the use of heroin itself. It highlights experiments with rats showing that when provided with a rich, social environment (a \"rat park\"), they showed little interest in drugged water, unlike isolated rats in cages. This suggests that environment and social context play significant roles. Additionally, studies on humans, such as soldiers in Vietnam who used heroin but stopped after returning home, indicate that external factors like stress, fear, and social support can influence addiction. The text also argues that societal factors, like isolation and the prioritization of material comfort over relationships, contribute to addiction, viewing it as a symptom of isolation rather than solely a result of the drug itself."
            ]
        },
        {
            "question": "What does the text suggest is a significant factor in addiction besides the substances themselves?",
            "reference-answers": [
                "The text suggests that a significant factor in addiction besides the substances themselves is the environment and social context. The experiments with rats showed that when given a choice in a more fulfilling environment, they were less likely to become addicted. Similarly, soldiers who used heroin in a stressful war zone stopped using it when returning to a supportive home environment. This indicates that factors like isolation, trauma, and social connections play a crucial role in addiction."
            ]
        }
    ],
    "popular/video-04/text.en.txt": [
        {
            "question": "What is the filtration efficiency of FFP-1, FFP-2, and FFP-3 masks, and how do they differ in terms of particle filtration?",
            "reference-answers": [
                "The filtration efficiency of FFP masks is as follows:\n\n- **FFP-1**: Allows no more than 20% of particles to pass through, meaning it filters out 80%.\n- **FFP-2**: Allows no more than 6% of particles to pass through, meaning it filters out 94%.\n- **FFP-3**: Allows no more than 1% of particles to pass through, meaning it filters out 99%.\n\nThese masks differ in their filtration efficiency, with FFP-3 being the most effective, followed by FFP-2, and then FFP-1."
            ]
        },
        {
            "question": "What is the difference between an FFP-1, FFP-2, and FFP-3 mask in terms of their filtration efficiency, as described in the text?",
            "reference-answers": [
                "The FFP-1 mask allows 20 out of 100 particles to pass through, the FFP-2 mask allows 6 out of 100 particles, and the FFP-3 mask allows only 1 out of 100 particles. Thus, FFP-3 has the highest filtration efficiency, followed by FFP-2, and then FFP-1 has the lowest filtration efficiency."
            ]
        },
        {
            "question": "What are the key differences in the construction and effectiveness of different types of masks, such as homemade fabric masks, surgical masks, and FFP masks, and how do they trap and filter particles like droplets, viruses, and bacteria?",
            "reference-answers": [
                "Homemade fabric masks, surgical masks, and FFP masks differ in construction and effectiveness primarily due to their materials and design. Homemade masks are made from woven fabric with visible holes, acting as a basic sieve that slows airflow and retains larger particles like droplets but allows smaller particles like viruses to pass through. Surgical masks consist of multiple layers, including spunbond and meltblown materials, which filter out more particles but still allow some airflow around the edges. FFP masks, designed for higher protection, have multiple meltblown layers and fit tightly, blocking up to 95% of particles, making them the most effective in trapping and filtering droplets, viruses, and bacteria."
            ]
        }
    ],
    "popular/video-06/text.en.txt": [
        {
            "question": "What is the significance of designating Sarajevo as the European Capital of Culture in 2014, according to the speaker?",
            "reference-answers": [
                "Designating Sarajevo as the European Capital of Culture in 2014 is significant as it sends a positive signal about the city's multi-ethnic character and cultural vibrancy, despite political divisions. It highlights Sarajevo's resilience, such as its cultural life enduring through challenging times, and its ability to foster intercultural exchanges, as seen in events like the Winter Festival and international filmmaker collaborations. The designation also serves as an encouraging message to Bosnia-Herzegovina and Europe, showcasing Sarajevo's potential and resilience, while urging European support for the city's efforts."
            ]
        },
        {
            "question": "Why did the speaker argue that Sarajevo should be designated as the European Capital of Culture 2014?",
            "reference-answers": [
                "The speaker argued that Sarajevo should be designated as the European Capital of Culture 2014 because it holds a special place in European history and culture, has a multi-ethnic character, and its cultural life remained vibrant despite challenges. The speaker highlighted Sarajevo's resilience during the Bosnian War, such as the cellist Vedran Maljovic's performance during the siege, and its intercultural festivals and collaborations. Designating Sarajevo would send a positive signal to Bosnia-Herzegovina's political class and demonstrate Europe's support, recognizing the city's immense intercultural potential and extensive preparations for the title."
            ]
        },
        {
            "question": "What is the significance of Sarajevo and why does the speaker argue it should be designated as the European Capital of Culture 2014?",
            "reference-answers": [
                "Sarajevo holds significant historical and cultural importance, being the birthplace of the assassination that triggered World War I in 1914 and enduring the longest siege of a capital during the Bosnian War. The speaker argues that designating Sarajevo as the European Capital of Culture 2014 would highlight its multi-ethnic character and intercultural potential, despite political divisions. This recognition would send a positive signal of European support, encouraging the country's political class to reflect and affirm Sarajevo's resilience and cultural continuity, as evidenced by its ongoing festivals and festivals that bring people of all nationalities together."
            ]
        }
    ],
    "popular/video-07/text.en.txt": [
        {
            "question": "What is the speaker's position on harmonizing divorce laws in the EU, and what alternatives are suggested?",
            "reference-answers": [
                "The speaker is against harmonizing EU divorce laws, emphasizing that such matters are for individual Member States. They suggest alternatives such as ensuring transparency for citizens, allowing couples to choose applicable law with safeguards to prevent forum shopping, and requiring both partners to understand the consequences of their choices. Additionally, they propose the possibility of enhanced cooperation among countries if the Council cannot reach a unanimous decision."
            ]
        },
        {
            "question": "What is the European Union's position on harmonizing divorce laws for cross-border couples, and why is this approach significant?",
            "reference-answers": [
                "The European Union's position is that it will not harmonize divorce laws for cross-border couples. This approach is significant because the Treaties of the European Union clearly state that family law matters, including divorce, are a matter for the Member States. Harmonization is not desired as it respects the diversity of national legal systems and cultural differences across Europe. Instead, the focus is on ensuring transparency and that citizens are well-informed about the consequences of choosing a particular jurisdiction for divorce. The EU aims to prevent forum shopping and ensure that the fundamental principles of the European Union are upheld in any applicable law. By maintaining the autonomy of Member States in this area, the EU respects their cultural and legal identities while addressing the practical needs of cross-border couples."
            ]
        },
        {
            "question": "Question: Does the speaker support harmonizing EU divorce laws, and why or why not?",
            "reference-answers": [
                "No, the speaker does not support harmonizing EU divorce laws. They explicitly state that harmonization is not desired and should not occur, citing the EU Treaties that assign legal matters like divorce to Member States. The speaker emphasizes maintaining national competencies and ensuring transparency and fairness in the application of laws, rather than creating uniform EU divorce laws."
            ]
        }
    ],
    "popular/video-08/text.en.txt": [
        {
            "question": "What does the speaker suggest about the European Maritime Safety Agency (Emser) in response to the risks of deep-sea drilling?",
            "reference-answers": [
                "The speaker suggests that the European Maritime Safety Agency (Emser) should have its mandate extended to include deep-sea drilling safety, bringing it closer to addressing the risks associated with such activities."
            ]
        },
        {
            "question": "What measure did the speaker propose to address the risks of deep-sea drilling in European waters?",
            "reference-answers": [
                "The speaker proposed two main measures: declaring a moratorium on new applications for deep-sea drilling until a comprehensive review is conducted and extending the mandate of the European Maritime Safety Agency (Emser) to include oversight of deep-sea drilling safety."
            ]
        },
        {
            "question": "What proposal did the speaker make regarding the European Maritime Safety Agency in the context of deep-sea drilling?",
            "reference-answers": [
                "The speaker proposed extending the mandate of the European Maritime Safety Agency (Emser) to include deep-sea drilling safety, allowing it to address these issues adequately."
            ]
        }
    ],
    "popular/video-13/text.en.txt": [
        {
            "question": "What is a micro-apartment, as described in the text?",
            "reference-answers": [
                "A micro-apartment is a very small living space, typically ranging from seven to eight square meters, designed to include basic amenities like a bathroom and kitchen through space-saving features. These compact units are affordably priced between 100 to 200 euros and emphasize minimalistic living, with essential elements like foldable furniture integrated into walls to maximize functionality in a limited area."
            ]
        },
        {
            "question": "What are the main reasons for the popularity of micro-apartments according to the text?",
            "reference-answers": [
                "The main reasons for the popularity of micro-apartments, as stated in the text, are the rising costs of living in cities, particularly the increasing expense of housing, and the growing shift towards minimalist and affordable living solutions. Additionally, micro-apartments are becoming popular in urban areas like Vienna, reflecting a change in societal preferences towards smaller, more refined living spaces."
            ]
        },
        {
            "question": "What problem do micro-apartments aim to address, as discussed in the text?",
            "reference-answers": [
                "Micro-apartments aim to address the problem of rising housing costs in cities, making it difficult for people, especially young individuals, to afford decent living spaces."
            ]
        }
    ],
    "popular/video-14/text.en.txt": [
        {
            "question": "What reasons did the author give for not owning a car?",
            "reference-answers": [
                "The author did not own a car due to several reasons: the lack of parking space, dislike for driving in Brussels, high costs associated with repairs and MOT checks, environmental concerns, and the inefficiency of car usage since it often remains parked and unused."
            ]
        },
        {
            "question": "What statistic is given in the text about annual road fatalities worldwide, and how does it compare to the accident record of Google's autonomous vehicles?",
            "reference-answers": [
                "The World Health Organisation reports that 1.2 million people die on the world's roads every year. In comparison, Google's autonomous cars have had no accidents in their tests so far."
            ]
        },
        {
            "question": "How many test miles have Google's autonomous vehicles completed according to the text?",
            "reference-answers": [
                "Google's autonomous vehicles have completed 300,000 test miles according to the text.\n\nAnswer: 300,000 test miles."
            ]
        }
    ],
    "popular/video-15/text.en.txt": [
        {
            "question": "What did the two pieces of parchment found in Admont represent?",
            "reference-answers": [
                "The two pieces of parchment found in Admont represent a fragment of the Abrogans, the oldest book in the German language. This manuscript is an alphabetical dictionary that translates Latin terms into Old High German, providing valuable insights into the early history of the German language and the methods used by medieval monks."
            ]
        },
        {
            "question": "Question: In which monastery were the fragments of the Abrogans, the oldest book in the German language, found?",
            "reference-answers": [
                "The fragments of the Abrogans were found in the Atmund monastery."
            ]
        },
        {
            "question": "What is the significance of the Abrogans manuscript found in the Admont monastery?",
            "reference-answers": [
                "The Abrogans manuscript found in the Admont monastery is significant as it is the fourth known copy of the oldest book in the German language, providing insights into the early development of the German language and script. It offers valuable knowledge about the working methods of monastic scriptoria and the early history of language standardization, reflecting the influence of Charlemagne's educational reforms and the Carolingian script."
            ]
        }
    ],
    "popular/video-23/text.en.txt": [
        {
            "question": "What are some examples of the homemade toys described in the text?",
            "reference-answers": [
                "Examples of homemade toys include a little sheep made from a stick, a duck with a foot underneath, a hut made from a cigar box and sticky tape, a dog on a chain, a doll's pram with a doll, a slide made from a bent iron beam, a swing made from an airplane cockpit, rows of rings, and dolls made from wood and fabric scraps."
            ]
        },
        {
            "question": "What were some of the homemade toys or games that children played with during that time?",
            "reference-answers": [
                "- Collecting wood  \n- Coal claw (stealing coal)  \n- Homemade sheep made from a stick  \n- Homemade duck from a scrap  \n- Farm game with four animals  \n- Hut made from a cigar box and sticky tape  \n- Dog on a chain  \n- Sliding on a bent iron beam  \n- Horse with wheels made from money  \n- Swing made from an airplane cockpit  \n- Doll's pram made from wood scraps  \n- Doll made from wood and fabric  \n- Gas mask box as a toy  \n- Milk crane game  \n- Throwing stones into puddles and climbing over them"
            ]
        },
        {
            "question": "What are some examples of homemade toys mentioned in the text, and how were they made?",
            "reference-answers": [
                "- **Little sheep**: Made from a stick, either sawed out with a fretsaw or carved.  \n- **Duck**: Made from scraps, with a foot underneath.  \n- **Hut**: Made from a cigar box and held together with sticky tape.  \n- **Dog**: Made from a piece of scrap metal and attached to a chain that might have once hung from a toilet (loo).  \n- **Slide**: A bent iron beam used as a slide.  \n- **Horse with wheels**: Made from money (which was worthless but used for play).  \n- **Swing**: Made from an airplane cockpit, possibly from a jettisoned plane.  \n- **Milk crane**: Likely made from scrap materials to resemble a crane used for collecting milk.  \n- **Gas mask box converted into a doll's pram**: A gas mask box was repurposed to create a doll's pram, which included a doll made from wood and fabric scraps.  \n\nThese toys were crafted from available materials, often repurposed or scavenged, to provide entertainment and creativity during difficult times."
            ]
        }
    ],
    "ukr-biology/book02/topic01-Обмін речовин та перетворення енергії в організмі людини/text.en.txt": [
        {
            "question": "The main sources of minerals are table salt and dairy products.",
            "reference-answers": [
                "The main sources of minerals are table salt and dairy products."
            ]
        },
        {
            "question": "Which vitamin deficiency causes the disease scurvy?  \na) Vitamin A  \nb) Vitamin C  \nc) Vitamin D  \nd) Vitamin PP",
            "reference-answers": [
                "b) Vitamin C"
            ]
        },
        {
            "question": "In which cell organelle is energy primarily stored for the body's use?",
            "reference-answers": [
                "mitochondria"
            ]
        }
    ],
    "ukr-biology/book02/topic02-Травлення/text.en.txt": [
        {
            "question": "What is the cause of caries and what are the prevention measures according to the text?",
            "reference-answers": [
                "The cause of caries is the destruction of tooth enamel under the influence of bacteria or physical/chemical factors. Prevention measures include compliance with the rules of dental and oral hygiene and proper nutrition."
            ]
        },
        {
            "question": "What structural feature of the small intestine enhances nutrient absorption?",
            "reference-answers": [
                "The structural feature of the small intestine that enhances nutrient absorption is the presence of villi. These villi increase the surface area of the intestinal walls, allowing for more efficient absorption of nutrients into the bloodstream and lymphatic system."
            ]
        },
        {
            "question": "What is the role of bile in digestion?",
            "reference-answers": [
                "Bile emulsifies fats, breaking them into smaller droplets to facilitate digestion and absorption by digestive enzymes in the small intestine."
            ]
        }
    ],
    "ukr-biology/book02/topic03-Дихання/text.en.txt": [
        {
            "question": "**Question:**  \nWhat is the role of the respiratory center in the medulla oblongata, and how do the intercostal muscles and diaphragm work together to regulate breathing?",
            "reference-answers": [
                "The respiratory center in the medulla oblongata controls the breathing process by sending signals to the diaphragm and intercostal muscles. The diaphragm contracts to draw air into the lungs during inhalation, while the intercostal muscles assist by expanding the ribcage. During exhalation, the diaphragm relaxes, and the intercostal muscles help push air out by moving the ribs inward. This coordination ensures efficient regulation of breathing."
            ]
        },
        {
            "question": "Explain the process of gas exchange in the human respiratory system, including the changes in oxygen and carbon dioxide levels in the air during inhalation and exhalation.",
            "reference-answers": [
                "Gas exchange in the human respiratory system involves the intake of oxygen-rich air during inhalation and the expulsion of carbon dioxide-rich air during exhalation. Upon inhalation, air enters the nasal cavity, travels through the pharynx, larynx, trachea, and bronchi, eventually reaching the alveoli in the lungs. Here, oxygen diffuses from the alveoli into the surrounding capillaries, while carbon dioxide moves from the capillaries into the alveoli. This process is facilitated by the structure of the alveoli, which are rich in capillaries, allowing for efficient gas exchange. \n\nDuring exhalation, the carbon dioxide-rich air is expelled from the alveoli through the bronchi, trachea, and out of the body. The changes in air composition are evident: inhaled air has 20.9% oxygen and 0.04% carbon dioxide, while exhaled air has 16.3% oxygen and 4.00% carbon dioxide. Additionally, the process of internal respiration occurs at the tissue level, where oxygen is delivered to cells, and carbon dioxide is produced as a waste product, which then diffuses into the tissue fluid and eventually into the capillaries to be expelled during exhalation."
            ]
        },
        {
            "question": "What are the components that make up the Vital Capacity of the Lungs (VLC)?",
            "reference-answers": [
                "The Vital Capacity of the Lungs (VLC) is composed of the reserve volume, respiratory volume (tidal volume), and supplementary volume."
            ]
        }
    ],
    "ukr-biology/book02/topic04-Транспорт речовин/text.en.txt": [
        {
            "question": "**Question:** What is the functional unit of the kidneys?",
            "reference-answers": [
                "The functional unit of the kidneys is the nephron."
            ]
        },
        {
            "question": "What is the process by which primary urine is formed in the kidneys, and where does it occur?",
            "reference-answers": [
                "The process by which primary urine is formed in the kidneys is filtration, and it occurs in the glomerulus of the nephron."
            ]
        },
        {
            "question": "What is the functional unit of the kidneys, and what are its main structures?",
            "reference-answers": [
                "The functional unit of the kidneys is the nephron. Its main structures include the glomerulus, the nephron capsule (Bowman's capsule), and the convoluted tubules."
            ]
        }
    ],
    "ukr-biology/book02/topic05-Опора та рух/text.en.txt": [
        {
            "question": "What are the types of muscles and what are their respective functions?",
            "reference-answers": [
                "Muscles are classified into types based on their shape, size, or function, as follows:\n\n**Types of muscles based on shape:**\n1. **Spindle-shaped:** Examples include the biceps and quadriceps.\n2. **Square:** Examples include the square-shaped muscles like the quadriceps.\n3. **Triangular:** Examples include the deltoid muscle.\n4. **Ribbon-shaped:** Examples include the rectus abdominis.\n5. **Circular (sphincters):** Examples include the circular muscles around openings in the body, such as the mouth.\n\n**Types of muscles based on function:**\n1. **Flexors:** These muscles bend parts of the body, such as the biceps.\n2. **Extensors:** These muscles extend parts of the body, such as the triceps.\n3. **Abductors:** These muscles move limbs away from the body, such as the gluteus medius.\n4. **Adductors:** These muscles bring limbs toward the body, such as the adductor major.\n5. **Rotators:** These muscles rotate limbs around their axis, such as the sartorius.\n\nEach type of muscle is associated with specific functions and examples as listed above."
            ]
        },
        {
            "question": "**Question:**  \nClassify muscles based on their shape, size, and function, and provide examples for each category.",
            "reference-answers": [
                "Muscles can be classified as follows:\n\n**By Shape:**\n1. **Spindle-shaped**: Biceps.\n2. **Square**: Quadrate muscle of the thigh.\n3. **Triangular**: Deltoid muscle.\n4. **Ribbon-shaped**: Rectus abdominis.\n5. **Circular**: Orbital muscle of the mouth.\n\n**By Size:**\n1. **Long muscles**: Biceps, triceps, quadriceps, sartorius, and calf muscles.\n2. **Short muscles**: Intercostal muscles.\n3. **Broad muscles**: Latissimus dorsi.\n\n**By Function:**\n1. **Flexors**: Biceps.\n2. **Extensors**: Triceps.\n3. **Abductors**: Gluteus medius muscle.\n4. **Adductors**: Adductor major muscle.\n5. **Rotators**: Sartorius muscle."
            ]
        },
        {
            "question": "Which type of muscle is the deltoid muscle classified as, based on its shape?",
            "reference-answers": [
                "The deltoid muscle is classified as a triangular type of muscle based on its shape."
            ]
        }
    ],
    "ukr-biology/book02/topic06-Зв’язок організму людини із зовнішнім середовищем. Нервова система/text.en.txt": [
        {
            "question": "What neurotransmitters are used by the sympathetic and parasympathetic nervous systems?",
            "reference-answers": [
                "The sympathetic nervous system uses noradrenaline, while the parasympathetic nervous system uses acetylcholine as neurotransmitters."
            ]
        },
        {
            "question": "What is the primary function of the cerebellum?",
            "reference-answers": [
                "The primary function of the cerebellum is to coordinate various motor acts and adapt the body's motor reactions to environmental conditions."
            ]
        },
        {
            "question": "What are the three membranes that externally cover the spinal cord?",
            "reference-answers": [
                "The three membranes that externally cover the spinal cord are the dura, the arachnoid, and the soft."
            ]
        }
    ],
    "ukr-biology/book02/topic07-Зв’язок організму людини із зовнішнім середовищем. Сенсорні системи/text.en.txt": [
        {
            "question": "The middle ear contains the auditory ossicles.\n\n**Answer:** a) auditory ossicles",
            "reference-answers": [
                "a) auditory ossicles"
            ]
        },
        {
            "question": "What is the function of the auditory ossicles in the middle ear and how do they contribute to sound perception?",
            "reference-answers": [
                "The auditory ossicles in the middle ear amplify and transmit sound vibrations from the eardrum to the inner ear, facilitating the perception of sound by converting mechanical vibrations into movements within the inner ear, which then activate the auditory receptors in the cochlea."
            ]
        },
        {
            "question": "What structures are located in the middle ear?",
            "reference-answers": [
                "The middle ear contains the tympanic cavity, Eustachian tube, eardrum, and the three auditory ossicles."
            ]
        }
    ],
    "ukr-biology/book02/topic08-Вища нервова діяльність/text.en.txt": [
        {
            "question": "**Answer:**\nThe correct answer is d) salivation at the smell of food. This is a conditioned reflex because it is a learned response, where the stimulus (smell of food) has been paired with the natural reflex (salivation), leading to a conditioned reaction.",
            "reference-answers": [
                "The correct answer is d) salivation at the smell of food. This is a conditioned reflex because it is a learned response, where the stimulus (smell of food) has been paired with the natural reflex (salivation), leading to a conditioned reaction."
            ]
        },
        {
            "question": "Which type of thinking is used when playing chess?",
            "reference-answers": [
                "Verbal-logical thinking is used when playing chess."
            ]
        },
        {
            "question": "Which stage of sleep is associated with the occurrence of dreams?",
            "reference-answers": [
                "The fifth stage of sleep, REM sleep, is associated with the occurrence of dreams."
            ]
        }
    ],
    "ukr-biology/book02/topic09-Регуляція функцій організму/text.en.txt": [
        {
            "question": "Differentiate between natural passive, artificial passive, natural active, and artificial active immunity, providing examples for each.",
            "reference-answers": [
                "Natural passive immunity occurs when antibodies are transferred from one person to another without the recipient's immune system producing them. For example, a baby may acquire antibodies from its mother through the placenta or colostrum (初乳), providing temporary protection against certain infections.\n\nArtificial passive immunity is created by人为地注射 antibodies into a person's body. For example, antivenom treatments (如蛇咬伤的血清) are made by immunizing horses with specific toxins and then extracting antibodies from their blood to treat envenomed individuals.\n\nNatural active immunity results from the body being exposed to and fighting a pathogen, leading to the production of its own antibodies. For example, if a person contracts measles, their immune system produces antibodies that provide long-term protection against future infections.\n\nArtificial active immunity is induced by administering a vaccine, which contains killed or weakened pathogens or parts of them, to stimulate the immune system to produce its own antibodies. For example,接种牛痘疫苗 (vaccination against smallpox) protects the body by preparing its immune system to recognize and combat the virus."
            ]
        },
        {
            "question": "What is the difference between natural and artificial immunity, and provide an example of each?",
            "reference-answers": [
                "Natural immunity arises without human intervention, such as through maternal antibodies or after recovering from a disease. An example is immunity to measles after contracting the illness. Artificial immunity is created intentionally, often through vaccines. For example, vaccination against smallpox provides artificial active immunity."
            ]
        },
        {
            "question": "What is the main function of the immune system, and how does it protect the body from pathogens?",
            "reference-answers": [
                "The main function of the immune system is to protect the body from foreign organisms that can cause diseases and from its own cells, such as cancer cells. It achieves this through immune reactions, which involve recognizing antigens (foreign molecules like proteins and carbohydrates). The immune system uses two primary mechanisms: cellular immunity, provided by T-lymphocytes that destroy pathogens directly, and humoral immunity, provided by B-lymphocytes that produce antibodies to neutralize antigens. These processes are supported by organs like the thymus, spleen, tonsils, lymph nodes, and bone marrow, which help produce and distribute the necessary cells and substances to protect the body."
            ]
        }
    ],
    "ukr-biology/book02/topic10-Розмноження та розвиток людини/text.en.txt": [
        {
            "question": "**Question:**  \nThe embryonic period of development is characterized by which key processes and why is it significant in human development?",
            "reference-answers": [
                "The embryonic period of development is characterized by key processes such as cell division, the formation of germ layers and membranes, implantation, and the development of tissues and organs. These processes are significant because they establish the foundation for the formation of all body systems and structures. The period is critical as it is the time when the embryo transitions from a single cell to a complex organism with various organ systems, making it a sensitive and vulnerable phase where disruptions can lead to serious developmental issues."
            ]
        },
        {
            "question": "Where does the process of fertilization usually occur?",
            "reference-answers": [
                "The process of fertilization usually occurs in the fallopian tubes."
            ]
        },
        {
            "question": "What is the primary sexual characteristic of a woman?",
            "reference-answers": [
                "d) the presence of a vagina"
            ]
        }
    ],
    "ukr-biology/book03/topic01-Хімічний склад клітинита біологічні молекули/text.en.txt": [
        {
            "question": "What is the primary function of ATP in the cell?",
            "reference-answers": [
                "The primary function of ATP in the cell is to provide energy for biochemical reactions, particularly for processes like growth, movement, and reproduction. ATP releases energy when it breaks down into ADP and orthophosphoric acid, which is then used by enzymes to drive various cellular activities."
            ]
        },
        {
            "question": "What are the three components of a nucleotide?",
            "reference-answers": [
                "The three components of a nucleotide are a nitrogenous base, a monosaccharide (ribose or deoxyribose), and an orthophosphate acid residue."
            ]
        },
        {
            "question": "What is the main function of nucleic acids in living organisms?",
            "reference-answers": [
                "The main function of nucleic acids in living organisms is to store and reproduce hereditary information."
            ]
        }
    ],
    "ukr-biology/book03/topic02-Структура клітини/text.en.txt": [
        {
            "question": "What are the key structural features that differentiate prokaryotic cells from eukaryotic cells, based on the given text?",
            "reference-answers": [
                "Prokaryotic cells lack a nucleus and membrane organelles, while eukaryotic cells have a nucleus and various membrane organelles. Eukaryotic cells possess a cytoskeleton and are generally larger, enabling them to form multicellular organisms. Prokaryotic cells, being simpler, lack these features and cannot form complex multicellular structures."
            ]
        },
        {
            "question": "Which organelle is responsible for photosynthesis in plant cells?",
            "reference-answers": [
                "The organelle responsible for photosynthesis in plant cells is the chloroplast."
            ]
        },
        {
            "question": "Which structures are unique to eukaryotic cells and which are unique to prokaryotic cells?",
            "reference-answers": [
                "Unique to eukaryotic cells: nucleus, membrane organelles (e.g., mitochondria, endoplasmic reticulum), and a developed cytoskeleton.\n\nUnique to prokaryotic cells: cell wall containing murein, flagella, and the nucleoid region with circular DNA."
            ]
        }
    ],
    "ukr-biology/book03/topic03-Принципи функціонування клітини/text.en.txt": [
        {
            "question": "**Question:**  \nDescribe the relationship between photosynthesis and cellular respiration, including their interdependence and the substances exchanged between plants and animals.",
            "reference-answers": [
                "Photosynthesis and cellular respiration are interdependent processes that form a vital cycle essential for life on Earth. Photosynthesis occurs in plants, capturing light energy to produce glucose and oxygen (6CO₂ + 6H₂O → C₆H₁₂O₆ + 6O₂). This oxygen is utilized by animals during cellular respiration, where they break down glucose to produce energy (ATP), releasing carbon dioxide (CO₂) as a byproduct. Plants, in turn, use this CO₂ for photosynthesis. This exchange of oxygen and carbon dioxide is crucial, maintaining atmospheric balance and sustaining life. Without photosynthesis, oxygen levels would drop, and without cellular respiration, plants wouldn't receive the necessary CO₂. Thus, these processes are mutually reliant, supporting each other and the planet's ecosystems."
            ]
        },
        {
            "question": "Where in the chloroplast does the light phase of photosynthesis occur, and where does the dark phase take place?",
            "reference-answers": [
                "The light phase of photosynthesis occurs in the thylakoid membranes of the chloroplast, while the dark phase takes place in the stroma of the chloroplast."
            ]
        },
        {
            "question": "**Question:**  \nWhat is the main difference between autotrophs and heterotrophs in terms of their energy and carbon sources?",
            "reference-answers": [
                "Autotrophs obtain their carbon from inorganic sources (carbon dioxide) and use light energy, while heterotrophs obtain their carbon from organic sources and use the energy from oxidizing these organic compounds."
            ]
        }
    ],
    "ukr-biology/book03/topic04-Збереження та реалізаціяспадкової інформації/text.en.txt": [
        {
            "question": "What is the main difference between mitosis and meiosis in terms of the number of daughter cells and their chromosome content?",
            "reference-answers": [
                "Mitosis results in two daughter cells with the same number of chromosomes as the parent cell, while meiosis results in four daughter cells, each with half the number of chromosomes of the parent cell."
            ]
        },
        {
            "question": "What are the four phases of mitosis, listed in the correct sequence?",
            "reference-answers": [
                "The four phases of mitosis, listed in the correct sequence, are: **prophase, metaphase, anaphase, telophase**."
            ]
        },
        {
            "question": "What is the penultimate phase of mitosis?",
            "reference-answers": [
                "d) telophase"
            ]
        }
    ],
    "ukr-biology/book03/topic05-Закономірності успадкування ознак/text.en.txt": [
        {
            "question": "Question: What are the examples of hereditary human diseases mentioned in the text?",
            "reference-answers": [
                "The examples of hereditary human diseases mentioned are: Down syndrome, Patau syndrome, Edwards syndrome, Klinefelter syndrome, cystic fibrosis, phenylketonuria, color blindness, and hemophilia."
            ]
        },
        {
            "question": "What are the main types of mutations, and can you provide examples of hereditary human diseases discussed in the text?",
            "reference-answers": [
                "The main types of mutations are gene mutations, chromosomal mutations, and genomic mutations. Examples of hereditary human diseases discussed include Down syndrome, Patau syndrome, Edwards syndrome, Klinefelter syndrome, hemophilia, cystic fibrosis, phenylketonuria, and color blindness."
            ]
        },
        {
            "question": "Question: Explain the classification of mutations based on the level of hereditary material and provide examples for each type.",
            "reference-answers": [
                "Mutations are classified based on the level of hereditary material they affect:\n\n1. **Gene Mutations**: These involve changes at the level of a single gene, such as alterations in the DNA sequence. Examples include mutations causing cystic fibrosis and phenylketonuria.\n\n2. **Chromosomal Mutations**: These affect a larger segment of a chromosome, such as inversions or deletions. An example is Le Gene syndrome, resulting from a chromosome fragment loss.\n\n3. **Genomic Mutations**: These involve changes in the entire set of chromosomes, such as polyploidy or aneuploidy. Down syndrome is an example, where an extra chromosome 21 causes trisomy 21."
            ]
        }
    ],
    "ukr-biology/book03/topic06-Еволюція органічного світу/text.en.txt": [
        {
            "question": "Which early hominid species lived in both Africa and Europe?",
            "reference-answers": [
                "Homo heidelbergensis"
            ]
        },
        {
            "question": "When did Sahelanthropus, Ardipithecus, Paranthropus, and Neanderthals live?",
            "reference-answers": [
                "Sahelanthropus lived approximately 6–7 million years ago, Ardipithecus lived between 4–6 million years ago, Paranthropus lived from 2.5 to 0.9 million years ago, and Neanderthals lived approximately from 400,000 to 30,000 years ago."
            ]
        },
        {
            "question": "Who lived in Africa approximately 7 million years ago?",
            "reference-answers": [
                "Sahelanthropus"
            ]
        }
    ],
    "ukr-biology/book03/topic07-Біорізноманіття/text.en.txt": [
        {
            "question": "What are the main systematic categories introduced by Carl Linnaeus, and what are the hierarchical divisions for the kingdoms of Plants and Animals?",
            "reference-answers": [
                "The main systematic categories introduced by Carl Linnaeus are species, genus, family, order, class, division (for plants), and phylum (for animals). The hierarchical divisions are as follows:\n\nFor the Plant Kingdom:\n- Species → Genus → Family → Order → Class → Division → Kingdom of Plants\n\nFor the Animal Kingdom:\n- Species → Genus → Family → Order → Class → Phylum → Kingdom of Animals"
            ]
        },
        {
            "question": "What is the taxonomic category that comes after 'Class' in the classification hierarchy for plants and animals respectively?",
            "reference-answers": [
                "The taxonomic category after 'Class' is 'Division' for plants and 'Phylum' for animals."
            ]
        },
        {
            "question": "What are the main systematic categories, and how do the hierarchies of plants and animals differ in terms of these categories?",
            "reference-answers": [
                "The main systematic categories introduced by Carl Linnaeus are species, genus, family, order, class, division, and kingdom. For plants, the hierarchy is:\n\nSpecies → Genus → Family → Order → Class → Division → Kingdom of Plants.\n\nFor animals, the hierarchy is:\n\nSpecies → Genus → Family → Order → Class → Phylum → Kingdom of Animals.\n\nThe primary difference is that plants use \"division\" while animals use \"phylum\" after the class category."
            ]
        }
    ],
    "ukr-biology/book03/topic08-Надорганізмові біологічні системи/text.en.txt": [
        {
            "question": "13. Write the names of the components of the biosphere to which the indicated lists of objects belong.\n\na) Basalt, granite  \nb) Bacteria, plants, animals, fungi  \nc) Soil, silt  \nd) Coal, peat, chalk, oxygen, carbon dioxide  \n\na) Inorganic  \nb) Living  \nc) Inorganic  \nd) Biogenic",
            "reference-answers": [
                "a) Inorganic  \nb) Living  \nc) Inorganic  \nd) Biogenic"
            ]
        },
        {
            "question": "What are the main factors that can disrupt the stability of an ecosystem, and how do they affect the biosphere?",
            "reference-answers": [
                "The main factors that can disrupt the stability of an ecosystem are abiotic, biotic, and anthropogenic factors. These factors affect the biosphere by altering species diversity, genetic diversity, and ecosystem diversity, ultimately reducing the resilience of the biosphere.\n\n1. **A biotic factors** (non-living factors such as temperature, water salinity, and pressure) can disrupt ecosystems. For example, a volcanic eruption can destroy the ecosystem of a particular territory, leading to the extinction of many species and altering the biosphere's structure.\n\n2. **Biotic factors** (living organisms) can also disrupt ecosystems. The introduction of new species, such as goats on islands, can lead to overgrazing and habitat destruction, turning islands into deserts and drastically changing the biosphere. Additionally, the evolution of new groups of organisms, like gymnosperms, can alter ecosystems by changing vegetation patterns and affecting water systems.\n\n3. **Anthropogenic factors** (human-induced factors) often lead to ecosystem instability. Human activities, such as deforestation, pollution, and overfishing, disrupt ecosystems and reduce biodiversity. Artificial ecosystems, like agrocenoses, rely heavily on human intervention and cannot self-regulate, making them vulnerable to collapse if human support is removed.\n\nThese disruptions can lead to a loss of biodiversity, reducing the resilience of the biosphere and making it harder for ecosystems to recover from disturbances. For example, monoculture farming reduces genetic diversity, making crops more vulnerable to diseases and pests, and disrupting the balance of ecosystems."
            ]
        },
        {
            "question": "Question:  \n**Categorize the following objects into the components of the biosphere: basalt, granite, bacteria, plants, animals, fungi, soil, silt, coal, peat, chalk, oxygen, and carbon dioxide.**",
            "reference-answers": [
                "Living: bacteria, plants, animals, fungi  \nBiogenic: coal, peat, chalk, oxygen, carbon dioxide  \nInorganic: basalt, granite, soil, silt"
            ]
        }
    ],
    "ukr-biology/book03/topic09-Біологія як основа біотехнологіїта медицини/text.en.txt": [
        {
            "question": "Question: Based on the text, match the branches of biotechnology (cell engineering, genetic engineering, traditional biotechnology) to the following processes:\n\nProcesses:\na) Technologies for obtaining recombinant DNA\nb) Technologies for cell engineering\nc) Technologies using fermentation processes\n\nBranches:\n1. Genetic Engineering\n2. Cell Engineering\n3. Traditional Biotechnology\n\nMatch each process to the correct branch.",
            "reference-answers": [
                "a) Genetic Engineering  \nb) Cell Engineering  \nc) Traditional Biotechnology"
            ]
        },
        {
            "question": "What are the key differences between classical selection and genetic engineering (recombinant DNA technology) in terms of duration of use, number of genes modified, and the time required to develop new forms?",
            "reference-answers": [
                "Classical selection and genetic engineering (recombinant DNA technology) differ in several key aspects:\n\n1. **Duration of Use**: Classical selection has been used for over 30 years (likely thousands of years, as implied by the text), while recombinant DNA technology is a modern method that has been around for about 40 years (since the 1980s).\n\n2. **Number of Genes Modified**: Classical selection works with several thousand genes at once, whereas recombinant DNA technology targets and modifies only one specific gene.\n\n3. **Time Required to Develop New Forms**: Classical selection requires a significant amount of time to develop new forms, while recombinant DNA technology is faster and requires less time to achieve desired results."
            ]
        },
        {
            "question": "What is the role of genetic engineering in modern medicine, and what are some key developments and applications in this field?",
            "reference-answers": [
                "Genetic engineering plays a significant role in modern medicine, primarily through the production of life-saving drugs, gene therapy, and disease diagnostics. Key developments include the production of insulin since 1982, revolutionizing diabetes treatment. Gene therapy has successfully treated hereditary diseases by introducing healthy genes, with the first human trial in 1990. Additionally, genetic engineering enables the production of various therapeutic proteins and enzymes, such as growth hormone and interferon. It also enhances disease diagnostics, offering highly sensitive methods for detecting infections, hereditary disorders, and cancers. Despite concerns, extensive research has not yielded reliable evidence of negative impacts, highlighting the potential benefits of genetic engineering in medicine."
            ]
        }
    ],
    "ukr-biology/book04/topic01-Біорізноманіття/text.en.txt": [
        {
            "question": "**Question:**  \nWhat are the main differences between true fungi and slime molds?",
            "reference-answers": [
                "The main differences between true fungi and slime molds are as follows:\n\n1. **Nutritional Mode**: True fungi are osmotrophs, absorbing organic and inorganic substances from the environment through their mycelium. Slime molds, while also heterotrophic, can exist in unicellular or multinucleate forms and often exhibit different feeding strategies.\n\n2. **Structural Organization**: True fungi have a well-defined mycelial structure with internal partitions (septa) in their hyphae. Slime molds, particularly true slime molds, lack such partitions and instead form plasmodia or other aggregating structures during their life cycle.\n\n3. **Life Cycle**: True fungi produce spores through sexual processes that result in zygospores, ascospores, or basidiospores. Slime molds have a more complex life cycle, alternating between unicellular myxamoebae and multinucleate plasmodia, which then form fruiting bodies for spore production.\n\n4. **Reproductive Structures**: True fungi typically form specific fruiting bodies (like mushrooms) for spore dispersal. Slime molds also form fruiting bodies, but these are structurally different and serve the same purpose of spore dispersal.\n\nIn summary, while both true fungi and slime molds are heterotrophic eukaryotes, they differ significantly in their structural, reproductive, and life cycle characteristics."
            ]
        },
        {
            "question": "What are the key characteristics of basidiomycetes and which representatives are typically associated with this group?",
            "reference-answers": [
                "The key characteristics of basidiomycetes include a mycelium divided into cells, each containing one or two nuclei, their ability to form fruiting bodies, and the production of basidiospores through sexual processes. They function as saprotrophs or parasites. Representatives of this group include boletus, buttercup, and smut."
            ]
        },
        {
            "question": "What are the three main groups of eukaryotes, and can you provide examples of organisms in each group as discussed in the text?",
            "reference-answers": [
                "The three main groups of eukaryotes, as discussed in the text, are:\n\n1. **Fungi and Fungus-like Organisms**  \n   These include true fungi (e.g., mushrooms, truffle, bread mold), false fungi (e.g., oomycetes), and slime molds (e.g., Physarum, Dictyostelium). They are characterized by their heterotrophic生活方式, absorption of nutrients through osmotrophy, and the presence of a cell wall.\n\n2. **Animals**  \n   This group includes multicellular animals such as insects, birds, mammals, and even simpler forms like sponges and corals. Animals are heterotrophic and obtain nutrients by eating food, which they digest.\n\n3. **Plants**  \n   The text mentions land plants, including flowering plants and gymnosperms, such as trees like Scots pine. Plants are autotrophic, meaning they synthesize their own food through photosynthesis.\n\nThese groups represent the major divisions of eukaryotes discussed in the text."
            ]
        }
    ],
    "ukr-biology/book04/topic02-Обмін речовин і перетворення енергії/text.en.txt": [
        {
            "question": "What are the primary systems of the human body that eliminate toxic substances, and how do they function in this process?",
            "reference-answers": [
                "The primary systems of the human body that eliminate toxic substances are the kidneys, intestines, lungs, and skin. \n\n1. **Kidneys**: The kidneys are the main organ for eliminating toxic substances. They filter water-soluble toxins from the blood into the nephrons through passive filtration (diffusion) and active transport. This process results in the formation of urine, which carries the toxins out of the body.\n\n2. **Intestines**: Fat-soluble compounds and their breakdown products are excreted through the intestines. These substances are eliminated as part of feces, either through bile secretion from the gallbladder, direct secretion by intestinal epithelial cells, or with undigested food residues.\n\n3. **Lungs**: Volatile compounds are excreted through the lungs. For example, acetone, a ketone body, is expelled through respiration, especially during fasting or metabolic disorders.\n\n4. **Skin**: The skin eliminates water-soluble compounds through sweat glands. This explains why substances like alcohol and their breakdown products can cause an unpleasant odor on the skin the next day.\n\nThese systems work together, often with the support of artificial methods like dialysis, to neutralize and remove toxic substances from the body."
            ]
        },
        {
            "question": "Which systems of the human body are primarily responsible for the elimination of toxic substances?",
            "reference-answers": [
                "The primary systems responsible for the elimination of toxic substances are the kidneys, intestines, lungs, and skin."
            ]
        },
        {
            "question": "Explain the role of the nervous and humoral systems in the detoxification process and discuss both natural and artificial methods of detoxification.",
            "reference-answers": [
                "The nervous and humoral systems play significant roles in the body's detoxification processes. The autonomic nervous system, which includes the sympathetic and parasympathetic branches, regulates the excretion of toxic substances. The sympathetic nervous system influences urine production by adjusting blood flow in the kidneys, while the parasympathetic system affects the excretion of sodium chloride (NaCl), both of which are crucial for eliminating toxins.\n\nThe humoral system, involving hormones and endocrine glands, indirectly supports detoxification by regulating metabolic processes that influence the efficiency of toxin elimination. For instance, hormones can adjust the body's metabolic rate, affecting how quickly toxins are processed and excreted.\n\nIn natural detoxification, the kidneys, intestines, lungs, and skin work together to remove toxins. Artificial methods, such as dialysis, hemofiltration, and hemoperfusion, are used when the body's systems are overwhelmed. Dialysis filters blood through a semipermeable membrane, while hemofiltration and hemoperfusion remove toxins by other means, such as adsorption. Antidotes like activated carbon are also used to prevent toxin absorption.\n\nThus, the nervous and humoral systems collaborate to regulate detoxification, and artificial methods are employed to enhance or replace natural processes when necessary."
            ]
        }
    ],
    "ukr-biology/book04/topic03-Спадковість і мінливість/text.en.txt": [
        {
            "question": "What are some examples of chromosomal disorders and genetic diseases discussed in the text, and what are their causes?",
            "reference-answers": [
                "Examples of chromosomal disorders include Down syndrome, caused by trisomy 21; Patau syndrome (trisomy 13) and Edwards syndrome (trisomy 18), which cause significant developmental issues. Sex chromosome disorders include Klinefelter syndrome (XXY) and Shereshevsky-Turner syndrome (single X chromosome). Genetic diseases mentioned are Phenylketonuria, due to a gene on chromosome 12 affecting phenylalanine processing; Hemophilia, an X-linked disorder impacting blood clotting; and Sickle cell anemia, caused by a mutation in chromosome 11 altering hemoglobin structure. Le Gen syndrome is another example, resulting from a chromosome fragment loss."
            ]
        },
        {
            "question": "Using the formula \\( C = \\frac{(M\\% - D\\%)}{(100 - D\\%)} \\), calculate the hereditary contribution to hypertension where \\( M\\% = 26 \\) and \\( D\\% = 21 \\).",
            "reference-answers": [
                "The hereditary contribution to hypertension is calculated as follows:\n\nC = (26% - 21%) / (100% - 21%) = 5% / 79% ≈ 6.33%\n\nANSWER: 6.33%"
            ]
        },
        {
            "question": "What is the percentage of hereditary contribution (C) to the development of the following diseases: Hypertension, Myocardial infarction, Stroke, Rheumatism? Also, describe the methods used to study the hereditary component in these diseases.",
            "reference-answers": [
                "The percentage of hereditary contribution (C) for the diseases is calculated as follows:\n\n1. **Hypertension**: C ≈ 24.49%  \n2. **Myocardial infarction**: C ≈ 14.74%  \n3. **Stroke**: C = 18.75%  \n4. **Rheumatism**: C ≈ 22.11%  \n\nThe methods used to study the hereditary component involve comparing the concordance rates of monozygotic and dizygotic twins through the twin method. This approach effectively measures hereditary influence by analyzing the similarity of traits in genetically identical versus less identical twins."
            ]
        }
    ],
    "ukr-biology/book04/topic04-Репродукція та розвиток організмів/text.en.txt": [
        {
            "question": "What are the three main stages of human embryogenesis and what significant events occur during each stage?",
            "reference-answers": [
                "The three main stages of human embryogenesis and significant events during each stage are as follows:\n\n1. **Initial Stage (First Week):**\n   - Formation of the zygote.\n   -Implantation of the zygote into the uterine mucous membrane.\n\n2. **Embryonic Stage (Weeks 2–8):**\n   - Formation of three germ layers and membranes.\n   - Embryonic induction occurs, where organizer cells release signals to differentiate cells into specific organs and tissues.\n\n3. **Fetal Stage (From Week 9 Until Birth):**\n   - Development of the fetus.\n   - Formation of the placenta.\n   - Development of amniotic fluid to protect the embryo."
            ]
        },
        {
            "question": "What are the subdivisions of the embryonic period of human development, and what are the key events that occur during each subdivision?",
            "reference-answers": [
                "The embryonic period of human development is divided into three subperiods, each with distinct key events:\n\n1. **Initial Subperiod (Weeks 1)**: This stage begins with the formation of the zygote through fertilization. The zygote undergoes cleavage divisions to form a blastocyst, which then implants into the uterine mucosa. Implantation is crucial as it establishes the connection to the mother's body for nutrient exchange.\n\n2. **Embryonic Subperiod (Weeks 2–8)**: During this phase, the major organ systems start to develop. The embryo undergoes gastrulation, forming three germ layers: ectoderm, mesoderm, and endoderm. Each layer gives rise to specific organs and tissues. Key developments include the formation of the heart, which begins to beat, and the differentiation of the body into head, limb buds, and tail.\n\n3. **Fetal Subperiod (Weeks 9 to Birth)**: This stage is marked by rapid growth and maturation of all organ systems. The placenta fully forms, facilitating nutrient and oxygen exchange from the mother. The fetus continues to develop its respiratory, digestive, and circulatory systems, preparing for life outside the womb."
            ]
        },
        {
            "question": "How do the biological and social factors influence human reproductive behavior, as illustrated by the comparison between agricultural practices in Europe and Africa?",
            "reference-answers": [
                "Biological and social factors significantly influence human reproductive behavior, as illustrated by the comparison between agricultural practices in Europe and Africa. Biologically, human reproductive behavior is based on instinctive programs established over millions of years of evolution. However, these instincts are shaped and modified by social environments. In Europe, the use of the plow, which required heavy physical labor, led to the predominance of monogamous marriages. This social structure ensured the effective transfer of genetic and material inheritance, as men were primarily responsible for agricultural work. In contrast, in many African societies, the cultivation of land with a hoe allowed women and children to play a more active role in agriculture. This contributed to the consolidation of polygynous marriages, as men with multiple wives and children had a competitive advantage in transferring genetic and material resources to their descendants. Thus, the interaction of biological instincts with social practices shaped distinct reproductive behaviors in different regions."
            ]
        }
    ],
    "ukr-biology/book05/topic01-Адаптації/text.en.txt": [
        {
            "question": "How do parasites interact with their hosts, what are the different types of parasites and their methods of entering hosts, what are the consequences of these interactions, and how do biological rhythms and a healthy lifestyle contribute to the overall well-being of organisms?",
            "reference-answers": [
                "Parasites interact with their hosts through various levels of biological interaction, including molecular, cellular, and organismal mechanisms. Endoparasites, such as ascarids and flatworms, enter hosts through ingestion of contaminated food or via intermediate hosts, while ectoparasites like fleas and lice attach to the host's surface, using specialized structures for clinging and feeding. The consequences of these interactions can range from tissue damage and formation of structures like galls or cysts to the death of the host, though some parasites and hosts coexist in a relatively balanced manner.\n\nBiological rhythms, driven by external factors like day-night cycles and seasons, influence organisms' behaviors and physiological processes, aiding in adaptation. A healthy lifestyle, encompassing proper nutrition, physical activity, and hygiene, contributes to overall well-being by reducing disease risks and maintaining optimal health."
            ]
        },
        {
            "question": "Explain the life cycle of the pork tapeworm and describe how it affects its host organism, including the roles of intermediate and definitive hosts.",
            "reference-answers": [
                "The pork tapeworm (Taenia solium) has a complex life cycle involving two types of hosts. The definitive host, typically humans, ingests the tapeworm's eggs, usually through undercooked or raw pork. In the digestive system, the eggs hatch into oncospheres, which penetrate the intestinal walls and develop into cysticerci within the tissues of the intermediate host, such as pigs. These cysticerci can persist for years, causing tissue damage. When the intermediate host is consumed by the definitive host, the cysticerci are ingested and develop into adult tapeworms in the intestine. The adult tapeworms produce eggs, completing the cycle. The presence of the tapeworm in the host can lead to health issues, including malnutrition and inflammation, as the worm parasitizes the host's digestive system."
            ]
        },
        {
            "question": "What are the mechanisms by which parasites can influence the behavior of their hosts, and can you provide examples from the text?",
            "reference-answers": [
                "Parasites can influence the behavior of their hosts through specific mechanisms, such as direct neural interference or chemical manipulation. For example, the emerald wasp injects poison into the cockroach's head, disrupting its brain function and making it easier for the wasp to lay its eggs on the cockroach. Another example is Toxoplasma gondii, which produces substances that alter the mouse's brain, reducing its fear of predators and increasing the chance of being eaten by a cat, the parasite's definitive host. These mechanisms allow the parasites to control the host's behavior, ensuring their survival and transmission."
            ]
        }
    ],
    "ukr-biology/book05/topic02-Біологічні основи здорового способу життя/text.en.txt": [
        {
            "question": "What are the common prevention measures for heart attacks, strokes, and diabetes mellitus as outlined in the text?",
            "reference-answers": [
                "The common prevention measures for heart attacks, strokes, and diabetes mellitus as outlined in the text are:\n\n1. Giving up bad habits, such as smoking and alcohol abuse.\n2. Controlling body weight.\n3. Checking blood pressure and blood glucose levels regularly.\n4. Reducing stress and prioritizing active rest."
            ]
        },
        {
            "question": "Explain the two main types of diabetes mellitus and discuss the preventive measures that can help reduce the risk of developing each type.",
            "reference-answers": [
                "The two main types of diabetes mellitus are Type 1 and Type 2. Type 1 diabetes occurs when the pancreas produces little to no insulin due to the destruction of beta cells, often an autoimmune response. Type 2 diabetes results from insulin resistance, where the body's cells do not effectively use insulin.\n\nPreventive measures for Type 2 include controlling body weight, treating viral infections, reducing stress, and regular blood glucose monitoring. Type 1 diabetes lacks specific preventive measures as it is often genetic or autoimmune."
            ]
        },
        {
            "question": "What are the common prevention strategies mentioned in the text for heart attack, stroke, and diabetes, and how do they contribute to the biological foundations of a healthy lifestyle?",
            "reference-answers": [
                "The common prevention strategies for heart attack, stroke, and diabetes mentioned in the text include:\n\n1. **Giving up bad habits**: Smoking and alcohol abuse are identified as risk factors for all three conditions. Avoiding these habits contributes to a healthy lifestyle by reducing the risk of developing these diseases.\n\n2. **Controlling body weight**: Overweight individuals are at a higher risk for heart attack, stroke, and diabetes. Maintaining a healthy weight through proper diet and exercise is a key strategy for preventing these conditions.\n\n3. **Regular health check-ups**: Checking blood pressure, glucose, and cholesterol levels is crucial for early detection and prevention. This includes regular blood pressure monitoring for stroke prevention and blood glucose checks for diabetes.\n\n4. **Reducing stress**: Chronic stress can contribute to various health issues, including cardiovascular diseases and diabetes. Managing stress through active rest and relaxation techniques helps maintain overall health.\n\n5. **Active rest**: Prioritizing physical activity and avoiding excessive strain on the body supports cardiovascular health and helps manage weight, further reducing the risk of these conditions.\n\nThese strategies collectively contribute to the biological foundations of a healthy lifestyle by promoting balanced nutrition, regular physical activity, stress management, and early detection of potential health issues, thereby reducing the risk of heart attack, stroke, and diabetes."
            ]
        }
    ],
    "ukr-biology/book05/topic03-Екологія/text.en.txt": [
        {
            "question": "What are the four main components of the biosphere, and what do they include?",
            "reference-answers": [
                "The four main components of the biosphere are:\n\n1. **Living matter**: Includes all living organisms such as bacteria, fungi, plants, and animals.\n2. **Biogenic matter**: Consists of matter created or processed by living organisms, such as coal, peat, and oxygen.\n3. **Inert matter**: Formed without the participation of living organisms, for example, basalt and granite.\n4. **Bioinert matter**: Created with the simultaneous participation of living organisms and factors of inanimate nature, such as soil and silt."
            ]
        },
        {
            "question": "Question: Explain the components of the biosphere and discuss how human activities have affected it, referencing Vernadsky's noosphere concept.",
            "reference-answers": [
                "The biosphere is composed of four main components: living matter, which includes all living organisms (bacteria, fungi, plants, animals, etc.); biogenic matter, which comprises materials created or processed by living organisms (coal, peat, oxygen, etc.); inert matter, formed without the participation of living organisms (e.g., basalt, granite); and bioinert matter, created through the simultaneous involvement of living organisms and inanimate natural factors (e.g., soil, silt).\n\nHuman activities have significantly impacted the biosphere over the past few hundred years. For instance, the extraction and burning of fossil fuels like coal have released large amounts of carbon dioxide into the atmosphere, altering the carbon cycle and contributing to global warming. Similarly, human actions such as deforestation, industrial processes, and pollution have disrupted ecosystems and biodiversity.\n\nV. I. Vernadsky's concept of the noosphere refers to a state of the biosphere where human mental activity becomes a determining factor in its development. This concept emphasizes the need for ecological thinking, where humans approach problem-solving with a focus on preserving and improving the natural environment. The noosphere signifies the integration of human intelligence and ecological awareness into the biosphere's evolution, highlighting the responsibility of humanity to act sustainably and avoid global ecological crises."
            ]
        },
        {
            "question": "What is the biosphere composed of?",
            "reference-answers": [
                "The biosphere is composed of four main components: living matter, biogenic matter, inert matter, and bioinert matter."
            ]
        }
    ],
    "ukr-biology/book05/topic04-Сталий розвиток та раціональне природокористування/text.en.txt": [
        {
            "question": "Discuss the role of ecological thinking in achieving sustainable development, using examples from the text to illustrate your points.",
            "reference-answers": [
                "Ecological thinking plays a crucial role in achieving sustainable development by fostering an understanding of the interconnectedness of human actions and natural ecosystems, thereby guiding decisions that balance current needs with future environmental health. This approach emphasizes the importance of considering ecological principles in decision-making processes, whether at the individual, community, or international level.\n\nOne example from the text illustrating the role of ecological thinking is the Soviet campaign to kill birds of prey. The belief that these birds hindered game hunting led to their eradication, which inadvertently caused rodent populations to surge, resulting in significant crop damage. This scenario highlights the consequences of a lack of ecological thinking, where short-term gains led to long-term environmental and economic losses.Had ecological thinking been applied, the role of birds of prey in controlling rodent populations would have been recognized, preventing such an ecological disaster.\n\nAnother example is the introduction of the prickly pear cactus in Australia. Brought in for hedges, it soon became invasive, disrupting local ecosystems. This problem was mitigated by introducing cactus moths, demonstrating how ecological thinking can lead to sustainable solutions by understanding and managing ecosystem interactions.\n\nThe construction of the Dnieper cascade of hydroelectric plants serves as another example. While providing electricity, the dams caused ecological disruptions, such as soil flooding and obstacles to fish spawning. Ecological thinking would have encouraged alternative solutions that minimized harm, emphasizing the need to consider long-term environmental impacts in development projects.\n\nThe issue of refrigeration and the ozone layer underscores the global dimension of ecological thinking. The use of freon, a convenient coolant, had severe environmental consequences. This problem required international cooperation to replace freon with ozone-friendly alternatives, illustrating how ecological thinking leads to global solutions for environmental challenges.\n\nIn conclusion, ecological thinking is essential for sustainable development as it promotes informed decision-making, prevents ecological disasters, and fosters international cooperation. By encouraging a holistic view of environmental interactions, ecological thinking ensures that human actions align with the preservation of natural systems, ultimately supporting the goals of sustainable development."
            ]
        },
        {
            "question": "What is the concept of sustainable development, and how can it be implemented at different levels (local, state, international)? Provide examples from the text to illustrate your explanation.",
            "reference-answers": [
                "The concept of sustainable development refers to the responsible use of natural resources to meet current needs without compromising the ability of future generations to meet their own needs. It involves making decisions that balance economic, social, and environmental considerations.\n\n**Implementation at Different Levels:**\n\n1. **Local Level:**\n   Sustainable development at the local level involves decisions such as waste management, construction of treatment facilities, and educational initiatives in ecology. For example, local communities can implement recycling programs or organize clean-up events to reduce waste and promote environmental awareness. Educational campaigns can encourage residents to adopt eco-friendly practices, ensuring the responsible use of resources.\n\n2. **State Level:**\n   At the state level, sustainable development is implemented through legislative and executive actions. Governments can establish environmental standards, such as the Euro-6 emissions standard, which limits carbon dioxide emissions from vehicles. This policy encourages car manufacturers to develop more fuel-efficient engines and alternative fuels, reducing pollution and fostering technological innovation.\n\n3. **International Level:**\n   Internationally, sustainable development requires global cooperation to address transboundary issues. For instance, cleaning large rivers or protecting the ozone layer necessitates international agreements. The Montreal Protocol, addressing ozone depletion by phasing out Freon use, exemplifies how global cooperation can lead to effective environmental solutions.\n\n**Conclusion:**\nSustainable development is implemented through localized actions, state policies, and international cooperation, each contributing to the responsible use of resources and addressing environmental challenges comprehensively."
            ]
        },
        {
            "question": "Question: Explain the concept of ecological thinking and provide examples of its application in addressing environmental issues.",
            "reference-answers": [
                "Ecological thinking is the consideration of phenomena and decision-making taking into account the interactions between humans and the natural environment, based on diverse and deep knowledge about the environment and a responsible attitude towards nature and human health. It involves understanding the interconnectedness of all organisms and processes on the planet, recognizing the need for international cooperation to solve environmental problems, and critically perceiving information to make adequate decisions.\n\nExamples of the application of ecological thinking in addressing environmental issues include:\n\n1. **Birds of Prey in the Soviet Union**: The destruction of birds of prey led to a massive reproduction of rodents, causing crop losses. Ecological thinking would have considered the role of birds of prey in controlling rodent populations, preventing such an ecological disaster.\n\n2. **Prickly Pear Cactus in Australia**: The introduction of the prickly pear cactus without natural regulators led to its uncontrollable growth. Ecological thinking would have recognized the need for a balanced ecosystem and the importance of natural regulators like the cactus moth.\n\n3. **Clover and Bumblebees in Australia**: The failure to consider the role of bumblebees in pollinating clover led to the need for their importation. Ecological thinking would have understood the dependency of clover reproduction on pollinators like bumblebees.\n\n4. **Dnieper Hydroelectric Power Plants**: The construction of dams disrupted the river's natural processes, affecting fish spawning and soil. Ecological thinking would have considered the long-term impacts on local ecosystems and sought alternative solutions to minimize harm.\n\n5. **Ozone Layer and Refrigerators**: The use of freon in refrigerators threatened the ozone layer. Ecological thinking led to the development of alternatives, demonstrating the importance of considering the broader environmental impacts of technological solutions.\n\nThese examples highlight the importance of ecological thinking in making informed decisions that prevent ecological disasters and promote sustainable development."
            ]
        }
    ],
    "ukr-biology/book05/topic05-Застосування результатів біологічних досліджень у медицині, селекції та біотехнології/text.en.txt": [
        {
            "question": "What are the targets of biological terrorism, and what are the potential consequences for the population or economy?",
            "reference-answers": [
                "The targets of biological terrorism include either the human population directly through the use of pathogens causing diseases like anthrax or typhoid, or the economy by targeting agricultural resources such as farm animals or crops. The potential consequences are significant for both the population and the economy: direct attacks can lead to widespread illness, fear, and death, while indirect attacks can result in economic instability, food shortages, and increased costs for disease control."
            ]
        },
        {
            "question": "**Question:**  \nHow can biology contribute to solving global problems, and what specific examples from the text illustrate this contribution?",
            "reference-answers": [
                "Biology contributes to solving global problems through various methods, including improving agricultural productivity, ecosystem conservation, pollution control, and mitigating climate change. Specific examples from the text include:\n\n1. **Food Shortages**: The \"green revolution\" utilized classical selection and chemical agriculture to increase crop yields, while molecular biology and genetically modified organisms (GMOs) promise further enhancements.\n\n2. **Ecosystem Disruption**: Conservation efforts and the use of living organisms for ecosystem restoration help mitigate species extinction and trophic disruption.\n\n3. **Environmental Pollution**: Microorganisms, such as those producing biogas, are highlighted for their potential to process waste and reduce pollution effectively.\n\n4. **Climate Change**: The search for organisms that can efficiently utilize greenhouse gases like CO2 and methane offers biological solutions to mitigate climate impacts.\n\nThese examples demonstrate biology's pivotal role in addressing global challenges through innovative and sustainable solutions."
            ]
        },
        {
            "question": "What are the main sources of biological hazard and provide examples of each?",
            "reference-answers": [
                "The main sources of biological hazard and examples of each are:\n\n1. **Organisms that can cause physical damage**  \n   - Examples: Tigers, sharks, bears, cows, horses, elephants, porcupines, hedgehogs.\n\n2. **Disease agents**  \n   - Examples: Viruses (HIV, rabies virus), bacteria (Koch's bacillus, cholera vibrio), eukaryotes (malarial plasmodium, bovine tapeworm).\n\n3. **Disease vectors**  \n   - Examples: Mosquitoes, ticks, dogs, cats, bats.\n\n4. **Toxin producers**  \n   - Examples: Poisonous arthropods (karakurt, hornet), jellyfish, mollusks, poisonous vertebrates (viper, stingray), poisonous plants (dope, hemlock), poisonous mushrooms (pale toadstool, stinking fly agaric), some microorganisms.\n\n5. **Organisms whose by-products can be dangerous**  \n   - Examples: Allergen producers (poplars, wormwood, dust mites, domestic animals), organisms causing water \"blooming\" (dinophytes, diatoms, green algae, cyanobacteria)."
            ]
        }
    ],
    "world-history/chapter01-Understanding-the-Past/text.txt": [
        {
            "question": "How do different levels of causation and interpretative approaches, such as social history and postcolonialism, influence our understanding of historical events?",
            "reference-answers": [
                "Different levels of causation and interpretative approaches, such as social history and postcolonialism, significantly shape our understanding of historical events by encouraging a multifaceted analysis. Levels of causation, which include immediate, secondary, and tertiary factors, help historians move beyond surface-level explanations to explore deeper contexts. For example, while the immediate cause of the United States entering World War II might be seen as Japan’s attack on Pearl Harbor, secondary and tertiary causes, such as Roosevelt’s desire to aid Britain and broader geopolitical tensions, provide a more comprehensive understanding.\n\nInterpretative approaches like social history and postcolonialism further expand this understanding by focusing on marginalized groups and their experiences. Social history, which emerged in the 1960s, shifted the focus from elites to ordinary people, using sources like court records and letters to illuminate the lives of the poor and illiterate. This approach revealed how everyday aspects of life, such as work, family, and culture, shaped historical events. Postcolonialism, on the other hand, examines the impact of colonialism on marginalized communities, highlighting how power dynamics and systemic inequalities influenced their experiences. These interpretative methods challenge traditional narratives dominated by elites and provide a more inclusive perspective on history.\n\nBy considering multiple levels of causation and employing diverse interpretative frameworks, historians can uncover the complex interplay of personal, social, and structural factors that shaped historical events. This approach not only enriches our understanding but also acknowledges the biases and limitations inherent in historical sources, encouraging a more nuanced and inclusive interpretation of the past."
            ]
        },
        {
            "question": "How do social constructs influence historical interpretation, and what examples from the text illustrate this impact?",
            "reference-answers": [
                "Social constructs, such as class distinction and gender roles, influence historical interpretation by shaping the ways people think and behave, and thus how they interact with their world. These constructs are often created and reinforced by societies and can inform historical actors' decisions and actions. For example, the text notes that social constructs influence everyday decisions, such as what to buy for a child's birthday based on gender or what to wear to a job interview. Similarly, historians like Peter Berger and Thomas Luckmann argue that our belief systems are informed by these social constructs, which in turn shape our understanding of history. \n\nIn the context of history, social constructs can determine how people perceive their roles and relationships within society. For instance, social history focuses on the everyday lives of ordinary people, examining aspects like diet, work, and marriage practices, which are all influenced by social constructs. Additionally, the text highlights how different schools of historical thought, such as Marxism, gender history, and postcolonialism, interpret historical events through the lens of these constructs. For example, a Marxist historian might focus on class struggle, while a gender historian might examine the role of gendered power dynamics in shaping historical outcomes. \n\nUltimately, social constructs provide a framework through which historians interpret the past, and they can either illuminate or obscure certain aspects of history depending on the perspective used. This is why historians must be mindful of their own biases and the social constructs that inform their interpretations."
            ]
        },
        {
            "question": "How did the approach to studying history change in the 1960s and 1970s, and what factors influence the interpretation of historical events?",
            "reference-answers": [
                "The approach to studying history in the 1960s and 1970s underwent a significant shift, moving away from the traditional focus on elites and political events towards a more inclusive perspective known as \"history from the bottom up.\" This change emphasized the study of groups that had previously been marginalized, such as the poor, women, and minorities, using unconventional sources like court records, newspapers, and personal letters to illuminate their lives. This era also saw the rise of social history, which explored the everyday aspects of life, reflecting the influence of social movements like civil rights and feminism.\n\nFactors influencing the interpretation of historical events include historians' biases, shaped by their personal experiences and societal contexts. The availability and nature of sources, such as written records, can also skew interpretations, with regions like Africa and Latin America facing challenges due to limited or biased documentation. Additionally, different analytical frameworks, such as Marxism, postcolonialism, and gender history, offer varying lenses through which events are understood, highlighting the role of causation, both immediate and long-term, in shaping historical narratives."
            ]
        }
    ],
    "world-history/chapter02-Early-Humans/text.txt": [
        {
            "question": "What regions are mentioned in the text where agriculture first developed, and what are two crops that were domesticated in each region?",
            "reference-answers": [
                "The regions mentioned in the text where agriculture first developed and the two crops domesticated in each region are:\n\n1. **Fertile Crescent (Near East)**: Wheat, Barley\n2. **Yangtze and Yellow Rivers (Asia)**: Rice, Millet\n3. **Sub-Saharan Africa**: Sorghum, Peanuts\n4. **Central Mexico (Americas)**: Maize, Beans\n5. **Andean Region (South America)**: Potatoes, Manioc\n6. **New Guinea (Oceania)**: Taro, Yams"
            ]
        },
        {
            "question": "What was the key change that the rise of agriculture brought about, as described in the text?",
            "reference-answers": [
                "The key change brought by the rise of agriculture was that humans became the deciding factor in determining which plants would grow, through the selection of plants for their edible properties, leading to significant transformations in plant varieties."
            ]
        },
        {
            "question": "Question: What were the key regions where agriculture emerged independently, and what were the significant transformations in human society as a result of these developments?",
            "reference-answers": [
                "Key regions where agriculture emerged independently include the Fertile Crescent (Near East), the Yangtze and Yellow Rivers in Asia, sub-Saharan Africa (likely around modern Sudan), central Mexico (Mesoamerica), the Andean region of South America, New Guinea, and the Danube River valley in Europe. The significant transformations in human society as a result of these developments include:\n\n1. **Plant Domestication**: Humans began selecting and cultivating plants for desirable traits, leading to the evolution of crops like wheat, barley, rice, and maize. This shift allowed humans to control plant growth and distribution, moving away from relying on naturally occurring plants.\n\n2. **Social Stratification**: Agriculture led to labor specialization, creating distinct social roles such as farmers, artisans, and traders. This resulted in wealth accumulation and the emergence of social hierarchies, with elites gaining power through control of resources.\n\n3. **Gender Roles**: Men typically took on fieldwork, while women focused on domestic tasks like milling, weaving, and childcare. This division of labor contributed to gender-based social roles and furthered social stratification.\n\n4. **Religious and Cultural Transformations**: Agriculture influenced the development of organized religions focused on fertility and deities. Priestly classes emerged, controlling resources and constructing monuments, reflecting a move from animistic practices to more structured religious systems.\n\n5. **Economic Development**: The surplus of food allowed for trade networks, urbanization, and the establishment of complex settlements. Trade items like obsidian and agricultural products facilitated economic growth and interregional exchange.\n\nThese developments fundamentally altered human life, leading to the rise of settled communities, increased population densities, and the foundation of modern civilization."
            ]
        }
    ],
    "world-history/chapter03-Early-Civilizations-and-Urban-Societies/text.txt": [
        {
            "question": "What factors contributed to the decline of the Indus Valley Civilization?",
            "reference-answers": [
                "The decline of the Indus Valley Civilization was likely caused by several factors. These include environmental changes such as shifts in the Indus River floodplain and monsoon patterns, leading to drought and reduced water resources. Additionally, environmental degradation from urbanization and high population density may have made the land less sustainable. Tectonic activity could have altered river courses, disrupting water supplies. Epidemic diseases might have also played a role in reducing the population. Furthermore, the Aryan invasion theory suggests that nomadic Indo-European groups, possibly the Aryans, conquered the region, though this is debated. The combination of these factors likely contributed to the civilization's collapse by around 1500 BCE."
            ]
        },
        {
            "question": "What were the main reasons for the decline of the Indus Valley Civilization?",
            "reference-answers": [
                "The decline of the Indus Valley Civilization was likely due to a combination of factors. These included regional climate change, which altered the floodplain of the Indus River and disrupted water supplies, and environmental degradation from urbanization and high population density. Additionally, tectonic activity might have shifted river courses, and epidemic diseases could have reduced the population. Some theories also suggest that the civilization was vulnerable to attack by nomadic groups like the Aryans, who migrated into the region."
            ]
        },
        {
            "question": "What were the suggested causes for the decline of the Indus Valley Civilization?",
            "reference-answers": [
                "The suggested causes for the decline of the Indus Valley Civilization include regional climate change, which shifted the Indus River's floodplain and altered monsoon patterns, leading to water shortages. Environmental degradation from urbanization and high population density likely contributed to soil exhaustion. Tectonic activity might have disrupted river courses, affecting water supplies. Epidemics could have decimated the population. Additionally, the Aryan invasion theory suggests that nomadic Indo-European groups conquered the region, as described in the Rigveda. It is also possible that a combination of these factors led to the civilization's decline."
            ]
        }
    ],
    "world-history/chapter04-The-Near-East/text.txt": [
        {
            "question": "What role did the Babylonian Exile play in the formation of the Hebrew Bible?",
            "reference-answers": [
                "The Babylonian Exile played a crucial role in the formation of the Hebrew Bible by providing a period during which the Judean exiles edited and combined various earlier Hebrew writings. This process of compilation and editing resulted in the core of the Hebrew Bible, shaping it into its present form during their time in Babylon."
            ]
        },
        {
            "question": "What significant cultural activity occurred among the Judeans during the Babylonian Exile that contributed to the formation of the Hebrew Bible?",
            "reference-answers": [
                "During the Babylonian Exile, the Judeans engaged in the significant cultural activity of editing and compiling various earlier Hebrew writings into a larger, unified work, which formed the core of the Hebrew Bible."
            ]
        },
        {
            "question": "Question: How did the Babylonian Exile contribute to the editing and assembly of the Hebrew Bible, and what role did this process play in the development of Jewish culture and religious identity?",
            "reference-answers": [
                "The Babylonian Exile contributed to the editing and assembly of the Hebrew Bible by providing a period during which Judean leaders compiled and combined various earlier Hebrew texts into a cohesive collection, forming the core of the Hebrew Bible. This process played a crucial role in the development of Jewish culture and religious identity by reinforcing monotheism, centralizing worship around Yahweh, and establishing a unified religious tradition that influenced their laws and practices. The editing of the Bible during the exile helped shape the foundation of Judaism, emphasizing adherence to the laws of Moses and maintaining distinct cultural and religious customs."
            ]
        }
    ],
    "world-history/chapter05-Asia-in-Ancient-Times/text.txt": [
        {
            "question": "Question: How did the arrival of the Aryans and the subsequent developments in Vedic religion, the caste system, Buddhism, and the roles of the Mauryan and Gupta Empires shape the religious and social landscape of ancient India?",
            "reference-answers": [
                "The arrival of the Aryans introduced Vedic religion, which established the caste system (varna) as a social structure, rooted in religious beliefs about karma and rebirth. This system, with its hierarchical order, sanctified the elite's authority and reinforced social divisions, making it appear as a divine cosmic order. Vedic religion emphasized dharma, karma, and the cycle of life, reinforcing the caste system's rigidity.\n\nBuddhism emerged as a response to the Brahmanist values, offering an alternative path through the teachings of the Buddha. It challenged the caste system by promoting spiritual equality and attracting individuals seeking liberation from suffering. While Buddhism never replaced Brahmanism, it influenced and was influenced by it, leading to a blending of ideas and coexistence between the two religions.\n\nThe Mauryan Empire, particularly under Ashoka, promoted Buddhism through imperial support, spreading its teachings and infrastructure across regions. This imperial sponsorship enhanced Buddhism's visibility and facilitated its expansion beyond India.\n\nThe Gupta Dynasty fostered a golden age for both Hinduism and Buddhism, patronizing arts, learning, and intellectual growth. This period saw the rise of classical Sanskrit literature and the development of bhakti, a personal form of devotion, making Hinduism more accessible. The interplay between Vedic religion, the caste system, Buddhism, and imperial support from the Mauryan and Gupta Empires shaped India's religious diversity and social hierarchy, influencing the region's cultural and spiritual evolution."
            ]
        },
        {
            "question": "How did the Mauryan Empire and Gupta Dynasty influence the cultural and religious landscape of India, including their contributions to art, literature, and the spread of Buddhism, and how did these developments interact with the existing caste system and Vedic beliefs?",
            "reference-answers": [
                "The Mauryan Empire and Gupta Dynasty significantly shaped India's cultural and religious landscape, particularly through their contributions to art, literature, and the spread of Buddhism, while interacting with the existing caste system and Vedic beliefs.\n\n1. **Mauryan Empire**: \n   - **Buddhism**: Under Emperor Ashoka, Buddhism was propagated across Asia, with missionaries sent to regions like Burma and Sri Lanka. Ashoka's pillars inscribed with moral teachings facilitated the spread of Buddhist ideas and blended with Vedic influences.\n   - **Art and Administration**: The Mauryans patronized the arts, with Ashoka's reign marking the beginning of monumental architecture and sculpture. Their efficient administrative system influenced governance.\n\n2. **Gupta Dynasty**:\n   - **Sanskrit Literature**: The Guptas supported the development of classical Sanskrit texts, including epics like the Mahabharata and Ramayana, which became foundational to Hindu culture.\n   - **Scholarship**: Scholars like Brahmagupta advanced mathematics and astronomy, contributing to India's intellectual heritage. The Gupta period also saw the flourishing of Buddhist universities like Nalanda.\n\n3. **Interaction with Caste System and Vedic Beliefs**:\n   - **Caste System**: While both empires operated within the caste framework, Buddhism offered a challenge by providing a path to enlightenment irrespective of birth. However, the elite maintained their dominance, and the caste system persisted.\n   - **Vedic Beliefs**: The Mauryans and Guptas blended Buddhist and Vedic ideas. Ashoka's pillars reflected this synthesis, while the Guptas saw the rise of bhakti, a more personal form of worship, which challenged Brahmanic intermediaries.\n\nIn summary, the Mauryan and Gupta periods enriched India's cultural and religious tapestry by promoting Buddhism, patronizing the arts and sciences, and interacting with the caste system and Vedic traditions, resulting in a diverse and dynamic cultural landscape."
            ]
        },
        {
            "question": "What were the key challenges and influences of Buddhism on the social, religious, and cultural landscape of ancient India, including its impact on the caste system and its contributions to art and literature?",
            "reference-answers": [
                "Buddhism significantly influenced the social, religious, and cultural landscape of ancient India by challenging the rigid caste system and offering an alternative path to enlightenment. It attracted individuals from lower castes and women, who found greater equality and freedom in its teachings, thereby loosening the social hierarchies of the time. Religiously, Buddhism introduced the Four Noble Truths and the Eightfold Path, which provided a distinct pathway to liberation, resonating with existing concepts of karma and rebirth while diverging from Vedic rituals. Culturally, it contributed to the development of art and architecture, as seen in the Ajanta caves, and supported centers of learning like Nalanda University. However, Buddhism faced challenges, including competition with Brahmanism and the eventual integration of its elements into Hinduism, which diluted its distinctiveness. Despite these challenges, Buddhism left a lasting impact on Indian culture, influencing art, literature, and religious thought across Asia."
            ]
        }
    ],
    "world-history/chapter06-Mediterranean-Peoples/text.txt": [
        {
            "question": "**Question:**  \nHow did the personal loyalty of Roman soldiers to their generals contribute to the decline of the Roman Republic and the rise of the Roman Empire?",
            "reference-answers": [
                "The personal loyalty of Roman soldiers to their generals contributed significantly to the decline of the Roman Republic and the rise of the Roman Empire. During the late Republic, military leaders such as Sulla and Marius were able to gain the loyalty of their troops by promising them land in return for their service. This led to soldiers prioritizing their allegiance to their generals over their loyalty to the state, creating a situation where generals could amass power by leading their loyal armies against the government. For example, Sulla marched on Rome with his client army, hunting down and murdering political enemies, and Marius similarly marched on Rome, leading to further instability. This trend continued with Julius Caesar, whose soldiers' loyalty allowed him to cross the Rubicon and seize power, ultimately leading to the end of the Republic. The rise of the Empire under Augustus was further facilitated by the loyalty of his forces, as he consolidated power and established himself as the first emperor, marking the end of the Roman Republic."
            ]
        },
        {
            "question": "How did Augustus consolidate his power and establish the Principate after the fall of the Roman Republic?",
            "reference-answers": [
                "Augustus consolidated his power and established the Principate by maintaining a republican facade, restructuring political and military systems, and implementing social reforms. After defeating his rivals, he was granted titles like Princeps Senatus and Augustus, along with consular and tribunician powers. He reduced the army, settled veterans, and Romanized provinces. In Rome, he initiated public works, provided grain and entertainment, and created institutions like the Praetorian Guard. By creating new bureaucratic roles and ensuring his succession, Augustus transformed the Republic into the Principate, establishing himself as emperor with godlike authority."
            ]
        },
        {
            "question": "How did the actions of Julius Caesar and the subsequent rise of Augustus lead to the transition from the Roman Republic to the Roman Empire?",
            "reference-answers": [
                "Julius Caesar's actions and the rise of Augustus marked the transition from the Roman Republic to the Roman Empire. Caesar's military conquests and assumption of dictatorial powers, culminating in his appointment as dictator for life, bypassed the Republic's checks and balances, concentrating power in himself. After his assassination, Augustus formed the Second Triumvirate, defeated Caesar's enemies, and consolidated power, assuming titles like \"Augustus\" and \"Princeps Senatus.\" Despite maintaining a republican facade, Augustus centralised authority, controlled the military, and established hereditary leadership, effectively creating the Roman Empire. His policies and institutions laid the groundwork for future emperors, ensuring the Republic's transformation into an autocratic regime."
            ]
        }
    ],
    "world-history/chapter07-Experiencing-the-Roman-Empire/text.txt": [
        {
            "question": "Question: What was the Edict of Milan, and what were its key provisions and significance in the context of religious tolerance in the Roman Empire?",
            "reference-answers": [
                "The Edict of Milan, issued by Emperor Constantine in 313 CE following his victory in 312 CE, which he attributed to a vision of a cross, was a policy of religious toleration. It legalized Christianity across the empire and allowed traditional Roman religious practices to continue. This edict marked the end of the persecution of Christians and established a precedent for religious tolerance within the Roman Empire, shifting the legal framework to accommodate diverse religious beliefs."
            ]
        },
        {
            "question": "What was the purpose of the Edict of Milan, and how did it impact the status of Christianity within the Roman Empire?",
            "reference-answers": [
                "The Edict of Milan, issued in 313 CE by Emperor Constantine, aimed to grant religious toleration across the Roman Empire. It ended the persecution of Christians, making Christianity a legal religion and allowing its followers to practice freely. The edict also permitted traditional Roman religious practices to continue, marking a significant shift towards religious tolerance and setting the stage for Christianity's integration into the empire's culture and politics."
            ]
        },
        {
            "question": "How did the Roman Empire manage its diverse population, including the spread of Roman culture, administrative policies, and the handling of different religious and ethnic groups?",
            "reference-answers": [
                "The Roman Empire managed its diverse population through a combination of cultural integration, administrative governance, and sometimes harsh measures against non-conforming groups. Provinces were governed by magistrates or imperial officials, allowing local autonomy while adhering to Roman laws. Roman culture spread through infrastructure projects, urban planning, and the fusion with local elites, as seen in Gaul and Britain. Christianity faced initial persecution but gained legal recognition under Constantine. Ethnic groups like Jews experienced both respect and hostility, with varying degrees of tolerance and occasional conflicts. Roman policies, including extended citizenship and cultural absorption, facilitated integration, while military control and frontier defenses managed external threats."
            ]
        }
    ],
    "world-history/chapter08-The-Americas-in-Ancient-Times/text.txt": [
        {
            "question": "What were the key strategies used by the Inca Empire to maintain control over its expansive territory?",
            "reference-answers": [
                "The Inca Empire employed several key strategies to maintain control over its expansive territory:\n\n1. **Military Organization and Tactics**: The Inca military was formidable, using overwhelming numbers, surprise tactics, and swift movement along the empire's road network. Warriors were well-equipped and disciplined, enabling them to conquer and maintain control over distant regions.\n\n2. **Centralized Administrative System**: The empire was divided into four administrative regions controlled by the emperor's relatives. Provinces were organized along ethnic lines, with governors from the Inca nobility overseeing them. This system allowed for effective governance and integration of diverse groups.\n\n3. **Economic Control Through Labor Tax**: Subjects were required to provide labor for construction, agriculture, and military service. This system ensured a steady supply of resources and labor, supporting the empire's economy and infrastructure.\n\n4. **Religious Symbolism and Rituals**: The Inca used religion to unify their empire. They promoted the worship of Inti, the sun god, whom they claimed was their ancestor. Rituals, including human sacrifices, and a complex calendar overseen by religious experts reinforced the divine authority of the rulers.\n\n5. **Infrastructure Development**: An extensive network of roads and bridges connected the empire, facilitating the movement of armies, goods, and messengers. This infrastructure was crucial for maintaining communication and control across the vast and varied landscape.\n\nThese strategies allowed the Inca Empire to manage its extensive and diverse territory effectively, despite challenges such as geographic diversity and internal conflicts."
            ]
        },
        {
            "question": "What were the key factors that led to the rise and eventual decline of the Inca Empire and the major pre-Columbian civilizations in North America, such as the Anasazi and Mississippian traditions?",
            "reference-answers": [
                "The rise of the Inca Empire was driven by military expansion under leaders like Pachacuti, an efficient administrative system with four regions controlled by nobles, a strong military with superior tactics, an extensive road and bridge network, and the use of religious symbolism centered around the sun god Inti. The Inca also managed a surplus of crops for sustenance and military needs. \n\nIn North America, the Anasazi and Mississippian traditions arose from the adoption of agriculture, leading to permanent settlements and complex societies. The Anasazi built impressive cliff dwellings and ceremonial structures, while the Mississippian tradition developed large urban centers with mounds, evidence of trade networks, and a hierarchical society.\n\nThe decline of the Inca Empire was attributed to internal instability, including a civil war over succession, and the arrival of Spanish conquistadors who exploited these weaknesses. Environmental factors, such as disease introduced by Europeans, also contributed to its collapse.\n\nFor North American civilizations, environmental challenges like prolonged droughts and climate change led to the decline of the Anasazi and Mississippian traditions. The collapse of Cahokia and other Mississippian chiefdoms was linked to these environmental issues, leading to the dispersion of populations and the eventual disappearance of large settlements."
            ]
        },
        {
            "question": "What were the key factors that made the Inca military effective in conquering and maintaining control over a vast empire?",
            "reference-answers": [
                "The Inca military was effective due to several key factors: numerical superiority, the use of advanced weaponry, their ability to move swiftly via an extensive road network, and their disciplined yet courageous approach to combat. Additionally, the empire's administrative structure, which divided territory into regions controlled by nobles, and the use of religious symbolism and human sacrifice to maintain control were crucial. The Inca's infrastructure, including roads and bridges, facilitated efficient military campaigns and administrative control. Their system of labor and surplus storage ensured resources for sustenance and construction, further supporting their military and empire's stability."
            ]
        }
    ],
    "world-history/chapter09-Africa-in-Ancient-Times/text.txt": [
        {
            "question": "How did the introduction of camels by the Romans affect trans-Saharan trade?",
            "reference-answers": [
                "The introduction of camels by the Romans revolutionized trans-Saharan trade. Camels, with their ability to travel long distances without water, carry heavy loads, and navigate desert conditions, allowed for larger and more regular trading caravans. This made trans-Saharan trade more efficient and reliable, enabling the expansion of goods such as gold, ivory, and other valuable commodities across the region. Although the nomadic pastoralists continued to manage the transport, the use of camels facilitated the growth of trade between settled societies north and south of the Sahara."
            ]
        },
        {
            "question": "How did the introduction of dromedary camels by the Romans affect trans-Saharan trade?",
            "reference-answers": [
                "The introduction of dromedary camels by the Romans revolutionized trans-Saharan trade by enabling longer and more reliable journeys across the desert. The camels' ability to travel long distances without water, carry heavy loads, and adapt to desert conditions allowed for the establishment of larger and more regular trading caravans. This facilitated the expansion of trade in goods such as gold, ivory, and salt, and led to the development of new settlements and routes across the Sahara, significantly boosting trans-Saharan commerce."
            ]
        },
        {
            "question": "What role did the introduction of the dromedary camel play in transforming trans-Saharan trade?",
            "reference-answers": [
                "The introduction of the dromedary camel significantly transformed trans-Saharan trade by enabling larger and more regular caravans. The camel's ability to carry heavy loads, travel long distances without water, and adapt to sandy terrains allowed for the expansion of trade routes. This facilitated the movement of goods across the desert, making trans-Saharan trade more reliable and scalable, thus revolutionizing the exchange of commodities between North and West Africa."
            ]
        }
    ],
    "world-history/chapter10-Empires-of-Faith/text.txt": [
        {
            "question": "How did the Kushan Empire contribute to the spread of Buddhism, and what specific actions did its rulers take to promote this religion?",
            "reference-answers": [
                "The Kushan Empire significantly contributed to the spread of Buddhism through several key actions. Under the reign of Emperor Kanishka, the Kushan rulers organized the Fourth Buddhist Council in Kashmir around 100 CE, which recognized two major Buddhist sects and compiled important doctrinal texts. Additionally, they supported the translation of Buddhist texts into languages like Sanskrit, making the religion more accessible. The Kushan monk Lokaksema exemplified this by translating Mahayana sutras into Chinese, aiding Buddhism's spread there. Furthermore, Kushan rulers promoted Buddhist art, including the first human depictions of the Buddha, and their strategic location on the Silk Roads facilitated the movement of Buddhist monks and texts, aiding its diffusion to regions like China. Thus, the Kushan Empire's support for Buddhist texts, art, and trade networks were crucial in promoting and spreading Buddhism."
            ]
        },
        {
            "question": "How did the Kushan Empire and Palmyra serve as intermediaries in the Late Antiquity world, and what were the consequences of their actions?",
            "reference-answers": [
                "The Kushan Empire and Palmyra served as intermediaries in the Late Antiquity world by acting as key nodes in extensive trade and cultural networks. The Kushan Empire, centered in northwest India, was a vital link on the Silk Roads, facilitating the movement of goods such as jewelry, furs, and silk between China and the Roman Empire. They protected trade routes and connected maritime and overland trade, reaching as far as Scandinavia and Ethiopia. Their location allowed for the exchange of cultural and religious ideas, particularly promoting Buddhism through initiatives like translating texts and hosting councils, which contributed to regional religious diversity.\n\nPalmyra, as a city in south-central Syria, was a major trading hub and a semi-autonomous entity under Roman control. It thrived as a trade center, connecting the Roman Empire with the east via the Silk Roads and the Persian Gulf. Under Queen Zenobia, Palmyra expanded its territories, challenging Roman dominance during the Crisis of the Third Century. However, its independence was short-lived, as it was reconquered by the Roman emperor Aurelian, leading to a decline in its influence. After the reconquest, Palmyra's role as a trade hub diminished, and its cultural landscape changed, with Christianity growing and the city becoming more integrated into the Roman Empire.\n\nThe consequences of their actions were significant. The Kushan Empire's intermediaries role led to the spread of Buddhism and cultural syncretism, enriching the region's religious and cultural fabric. Palmyra's brief independence and subsequent decline under Roman rule highlight the shifting dynamics of power and trade in the Late Antiquity world, illustrating both the potential and limitations of intermediary states in the face of larger imperial forces."
            ]
        },
        {
            "question": "How did the Kushan Empire contribute to the spread of Buddhism, and what role did Emperor Kanishka play in this process?",
            "reference-answers": [
                "The Kushan Empire contributed to the spread of Buddhism by promoting Buddhist texts through translation and facilitating trade routes that enabled monks to travel and propagate the religion. Emperor Kanishka played a central role by translating Buddhist texts into Sanskrit, convening the Fourth Buddhist Council to recognize Mahayan and Hinayan sects, and supporting the production of Buddhist art. His actions helped spread Buddhism, particularly to China, through monks like Lokaksema, who translated sutras into Chinese, thereby expanding its reach."
            ]
        }
    ],
    "world-history/chapter11-The-Rise-of-Islam-and-the-Caliphates/text.txt": [
        {
            "question": "**Question:**  \nHow did the Abbasid Caliphate influence the religious and cultural landscape of the Islamic world during their rule?",
            "reference-answers": [
                "The Abbasid Caliphate significantly influenced the religious and cultural landscape of the Islamic world during their rule by several key actions and policies. They established Baghdad as their capital, shifting the focus of Islamic rule to the East and integrating Persian culture into their empire. This led to a cultural exchange that enriched Islamic traditions and practices. The Abbasid Translation Movement preserved and expanded upon ancient knowledge by translating Greek and Persian texts into Arabic, fostering a scholarly environment and making education more accessible through the introduction of papermaking. This movement elevated Baghdad as a center of learning and scholarship, attracting diverse scholars and contributing to the Islamic Golden Age. Religiously, the Abbasids navigated the evolving dynamics between Sunni and Shia sects, legitimizing their rule while influencing the development of distinct theological identities. Culturally, they allowed non-Muslim communities to retain their practices but promoted cultural blending, leading to widespread adoption of Arabic language and customs. Additionally, the Abbasids supported the rise of the ulama, a class of scholars who interpreted Islamic law, shaping the evolution of the caliph's role and reinforcing the integration of religious and cultural practices within the Islamic world."
            ]
        },
        {
            "question": "**Question:**  \nHow did the Abbasid Translation Movement contribute to the preservation and expansion of knowledge during the Abbasid Caliphate?",
            "reference-answers": [
                "The Abbasid Translation Movement significantly contributed to the preservation and expansion of knowledge during the Abbasid Caliphate by translating and preserving ancient texts from Greek, Persian, and other sources into Arabic. This effort not only safeguarded valuable knowledge that was becoming scarce in other regions but also enriched Islamic scholarship. By attracting scholars and correcting/expanding existing works, the movement fostered a culture of learning in Baghdad, making it a center for intellectual growth. The translated texts covered various fields, promoting advancements in science, medicine, and philosophy. This movement also facilitated the spread of knowledge across the Islamic empire and influenced neighboring regions, contributing to the development of Islamic sciences and madrasas. Overall, it bridged cultural and linguistic divides, enhancing global knowledge exchange and shaping medieval learning."
            ]
        },
        {
            "question": "How did the Abbasid Caliphate influence the religious and cultural experiences of people under its rule?",
            "reference-answers": [
                "The Abbasid Caliphate significantly shaped the religious and cultural experiences of its people through several key mechanisms. Firstly, it established Baghdad as a cultural and intellectual hub, fostering a vibrant environment for learning and innovation. The Greco-Arabic Translation Movement preserved and expanded ancient knowledge, making education more accessible and leading to the rise of madrasas. Culturally, the Abbasids blended Persian and Arab elements, creating a cosmopolitan court that attracted scholars from various backgrounds, enriching fields like science and philosophy.\n\nReligiously, the Abbasid period solidified Sunni Islam as dominant, though Shia Islam emerged as a distinct sect. The role of the caliph shifted from both spiritual and political leader to primarily political, while the ulama gained religious authority. Non-Muslims were allowed religious autonomy but faced pressures to convert, leading to a gradual increase in Muslim population despite remaining a minority. They adapted to Islamic culture through language, dress, and naming, integrating into the broader society while maintaining their traditions. Overall, the Abbasid period was marked by a synthesis of diverse influences and the promotion of Islamic identity."
            ]
        }
    ],
    "world-history/chapter12-India,-the-Indian-Ocean-Basin,-and-East-Asia/text.txt": [
        {
            "question": "What was the significance of the introduction of Buddhism into Korea and Japan, and how did each society respond to this foreign influence?",
            "reference-answers": [
                "The introduction of Buddhism into Korea and Japan was significant as it brought a new cultural and religious framework, influencing their societies and fostering cultural exchanges with China. In Korea, Buddhism was introduced by the Jin dynasty in 372 CE and spread rapidly among the elites and later kingdoms. It transformed Korean society by introducing new ideas, leading to the adoption of Confucianism and changes in social structures, such as shifting from matrilocality to a patriarchal system. Korea maintained its own cultural elements while integrating these foreign influences. In Japan, Buddhism was introduced via Baekje in the 6th century, coinciding with the rise of the Yamato clan. Japan responded by integrating Buddhism with its native Shintoism, making it a state religion and influencing governance and culture. This blend of Buddhism, Shintoism, and Confucianism shaped Japan's unique cultural and political identity."
            ]
        },
        {
            "question": "How did Chinese cultural influences impact the development of Korea and Japan?",
            "reference-answers": [
                "Chinese cultural influences significantly impacted the development of Korea and Japan through the introduction and adaptation of various elements:\n\n1. **Religion**: Buddhism and Confucianism were introduced to both Korea and Japan, shaping their spiritual and ethical frameworks. In Korea, Confucianism became a cornerstone of governance and education, influencing societal values and practices. In Japan, Buddhism was established as a state religion, alongside the indigenous Shintoism, creating a syncretic religious landscape.\n\n2. **Governance**: Both Korea and Japan adopted Chinese administrative systems, including examination-based bureaucracy and Confucian governance principles. Korea implemented examination systems for官吏 (civil officials), while Japan's Taika Reforms and later shogunate system drew on Chinese models, adapting them to their unique contexts.\n\n3. **Education and Culture**: Korea sent scholars to China for Confucian education, and Japan adopted Chinese writing, developing kana scripts. These cultural exchanges fostered literacy and literary traditions, such as Japan's Heian-period literature, including \"The Tale of Genji.\"\n\n4. **Social Practices**: Korean marriage customs evolved under Confucian influence, shifting from matrilocal to patrilocal practices. In Japan, Confucianism influenced gender roles and social hierarchies, alongside the development of the samurai code (bushido).\n\n5. **Technology and Art**: Korean and Japanese advancements, like metallurgy and architecture, were influenced by Chinese techniques. Korea's celadon pottery and Japan's lacquerware demonstrate cultural synthesis.\n\nIn summary, China's cultural influences were absorbed and adapted by Korea and Japan, leading to unique developments that blended Chinese principles with local traditions, resulting in distinctive East Asian cultural identities."
            ]
        },
        {
            "question": "Who was Murasaki Shikibu, and what is the significance of \"The Tale of Genji\" in the context of Heian-period Japan?",
            "reference-answers": [
                "Murasaki Shikibu was a noblewoman and lady-in-waiting at the Heian Court, renowned for writing \"The Tale of Genji,\" considered the world's first novel. This work depicts the life of Hikaru Genji, a fictional emperor's son, exploring themes of romance, political intrigue, and Buddhist ideals of life's transience. It offers a detailed portrayal of Heian-period courtly life and is celebrated for its literary sophistication, making it a groundbreaking achievement, especially notable for being authored by a woman in a period where women's roles were typically secondary."
            ]
        }
    ],
    "world-history/chapter13-The-Post-Roman-West-and-the-Crusading-Movement/text.txt": [
        {
            "question": "What was the outcome of the Third Crusade, and how did it affect the status of Jerusalem?",
            "reference-answers": [
                "The Third Crusade (1189–1192) ended with a treaty negotiated by Richard I of England and Saladin, allowing Christian pilgrims to visit Jerusalem while the city remained under Muslim control. This outcome did not restore Jerusalem to Christian rule but permitted continued Christian presence through pilgrimage."
            ]
        },
        {
            "question": "What event directly prompted the Third Crusade?",
            "reference-answers": [
                "The direct prompt for the Third Crusade was the fall of Jerusalem to Saladin after his victory at the Battle of the Horns of Hattin in 1187."
            ]
        },
        {
            "question": "What were the main reasons for the decline of the Crusading movement according to the text?",
            "reference-answers": [
                "The main reasons for the decline of the Crusading movement, as described in the text, include the loss of Crusader territories, internal conflicts among Crusaders, strained relationships with the Byzantine Empire, the rise of powerful Islamic kingdoms, waning enthusiasm for the Crusades due to changes in papal and royal priorities, and the shift in Crusading goals to more political aims rather than the original objective of liberating Jerusalem. Additionally, the decline was influenced by the revival of royal power and the transformation of Crusading ideals into conflicts that served political agendas rather than the original religious mission."
            ]
        }
    ],
    "world-history/chapter14-Pax-Mongolica-The-Steppe-Empire-of-the-Mongols/text.txt": [
        {
            "question": "What event led to the end of the Ayyubid dynasty and the rise of the Mamluk dynasty in Egypt?",
            "reference-answers": [
                "The event that led to the end of the Ayyubid dynasty and the rise of the Mamluk dynasty in Egypt was the overthrow of Turan Shah by the mamluks due to their dissatisfaction and fear of being replaced, resulting in the installation of Izz al-Din Aybak as the new ruler."
            ]
        },
        {
            "question": "What was the outcome of the Fifth Crusade and how did it shape the relationship between the Christian and Muslim powers in the Holy Land?",
            "reference-answers": [
                "The Fifth Crusade ended in failure for the Christian forces, as they were unable to capture Jerusalem and were eventually defeated and forced to retreat. This outcome deepened the mistrust and tension between Christian and Muslim powers in the Holy Land. The Ayyubid leader, al-Kamil, offered a truce that allowed the Christians to hold Jerusalem and other territories, but both sides viewed this as a compromise, leading to continued conflict rather than lasting peace. Subsequent events, including the rise of the Mamluk dynasty, further shifted the balance of power in favor of Muslim forces, solidifying their control over the region."
            ]
        },
        {
            "question": "What strategies did the Delhi Sultanate employ to repel Mongol invasions, and how did their governance policies contribute to their success?",
            "reference-answers": [
                "The Delhi Sultanate employed a combination of strong governance and military strategies to repel Mongol invasions. Internally, they established authoritarian policies, including a secret police network to monitor officials and suppress dissent. They imposed strict economic controls, such as price regulations and high taxes, to limit the wealth of potential rebels and centralize state control. Additionally, they encouraged conversion to Islam to expand their societal support base. Externally, their effective military leadership and territorial expansion allowed them to repel Mongol attacks, particularly under capable rulers like Alauddin Khilji. These governance policies maintained internal stability, enabling the Delhi Sultanate to successfully resist external threats."
            ]
        }
    ],
    "world-history/chapter15-States-and-Societies-in-Sub-Saharan-Africa/text.txt": [
        {
            "question": "What was the impact of the Sanhaja Almoravids' arrival on the Songhai state, and how did this influence the kingdom's political focus?",
            "reference-answers": [
                "The arrival of the Sanhaja Almoravids in the 11th century led to the Islamization of the Songhai rulers, marking the beginning of the Gao imperial period. This influence shifted the political focus from Kukiya to Gao, which became a major Islamic kingdom due to its strategic position in caravan trade routes."
            ]
        },
        {
            "question": "**Question:**  \nHow did the Songhai Empire transition from being part of the Mali Empire to becoming an independent empire?",
            "reference-answers": [
                "The Songhai Empire transitioned from being part of the Mali Empire to becoming an independent empire due to internal weaknesses in Mali, including periodic rebellions, civil war, economic struggles, and Almoravid incursions. These factors led Mali to withdraw from Gao. The Songhai rebels, led by Sunni Ali, took advantage of this situation, leading to the establishment of an independent Songhai Empire."
            ]
        },
        {
            "question": "What were the key developments in the rise and fall of the Fatimid, Almoravid, and Almohad empires in North Africa and the Sahel, and how did they shape the region's political and religious landscape?",
            "reference-answers": [
                "The rise and fall of the Fatimid, Almoravid, and Almohad empires in North Africa and the Sahel were marked by significant developments that reshaped the region's political and religious landscape.\n\n**1. Fatimid Empire:**\n- **Rise**: The Fatimids, a Shia Muslim dynasty, established a caliphate in Cairo in the 10th century. They promoted education and built madrasas, spreading Shia Islam through scholarly influence. Their army included indigenous African soldiers, crucial for their early successes.\n- **Fall**: The Fatimids faced internal strife, crusader invasions, and competition from the Zirid family, who declared independence and aligned with the Abbasids. By the 12th century, Saladin expelled crusaders and reestablished Sunni rule, leading to the Fatimids' decline and Egypt's incorporation into the Sunni Abbasid Caliphate.\n\n**2. Almoravid Empire:**\n- **Rise**: Originating in the Sahara, the Almoravids, led by Ibn Yasin, imposed strict Malikite Sunnism through military campaigns. They expanded across North Africa and into Spain, known for their intolerance of other Islamic practices.\n- **Fall**: The Almoravids' empire was short-lived due to internal issues and the rise of the Almohads, who overthrew them. Their strict religious policies and military challenges contributed to their decline.\n\n**3. Almohad Empire:**\n- **Rise**: The Almohads, led by Ibn Tumart, rejected Almoravid legalism, promoting a more cosmopolitan approach to Islam. They overthrew the Almoravids and expanded their empire, maintaining some indigenous cultural practices.\n- **Fall**: The Almohads faced external pressures from Christian forces and internal rebellions, leading to their decline by the 13th century. Their territories fragmented, ending their dominance.\n\n**Impact on the Region:**\n- **Religious Landscape**: These empires spread Islam, with the Fatimids promoting Shia Islam through education, while the Almoravids and Almohads advanced Sunni Islam, each with distinct interpretations. The use of the Arabic language and Islamic education left a lasting legacy.\n- **Political Landscape**: The rise of these empires demonstrated the potential of indigenous leadership in Islamic states. Their falls highlighted the challenges of internal divisions and external pressures, fragmenting political control.\n- **Cultural Impact**: The Fatimids' emphasis on scholarship and the Almohads' blend of Arab and African influences shaped the region's cultural identity. The Al-Azhar Mosque, established by the Fatimids, remains a symbol of Islamic learning.\n\nOverall, the Fatimid, Almoravid, and Almohad empires influenced the spread of Islam, political dynamics, and cultural evolution in North Africa and the Sahel, leaving a profound legacy on the region's development."
            ]
        }
    ],
    "world-history/chapter16-Climate-Change-and-Plague-in-the-Fourteenth-Century/text.txt": [
        {
            "question": "**Question:** How did the Black Death contribute to the decline of feudalism in medieval Europe?",
            "reference-answers": [
                "The Black Death contributed to the decline of feudalism in medieval Europe by causing widespread depopulation and labor shortages. This led to serfs leaving feudal estates in search of better terms, undermining the traditional feudal relationship. The nobility's power waned as their wealth decreased, and centralized monarchies rose. The merchant class grew influential, and laws like England's Statute of Labourers, which aimed to control wages, couldn't restore the old system due to labor shortages. These factors, along with changes in warfare and the rise of professional armies, weakened the feudal hierarchy and allowed for new social and political structures."
            ]
        },
        {
            "question": "Identify the key factors that contributed to the decline of feudalism in medieval Europe following the Black Death.",
            "reference-answers": [
                "The key factors contributing to the decline of feudalism in medieval Europe following the Black Death include:\n\n1. **Depopulation and Labor Shortages**: The Black Death significantly reduced the population, leading to labor shortages. This made peasants and serfs more valuable, as landlords increasingly had to offer better terms to attract and retain workers, weakening the traditional feudal relationship.\n\n2. **Economic Shifts**: The merchant class gained increasing power and wealth, particularly in cities. The demand for labor and the shift away from feudal agricultural economies further eroded the feudal system.\n\n3. **Laws and Regulations**: Governments enacted laws to control wages and labor, such as England’s Statute of Labourers, but these efforts could not fully restore the feudal order. Guilds also adapted to the changing labor dynamics, impacting the traditional feudal hierarchy.\n\n4. **Rebellions and Revolts**: Peasant uprisings, such as the Jacquerie and the English Peasants’ Revolt, challenged the authority of the nobility and called into question the traditional privileges of the feudal elites.\n\n5. **Rise of Monarchical Power**: Centralized monarchies began to replace the power of local nobles. The Hundred Years’ War contributed to this shift by changing warfare and reducing the influence of nobles as military leaders, further eroding their traditional privileges.\n\n6. **Religious and Social Changes**: The Black Death led to a crisis of faith and the rise of new religious movements, such as the flagellants and anticlerical groups, which challenged the authority of the Church and contributed to broader social changes."
            ]
        },
        {
            "question": "What were the long-term effects of the Black Death on European society, including changes in social structures, economic systems, and religious beliefs?",
            "reference-answers": [
                "The Black Death had profound and far-reaching effects on European society, reshaping its social structures, economic systems, and religious beliefs in the following ways:\n\n1. **Social Structures**:  \n   - The feudal system began to decline as the plague drastically reduced the population, leading to a decrease in the number of serfs and an increase in their bargaining power. Serfs increasingly left feudal estates in search of better conditions and wages.  \n   - The Nobility's power diminished as they struggled to maintain their estates with fewer laborers. This decline in their influence allowed for the rise of centralized monarchies, as seen in the development of nation-states like England and France.  \n   - Social hierarchies were challenged as the lower classes questioned the traditional privileges of the nobility and clergy, contributing to a growing sense of social mobility and equality.\n\n2. **Economic Systems**:  \n   - The severe labor shortages caused by the plague led to a rise in wages and inflation, shifting economic power from landowners to laborers and merchants.  \n   - The merchant class grew in prominence, particularly in urban centers, as trade and commerce expanded. Towns and cities flourished due to the influx of people seeking better economic opportunities.  \n   - Guilds adapted to the new labor dynamics by relaxing membership rules and training more apprentices, but they maintained control by restricting access to higher positions.  \n   - The economic crisis led to the decline of some industries and the emergence of new ones, fostering economic transformation and urbanization.\n\n3. **Religious Beliefs**:  \n   - The Black Death shook the spiritual foundations of Europe, leading to a crisis of faith. Many questioned the Church's ability to intercede with God, especially as clergy were also affected by the plague.  \n   - Persecution of marginalized groups, such as Jews, intensified as they were falsely accused of causing the plague. This led to widespread anti-Semitic violence and massacres.  \n   - The rise of mystical and individualistic spiritual movements, like Flagellantism and the practices of groups such as the Spiritual Franciscans and Lollards, reflected a growing dissatisfaction with the Church's perceived corruption.  \n   - The decline of the clergy's influence and the shift towards anticlerical sentiments set the stage for the Protestant Reformation in the 16th century, as people sought alternative spiritual paths and reforms within Christianity.\n\nOverall, the Black Death catalyzed significant changes in European society, leading to the decline of feudalism, the rise of the merchant class and centralized monarchies, and a questioning of the Church's authority, which ultimately contributed to broader religious reforms and the development of modern nation-states."
            ]
        }
    ],
    "world-history/chapter17-The-Ottomans,-the-Mamluks,-and-the-Ming/text.txt": [
        {
            "question": "What were the primary factors contributing to the decline of the Yuan dynasty and the rise of the Ming dynasty in China?",
            "reference-answers": [
                "The primary factors contributing to the decline of the Yuan dynasty and the rise of the Ming dynasty in China included internal political corruption and mismanagement, economic instability exacerbated by inflation and food shortages, widespread natural disasters such as droughts and floods, devastating epidemics that killed millions, and significant rebellions led by groups like the Red Turbans, which were influenced by the White Lotus sect. Additionally, the Yuan dynasty's expensive military campaigns and failures, particularly the invasions of Japan, drained resources and contributed to its decline. These factors collectively weakened the Yuan government's authority and led to the rise of the Ming dynasty under Zhu Yuanzhang."
            ]
        },
        {
            "question": "What were the main reasons for the decline of the Yuan dynasty in China?",
            "reference-answers": [
                "The Yuan dynasty in China declined due to several factors. Military campaigns against Southeast Asian kingdoms and failed invasions of Japan drained the treasury. Internal revolts, including the Ispah Rebellion, were fueled by discrimination against Muslims. Natural disasters like droughts, floods, and epidemics, along with economic hardship and inflation, destabilized the country. Additionally, banditry and ineffective governance led to lawlessness, while movements like the White Lotus sect and the Red Turbans, led by Zhu Yuanzhang, overthrew the Yuan dynasty, establishing the Ming dynasty."
            ]
        },
        {
            "question": "Question: How did the devshirme system operate, and what were its primary purposes in the Ottoman Empire?",
            "reference-answers": [
                "The devshirme system operated by recruiting Christian boys from the Balkans every three to five years, typically between the ages of eight and ten, as part of a tax imposed on European subjects. These boys were taken to Istanbul, educated in the Turkish language, converted to Islam, and trained for roles such as scribes, palace administrators, or soldiers. The primary purpose of the system was to create a loyal corps of soldiers, particularly the Janissaries, who were seen as more reliable than the Turkish nobility because they owed their status and privileges solely to the sultan. This allowed the sultan to maintain control over the military and administration by reducing dependence on traditional noble vassals."
            ]
        }
    ]
}