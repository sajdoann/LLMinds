Alternate Architectures Most of neural network research has focused on the use of recurrent neural networks with attention. But this is by no means the only architecture for neural networks. Arguable, a disadvantage of using recurrent neural networks on the input side is that it requires a long sequential process that consumes each input word in one step. This also prohibits the ability to parallelize the processing of all words at once, thus limiting the use of the capabilities of GPUs. There have been a few alternate suggestions for the architecture of neural machine translation models. We will briefly present some of them in this section. It remains to be seen, if they are a curiosity or conquer the field. 7.1 Convolutional Neural Networks The first end-to-end neural machine translation model of the modern era (Kalchbrenner and Blunsom, 2013) was actually not based on recurrent neural networks, but based on convolutional neural networks . These had been shown to be very successful in image processing, thus looking for other applications was a natural next step. See Figure 7.1 for an illustration of a convolutional network that encodes an input sentence. The basic building block of these networks is a convolution. It merges the representation of Input Word Embeddings.Layer.Layer.Layer.. input words into a single representation by using a matrix.Applying the convolution to every sequence of input words reduces the length of the sentence representation by.Repeating this process leads to a sentence representation in a single vector. The illustration shows an architecture with two convolutional.layers, followed by a final.layer that merges the sequence of phrasal representations into a single sentence representation. The size of the convolutional kernels.and.depends on the length of the sentences. The example shows a 6-word sentence and a sequence of.,., and.layers. For longer sentences, bigger kernels are needed. The hierarchical process of building up a sentence representation bottom-up is well grounded in linguistic insight in the recursive nature of language. It is similar to chart parsing, except that we are not committing to a single hierarchical structure. On the other hand, we are asking an awful lot from the resulting sentence embedding to represents the meaning of an entire sentence of arbitrary length. Generating the output sentence translation reverses the bottom-up process. One problem for the decoder is to decide the length of the output sentence. One option to address this problem is to add a model that predicts output length from input length. This then leads to the selection of the size of the reverse convolution matrices. See Figure 7.2 for an illustration of a variation of this idea. The shown architecture always uses.and.convolutional layer, resulting in a sequence of phrasal representations, not a single sentence embedding. There is an explicit mapping step from phrasal representations of input words to phrasal representations of output words, called transfer layer. The decoder of the model includes a recurrent neural network on the output side. Sneaking in a recurrent neural network here does undermine a bit the argument about better parallelization. However, the claim still holds true for encoding the input, and a sequential language model is just a too powerful tool to disregard. While the just-described convolutional neural machine translation model helped to set the scene for neural network approaches for machine translation, it could not be demonstrated to achieve competitive results compared to traditional approaches. The compression of the sentence representation into a single vector is especially a problem for long sentences. However, the model was used successfully in reranking candidate translations generated by traditional statistical machine translation systems. 7.2 Convolutional Neural Networks With Attention Gehring et al. (2017) propose an architecture for neural networks that combines the ideas of convolutional neural networks and the attention mechanism. It is essentially the sequence-tosequence attention that we described as the canonical neural machine translation approach, but with the recurrent neural networks replaced by convolutional layers. We introduced convolutions in the previous section. The idea is to combine a short sequence of neighboring words into a single representation. To look at it in another way, a convolution encodes a word with its left and right context, in a limited window. Let us now describe in more detail what this means for the encoder and the decoder in the neural model..Input Word Embeddings.Encoding Layer.Encoding Layer Transfer Layer.Decoding Layer.Decoding Layer Selected Word Output Word Embedding Input Word.Embeddings Convolution Layer 1 Convolution Layer 2 Convolution Layer 3 7.2.1 Encoder See Figure 7.3 for an illustration of the convolutional layers used in the encoder. For each input word, the state at each layer is informed by the corresponding state in the previous layer and its two neighbors. Note that these convolutional layers do not shorten the sequence, because we have a convolution centered around each word, using padding (vectors with zero values) for word positions that are out of bounds. Mathematically, we start with the input word embeddings.and progress through a sequence of layer encodings.at different depth . until a maximum depth for.,...The function . is a feed-forward layer, with a residual connection from the corresponding previous layer state.Note that even with a few convolutional layers, the final representation of a word.may only be informed by partial sentence context . in contrast to the bi-directional recurrent neural networks in the canonical model. However, relevant context words in the input sentence that help with disambiguation may be outside this window. On the other hand, there are significant computational advantages to this idea. All words at one depth can be processed in parallel, even combined into one massive tensor operation that can be efficiently parallelized on a GPU. 7.2.2 Decoder The decoder in the canonical model also has at its core a recurrent neural network. Recall its state progression defined in Equation 5.3 on page 55..where.is the encoder state,.the embedding of the previous output word, and.the input context. The convolutional version of this does not have recurrent decoder states, i.e., the computation does not depend on the previous state., but is conditioned on the sequence of the . most recent previous words..Furthermore, these decoder convolutions may be stacked, just as the encoder convolutional layers. for.,.ˆ.See Figure 7.4 for an illustration of these equations. The main difference between the canonical neural machine translation model and this architecture is the conditioning of the states of the decoder. They are computed in a sequence of convolutional layers, and also always the input context. 7.2.3 Attention The attention mechanism is essentially unchanged from the canonical neural translation model. Recall that is is based on an association.between the word representations computed by the encoder.and the previous state of the decoder.(refer back to Equation 5.6 on page 57). Since we still have such encoder and decoder states and . ˆ.), we use the same here. These association scores are normalized and used to compute a weighted sum of the input word embeddings (i.e., the encoder states A refinement is that the encoder state.Input Context Output Word Predictions Decoder Convolution 2 Decoder Convolution 1 Output Word Embedding Selected Word.and the input word embedding.is combined via addition when computing the context vector. This is the usual trick of using residual connections to assist training with deep neural networks. 7.3 Self-Attention The critique of the use of recurrent neural networks is that they require a lengthy walk-through, word by word, of the entire input sentence, which is time-consuming and limits parallelization. The previous sections replaced the recurrent neural networks in our canonical model with convolutions. However, these have a limited context window to enrich representations of words. What we would like is some architectural component that allows us to use wide context and can be highly parallelized. What could that be? In fact, we already encountered it. the attention mechanism. It considers associations between every input word and any output word, and uses it to build a vector representation of the entire input sequence. The idea behind self-attention is to extend this idea to the encoder. Instead of computing the association between an input and an output word, self-attention computes the association between any input word and any other input word. One way to view it is that this mechanism refines the representation of each input word by enriching it with context words that help to disambiguate it. 7.3.1 Computing Self-Attention Vaswani et al. (2017) define self attention for a sequence of vectors.(of size.), packed into a matrix . , as self-attention.softmax...Let us look at this equation in detail. The association between every word representation.any other context word.is done via the dot product between the packed matrix . and its transpose., resulting in a vector of . values.The values in this vector are first scaled by the size of the word representation vectors., and then by the softmax, so that their values add up to 1. The resulting vector of . values is then used to weigh the context words. Another way to put Equation 7.5 without the matrix . notation but using word representation vectors.raw association.exp.exp . normalized association (softmax) self-attention.7.3.2 Self-Attention Layer The self-attention step described above is only one step in the self-attention layer used to encode the input sentence. There are four more steps that follow it. . We combine self-attention with residual connections that pass the word representation through directly self-attention.Next up is a layer normalization step (described in Section 2.6.6 on page 25). ˆ.layer-normalization self-attention standard feed-forward step with ReLU activation function is applied. relu.This is also augmented with residual connections and layer normalization. layer-normalization relu.ˆ . Taking a page from deep models, we now stack several such layers (say, on top of each other..start with input word embedding.self-attention-layer . for.,.The deep modeling is the reason behind the residual connections in the self-attention layer . such residual connections help with training since they allow a shortcut to the input which may be utilized in early stages of training, before it can take advantage of the more complex interdependencies that deep models enable. The layer normalization step is one standard training trick that also helps especially with deep models..7.3.3 Attention in the Decoder Self-attention is also used in the decoder, now between output words. The decoder also has more traditional attention. In total there are 3 sub layers..Output words are initially encoded by word embeddings.We perform exactly the same self-attention computation as described in Equation 7.5. However, the association of a word.is limited to words.with., i.e., just the previously produced output words. Let us denote the result of this sub layer for output word . as.The attention mechanism in this model follows very closely self-attention. The only difference is that, previously, we compute self attention between the hidden states . and themselves. Now, we compute attention between the decoder states.and the final encoder states.attention.softmax.Using the same more detailed exposition as above for self-attention..raw association.exp.exp . normalized association (softmax) attention.This attention computation is augmented by adding in residual connections, layer normalization, and an additional ReLU layer, just like the self-attention layer described above. It is worth noting that, the output of the attention computation is a weighted sum over input word representations.To this, we add the (self-attended) representation of the decoder state.via a residual connection. This allows skipping over the deep layers, thus speeding up training..This sub layer is identical to the encoder, i.e., relu . Each of the sub-layers is followed by the add-and-norm step of first using residual connections and then layer normalization (as noted in the description of the attention sub layer). The entire model is shown in Figure 7.5,.Input Word Embeddings Self Attention Layer 1 Self Attention Layer 1 Decoder Layer 1 Decoder Layer 2 Output Word Prediction Selected Output Word Output Word Embedding Further Readings Kalchbrenner and Blunsom (2013) build a comprehensive machine translation model by first encoding the source sentence with a convolutional neural network, and then generate the target sentence by reversing the process. A refinement of this was proposed by Gehring et al. (2017) who use multiple convolutional layers in the encoder and the decoder that do not reduce the length of the encoded sequence but incorporate wider context with each layer. Vaswani et al. (2017) replace the recurrent neural networks used in attentional sequence-to-sequence models with multiple self-attention layers, both for the encoder as well as the decoder. There are a number of additional refinements of this model. so-called multi-head attention, encoding of sentence positions of words, etc.