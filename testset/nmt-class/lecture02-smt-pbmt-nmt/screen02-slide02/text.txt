 So here is the overview of the talk. First I'll give you a brief overview of approaches because we are not covering all the approaches to machine translation. We're only focusing on statistical machine translation. So then I'll need to describe what makes machine translation statistical. So what properties should it have in order to call it statistical machine translation. and then we will discuss the idea of a probability of a sentence. Obviously if there is probability there will be Bayes law and then we'll discuss Logriner model. And then more details will be talked about the phrase based translation which was the previous statistical state of the art and then neural machine translation which is the current state of the art. And I need to highlight that neural machine translation is statistical machine translation. So if you hear someone talking about statistical machine translation that does include both these old style and the new neural approaches. But the convention is that people put a contrast between the two abbreviations SMT and NMT. So if you see the SMT in contrast to NMT then SMT means the rest of statistical machine translation except for neural machine translation. So there is like a little bit of terminological confusion but anyway. Yeah so that will be then some of brief summary of deep learning. And then on representing text and encoder decoder architecture. So this is like the baseline architecture of neural MT and that architecture has been surpassed. But we will talk about that deep improvements only later.