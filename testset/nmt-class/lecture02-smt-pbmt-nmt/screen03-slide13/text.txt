 Okay, so here is a picture which everybody who has had a lecture on machine translation has to have seen. It's called the triangle of machine translation or the triangle by Professor Wokoa, Wokoa Triangle. And the idea is that when you are going from the text in the source language to the text in the target language, you may go either directly by processing the characters or you may want to do some abstract abstraction of the source text, some source side analysis. and then from that interlingua, you would synthesize and produce the target language. And the motivation for this is if you want to address many languages at once, then if you were able to go from one language to interlingua, then you would need just like N systems for the analysis and N system for the generation, and you would be able to do all the pairs. If you do the direct approach, you need to create n-squared number of machine translation systems. So that is the old motivation from the 60s of the previous century. And actually there are very few people who believe there would be any chance to construct a useful interlingua for domain-unconstrained machine translation. So what we did in the past was to follow the analysis side, and then we did transfer between these deeper representations of sentences and we generated from there. So the idea is that the deeper the analysis the easier should be the transfer. And with interlingua there is like no transfer. But the problem is, it's also indicated in this picture, that the actual path is longer. we're trying to do the same and to the same. So we first need to develop systems for the analysis, then you need to develop systems for the transfer and then you need to develop the systems for the synthesis. So if you go along this path, then errors can easily accumulate and the overall approach may be actually worse than the direct translation. So this was discussed before NeuralMT changed the landscape. And still it's relevant, this idea of an abstract representation. It just emerges in a different setting and I'm sure it will get more attention when we are at the end of the easy improvements which can be achieved by better data preparation, cleaner data. So there will be cases where we will need some deeper analysis even with the current new approaches. We're just not at that point yet. So that's one distinction like how deep the analysis is done in a machine and another distinction is whether these systems are statistical or rule based. Statistical systems learn automatically from the data and the rule based systems are implemented by linguists. So this was before NMT the best idea was to put everything together. So we had a system which was hybrid in multiple ways and we'll talk about it in one of the lectures. It uses statistical components, and rule based opponents and it did both shallow translation and deep translation at the same time. And that was the state of the art before NMT changed actually.