 So this equation is called the noisy channel and the same noisy channel is used in speech recognition still until the neural approaches again erase that approach and the motivation for that is that instead of building one table you now have to create two tables and there is some benefit aside from edit one checking whether the output you are producing is a likely sentence in the target language. So whether it is a nice sentence like a good grammatically form perhaps or whether it is some nonsense sequence of symbols. And the main benefit is that these components can be trained on different sets of data. So if you remember at the beginning I mentioned we have monolingual and parallel data. Monolingual data are very easy to obtain or is much easier to obtain. So the language model can be trained on much larger collections of data and therefore this probability table can be much better estimated. And this better estimation can help you smooth out your final decision. So if you are unsure given your parallel data the language model will help you to choose the right translation. So that's the main motivation.