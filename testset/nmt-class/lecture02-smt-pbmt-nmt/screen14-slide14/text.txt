 That's the big overview. We need to briefly summarize language models. I guess you have them covered in other lectures. And I'll discuss only the simple n-gram based language models. So this is the part which only assesses the target sentence without considering the source. So if we are asking the right questions, this model will understand or work well.au and Chomsky would disagree. Here it definitely makes sense to talk about these two numbers, whatever the two numbers are. But the number for the should be much lower than the number for the hello. The problem obviously would come if we asked was black is more or less likely than hello. And in that case, it really doesn't make much sense to talk about the probability of a sentence because it depends on the communication situation. But in machine translation, these questions are hopefully not going to be asked at all because we have some source. So someone, some component of the system is proposing the candidate sentences. And based on these proposals, we have the source. the source was either talking about a cat or it was the greeting. So you are not going to compare these two questions. So that's why the problem, the dismissal of Norm Chomsky is not too relevant for us anyway. So, okay. So how do we define the probability of a sentence? The problem now here is that sentences have varying number of words. So how do you write a table? Yeah. So, the idea is very simple. You break the sentence into smaller chunks and you use the same table for each of these smaller chunks. So these smaller chunks in the standard approach are are n-grams, short sequences of words. So if you have a trigly language model, the idea is that you are predicting the next word given the previous two. So you are, the probability of the whole sentence is then defined as the product of the probability of the given two beginnings of the sentence, then probability of cat given the beginning and the, the probability of was given the cat and so on. So, this way, the the full probability definition is decomposed into smaller units and these smaller units are quite easy to, like, train to, the probabilities of, of these n-grams are easy to get from the