 And the nice thing is that this Logdiner modal is a generalization of the base of the noisy channel approach. So if you use equal weights and two particular feature functions defined like that, the one takes the log of the probability of the source given the target, and the other takes the log of the target, probability of the target only, and then you plug these two feature functions and the equal weights into the formula, you will get an exponent, exponentiation of the sum, then these are the logs of the probabilities, and that simply cancels out, and you are back at the product of the probability of the source given the target and the language model the probability of the target alone. So the base law approach, the noisy channel approach, is just a special case of this Logdiner modal. So this is the mathematically sound approach that allows you to do all these nasty tricks. If you want to square the probability of the language model, you just use a different lambda for that feature function that corresponds to the language model. So that's the, if you want to use the reverse direction, well, you do not use this feature function but the other one, and what worked best in the practice actually was to use both. So we used both models at the same time because some of them were smoother for some sentences, and some of them were smoother based on the data for other sentences. So we used both models. So we used both models.