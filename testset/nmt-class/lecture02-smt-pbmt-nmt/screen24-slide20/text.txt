 So that's the log-le-mode. So now a brief summary of the features which were used in phrase-based machine translation or which are used in phrase-based machine translation. The most important one is the phrase translation probability. Again, we will talk about this in future lectures. So the phrase translation probability is an estimate of the sentence translation probability by decomposing the long sentences into smaller phrases. So the idea is that you have this the candidate target. You break them into using some segmentation into phrases. That's the third parameter that you get there. And for each of these phrases, you search your phrase table, a dictionary of phrases in a sense, and you check how frequent that particular pair is. And that will give you the probability after some normalization. So that's one very useful, the key feature of the phrase-based mode. Another feature that is successfully used in phrase-based model is the word penalty or word count feature. And that simply controls whether we prefer shorter or longer outputs. So there is one slight issue with the language model as we defined it. Do you remember the language model is defined as a product of n-grams? So is it fair when comparing longer and shorter sentences? With this language model? Yeah. Do you think it's fair if you have two sentences? One of them is 10 words, one of them is two words, and you are calculating the language model score? What was the problem there? qualcosa like 10 words, in which the only one chi idには just like how many words you use, the more points you get, the higher the better, you can counterbalance that effect. So the word penalty was actually sometimes a word bonus to compensate for the fact that the language model gives all longer sentences lower scores. This one with the weights, with the Lumb Dust set correctly, depending on your held out data set, and to implement this feature, the word penalty or word count, you do something very simple. Of all the inputs that the feature function gets, you look only at the length of the source of the candidate target sentence and that's it. Another very useful feature was the number of segments. So I've silently introduced, and we may in the phrase-based lecture, I've silently introduced the third parameter of this feature function. The decomposition of the full sentences into segments. And the third parameter specifies also the reordering of the segments, but it primarily indicates how many segments are there. So you can have, again, a feature which only counts how many segments you use, and sometimes it is useful to translate with smaller units, sometimes it is more useful to translate with larger units. And the question which is better when depends on the match between your training data and your test data. If your test data is very similar to your training data, then you can afford using longer segments, and therefore, or longer phrases, and therefore you would introduce fewer errors, there are fewer points in the sentence where you are artificially putting things together, the individual parts are okay by themselves, they were produced by humans. But again, a feature function can look at the source and target candidates and see how they got reordered and judge whether this reordering is likely or not. For some languages, you know that you have to move the word somewhere and for some languages, you know that you can stay more or less in the same word order as the source language was. So that is a model which can assess the order of words precisely. And the language model is something which we have already discussed. the source and it disregards the segmentation. It only looks at the engrams of the output. Or we can use other language, other type of language, not engram one. We could use also continuous language model.