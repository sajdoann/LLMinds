 from neural networks. Okay. So, I've talked about the source sentence, the target sentence, and then suddenly the probability of the target given the source. And I've suddenly started mentioning, well, someone will propose the target sentence. OST terms. So, when you are implementing these feature functions, you are writing a simple code which gets 2 strings as the input, this is the source, this is the target, give me a score. But now who proposes these candidate translations? Like you cannot list all possible sentences in a for loop, and score all them, that would never end. will be used. So the idea is that the phrase translation probabilities are used for construction of the candidates and we will discuss that in more detail. The counts or penalties obviously cannot be used for construction because they only observe the count of the words. They don't deal with the words themselves at all. So they are for scoring only. And the language models are normally in phrase-based systems used only for scoring as well. So someone has already proposed a sentence and you are checking whether the proposed engrams are fine. But would it make sense to change the the use of phrase probabilities or phrase feature function and language model feature function? Could you use language model to propose target sentences? Does that make sense? Think about like the operation of the system. How if you have you are given an input sentence and you have all tables of summarizing the observations that you made on the training data, whatever and how would that operate? It seems natural to look at the words in the source sentence and look up in your dictionary, which is one of the tables, which words can be translated as what. So you get a set of possible target words and then you somehow shuffle with them and that will give you the candidate translations. Can we do such a proposition, such a construction of the candidate hypothesis based on the language model? And how would that operate? So... So... Basically, this is from there. Yeah, yeah, exactly. So you predict the next word and confirm it from translation. But when you are predicting the next word, if your tables are not properly sorted by the probability of something, you could also be very stupid and you could propose these words alphabetically. So you would be given a sentence, good morning, my students. And then you would start with all Czech words from A, like abet seda, abakus, and you would consider each of those. The language model would propose them. And the translation model would say, well, abakus is not a good translation of good morning. And then you would go to the next one. So this approach seems like very risky in terms of and the translation of the language model would be very risky. So... So... So...