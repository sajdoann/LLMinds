 I'll come back to this in a second. Okay, so here is a summary of the traditional pipeline for training classical statistical machine translation systems. You will find relevant parallel texts. You will align them at the level of sentences. You will align them at the level of words. Then you will extract translation units based on whatever your modul is. If it's phrases, phrases. If it's some syntactic units, some syntactic units. And the language modul is also similar. You extract the language modul units from the target side only. The moduling modul data. Then you run something which is called tuning in the old terminology. And it's the actual training. This tuning is what identifies the weights. The lambdas in the model. And then you have the tables. So you have the tables for the feature functions. And you have the weights for the feature functions. So you already it will be the best translations of these units. And best combinations of those. And then it will score with the feature functions. And that's it. So we'll talk about all parts of this pipeline in future lectures in more.