 How do we do that? Well, let's totally rewrite it. Let's redefine how do we specify or let's redefine the probability of the target sentence given the source. So the base decomposition did not do the job well because of this independence assumption. Let's try the language model or the, well, I don't know the English name for that. Let's simply try producing the target sentence Lana and ì‰¬ with that illustration only means signers like I've used one word at a time. So let's try the language model decomposition, so to say. This is simply a rewrite. We will rewrite the target sentence as the sequence of the target words and all given the source then we need to define this probability. So the easy way is to define this draw. probability as a sequence of decisions. So you first decide, given the source, what should be your first output word. Then you decide what should be your second output word given the source and the source sentence. So this is the language model style decomposition. And this is the decomposition which is used these days in neural machine translation models. they are producing the output sentence one word at a time. So in essence all neural empty systems are only clever language models. So remember when I was talking about whether the features are used for construction or scoring, I said it is kind of risky to use the language model for construction because it doesn't know anything about the source. It will be proposing all the words. Now the language model is conditioned on the source. So it will be producing and then you can easily follow. So we have changed the whole approach. We are still statistical machine translation because we are defining the probability of the target given the source. But we are totally redefining this probability. We are redefining it as the sequential generation of the target. Now the question is what technical device can allow us to train this probability to and we are learning this probability distribution. Because remember this is a sequence of words that is very complex unit. So we need something which will be able to consider this whole source sentence and based on that make sensible predictions of what is the first word, what is the second word and so on. So the.