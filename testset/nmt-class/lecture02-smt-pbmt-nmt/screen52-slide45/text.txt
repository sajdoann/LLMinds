 Okay, so that's one problem kind of reused. We don't need to have these huge vectors. We can operate after the one more step after applying the embeddings with small vectors. There is still the problem of productive morphology of some languages. So the statistical machine translation already had some issues with words like nejne opos poda rog. or donau dampschiff's fargesellschaftskapiten and so on. So these are regular words in Czech and German. And then these are the words that... So the problem is that there is... Productive morphology means that you can create longer and longer words. The set of correct words is actually infinite, depending on the language. And in practice, neural machine translation you can handle only 30, two to 80,000 words. So the idea is that you do not translate words, but you will translate some subword units. And we will come to this in future lectures. But you simply break the sentence not only at spaces, but you break it also at other points, such as syllables or morphemes. Or you can go to individual characters. And there are people who try to go to individual bytes of UTF-8 encoding or people who go to individual bits of that signal and the network still tries to recover that. And then we are moving slightly to the area where speech recognition works because there you have like a continuous sequence of observed measures. But what works best is some subword units which are trained again on your particle data. So the first one was called byte-pair encoding and there are other sentence pieces or word pieces by various researchers. and they essentially make the, we will talk about this again later, but they essentially optimize the dictionary so that words that occur often enough are listed as individual entries and longer words and less frequent words are decomposed from, decomposed into smaller units that are already listed in the dictionary. Yeah.