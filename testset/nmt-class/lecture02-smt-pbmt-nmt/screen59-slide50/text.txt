 So the whole architecture of the simplest approach is this one. You encode the input sentence with the encoder and you decode it into the target sentence. You start with random weights in this setting and you like with random weights the encoder will digest words and produce some random vector. From this random vector it will produce some very random bad output and during the training you will absolutely hurts. So 4.4 input economic growth has slowed down in recent years. I expect you to say LA in the first position and croissants in the second position and so on. And then you backpropagate, so you tell the system you are wrong here change your weights change your weights and so on. And so by doing over many pairs of sentences many times all the parameters here will get updated and suddenly it will start working. So after three weeks of training on 50 million sentence pairs, it will suddenly set the numbers right. We had 30 numbers for distinguishing the center from the circumference. There is easily 50 million parameters in this network. So it takes much longer.