 And here is an illustration from CollaxBlock what it does to the transformation of coordinates. So in the previous talk I was briefly talking about neural networks changing the representation. So the hidden layers are adapting what the algorithm can use. At the end of this sequence of transformations there will simply a linear classification. So we need to adjust the space of coordinates in a way that allows us to linearly separate the things that we want to separate. And one layer is capable of doing this these types of transformation. So when you multiply a vector of x and y coordinates with a weight matrix W and we always like stay in two dimensions then this causes of the coordinate system. And then the transposition that is not so visible here but it is here. The transposition is governed by the bias vector. So the transposition slightly moves the coordinate system. So you see that that it is moving a little less as you are like changing the values of the bias vector. And then the non-linearity is something which allows us to squash the infinite canvas of the coordinates into the finite space. And depending on the activation function on the non-linearity this squashing is different. So if we use 10h then this squashing will convert the infinite to infinite space to the space of minus 1 to plus 1 in both coordinates. So the original plane of like parallel and perpendicular lines infinite in size is squashed into this. And the exact shape of the squashing was controlled by the values in the weight matrix and the the bias vector. So this is what type of transformation is happening at one layer. Normally