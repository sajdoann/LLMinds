 para đến cxx to create like bottles in the network and then change a representation to something white again. So the idea is that you construct the computation structure you specify how many matrices should there be and what the functionality functions. And what are the sizes and you feed the input vector x. You get it processed through the set of computations, the non-linear functions and so on. And then you get the output candidate and this output candidate is checked against the expected output. So a training item consists of this input x and the expected output t and this output of your network is contrasted with the expected output. And if the expected output does not match, then. There is the backpropagation algorithm and we are not going to these details of training of deep neural networks, but the backpropagation algorithm will tell you exactly how to update the weights so that the output vector is closer to the expected output. So essentially you need to derive, you need to calculate the derivative of all the parameters with respect to some loss function and this loss function specifies how far are you, what is the error penalty or whatever that you are getting, how far is your current output from the expected output. And this derivation will tell you in which direction you should change all the weights so that you are getting closer to that. So the whole neural network setup is simply the structure of computation and then a set of weights set of parameters at each of these layers and these parameters are first initialized randomly. So the first calculation will give you some very bad output and then you modify that and you gradually in the training process you improve the inner parameters in some way so that your expected output so that your expected output matches your current output. So that is feedforward neural network is capable of consuming a vector of a fixed size and producing a vector of another fixed size through a series of hidden layers. This is the basis of neural network computation.