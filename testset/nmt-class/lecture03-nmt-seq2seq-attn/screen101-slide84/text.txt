 then the attention model is much better than the encoder decoder architecture. So this is again just reiteration of the thing you should always compare comparable setups. And yeah, that's the message.