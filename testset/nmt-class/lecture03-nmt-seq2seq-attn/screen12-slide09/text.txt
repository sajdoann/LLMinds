 If you have any questions, feel free to ask. Remember to unmute yourself and hopefully we'll be able to answer. Yeah. So that was a network that was able to process a fixed size input, a vector of a fixed size producing another vector of a fixed size. In natural language processing, we have another type of inputs. We usually have sequences of inputs. So we have sequences of words. of characters, sequences of sound samples and we also expect sequences of outputs at the end or maybe even something more complex. So first we need to change the network structure, the design so that we can handle not only as fixed size input but variable size inputs. And the way we do this is the basic approach is to use recurrent neural networks and these recurrent neural networks of the network. So you will have some state of the network and the state will be gradually updated. You start with initializing the state with some value such as zero and then you will be combining the current input with the current state given the transformation function and this transformation function will be used many times as many times as there are input values. So this is the simplest way how to digest an infinite length input or variable length input with a fixed number of parameters. The number of parameters is the size of the weight matrix and the bias for or whatever is the hidden structure of this transformation and this transformation is applied recursively. So one way to visually illustrate the behavior is this picture where there is like a loop in the network so that the output of the transformation again becomes the input of the transformation but this hides the fact that it is the input in the next time step. So what is always done is you unroll this computation in time so the network actually just becomes the way you do it is that you prepare the network to process input samples of some length up to a fixed length limit and you always you build the you unroll the computation so have you have the transformation repeated as many times as needed to digest this maximum length and then if some shorter input comes you will pet the tail of that so you will you will fill all the unused variables with just zeros or some some other specific value. So the networks that are created to digest variable length inputs are actually in practice always trained with a fixed length limit and the unused space of that length is filled with padding and it's normally filled with zeros so zeros will not affect the training so you can then do some various savings in computation so that you do not try to update the weights based on on the zeros but the the structure in the the graph of computation is actually built so that so that it's a fixed size regardless that the variable length inputs that are there are coming in. Yeah, so i can convey. c