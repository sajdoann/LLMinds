 Now the question is what the transformation A does. We know that the transformation should digest the previous state and the current input and it should produce a new state and also possibly a new output in some way. So if we go for the vanilla recurrent neural network this transformation will be the simple layer the fully connected layer which we which is defined by the weight matrix and the bias vector the input vector for this fully connected layer is the concatenation of the previous state and the current input. So this is longer than your input vectors because it needs to digest the state as well and it again you can scale it to any size that that you like with the weight matrix you apply the bias you apply the non-linearity and there you get the the new state. And then this new state is again the next step concatenated with the new XT and transform it further on. Yeah so there is one problem with this simple definition. The problem is that with this with this setup all the values of this current state go through this nonlinear transformation. So at every at every step of the of time of time of which is then like defined by the various inputs by the sequential inputs the current state of the network can radically change and if you would consider some type theory behind what is represented in this in this state in the states of the recurrent neural network then each of these steps of the state will live in different vector spaces. So So if you have the simple vanilla layout then at every time step of the network computation all the elements in the state of the network go through the nonlinearity so the state the representation of the state radically changes. So there is no like continuity in the representation and that's something that makes the training hard to hard to work and also it would make the the learned representation representations very hard to or close to impossible to to interpret. So this actually these lines is the most powerful von TehËÄÅh