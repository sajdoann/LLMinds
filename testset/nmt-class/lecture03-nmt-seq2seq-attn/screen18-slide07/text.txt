 This deeper network, the idea is that you proceed through the layers. Now it's much longer because you have the time unrolled in the depth of the network. Then you spot that you have produced something bad and this is the backpropagation. You are changing your parameters based on how much you were far away from your expected output. grit is the backpropagation and if these updates go through the nonlinearity backwards at every step, then their values will get too small. So the early parameters will not really change. So that's called the vanishing gradient problem. Your beginning of the network, which is the most important in interpreting the input values, will take ages to train. So it will there can be you can even run into like under flow issues the two small values so that due to numerical errors you won't get any updates in the early layers at all. So we do not want to have the non-linearity at every step and this is