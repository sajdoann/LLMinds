 What has allowed these deeper neural networks to train? The first such approach were long shorter memory cells LSTMs from 1997. The simplified version of this is called gated recurrent unit by Chung and Kyung-Hun Cho and others. And the idea is that the computation within each of these cells is a little bit more complicated. But there is something which we call the information highway. Some linear path through that network. So this is the path through which the gradients can then flow without going through the non-linear. Using their magnitude too much. So what is done here? Here we again have the state from the previous time step. We have the current input and we are again expected to produce the output state which then is used in two ways. First it serves for the next step obviously and also it is used to compute the output from that. So that's the HT and we have the HT minus one as the input. And instead of directly combining these two we apply something which is called gates. So these gates are again automatically trained. are always automated. There are numerous knocking down into them. Some idea that they have somenie. All do not need to handle graphics. Thatобы into these nodes. So what are the terms that areantes can then select that would be a great destination. As you addzon?" And the whole point is then roughly on DRA. You chart a lot. So what do you apply the PL. SEP is the gain that has anata LGET by testing in the inside. This machine out such a neighborhood that you have any kind of daily, you should those τηragment of your perception and how the instrument of it. What's happening? the input state. Then we have the decision which of the values from the previous time step should be used in the subsequent time step. So how many of these values do we want to preserve and this is where the information like flows linearly without changes. So the GRU has the capacity to decide we don't want to change anything about our state. I don't want to change the meaning of the sentence at all. Let's leave the hidden state unchanged. Obviously this doesn't happen in practice, but it has the capacity. So this is controlled by the reset gate. The reset gate tells you which of the values you should keep the same and which of these values you should remove. You should set to zero. and you still have one more gate and this this one more gate controls what do you want to save into the new state. So this is called the update gate. So the GRU unit is controlled by two gates. The reset gate which erases some information from the previous state or preserves it and the output gate which decides how much of the new information should be stored in the next in the next state. So this is illustrated by this simple diagram. So the input state is used in two parts. It is used both as the input to the gates which have the sigmoid nonlinearity and it is obviously also used in the actual computation with the tanH. So where is that like? Yeah, yeah, yeah, that is so that is this line. So the input the input is first the input state and the input symbol are considered by the reset gate. The reset gate considers what should we include in our computation from the previous state. That is then concatenated with the current input symbol representation and that goes to through the standard tanH nonlinearity. So this is this is like the vanilla R&N path but it is the the inputs the previous state is first possibly somehow simplified some information is deliberately dropped and then we have like the partial the candidate for the output the HT with with tilde and this HT is not directly produced the output the HT is still combined with the previous state. and this combination is a simple addition element wise. Addition of factors is always element wise. So it's element wise addition with the previous state and there is this update gate and this update gate update gate for each element decides should we in what proportion should we mix the old value of that element and the new value of that element. So the update gate mixes the old state with the new candidate state to the new state of the new state of the new state of that network. Yeah, so that's it. That's the gated recurrent unit. Those who are listening. Would you know why there is two nonlinearities used two different nonlinearity functions. We have 10H here and we have the sigmoid. So would you have any idea why why two different nonlinearity functions are used there? So can anyone unmute themselves so maybe you are not following or maybe you don't know or whatever so I'll just tell you. So the reason is that these nonlinearity functions serve for different purposes. The 10H was what we had there before. The 10H squashes the input from minus infinity to plus infinity to the interval from minus 1 to plus 1 and that is like scaling of an infinite space into a smaller interval. But the sigmoid function has a different output range. It produces values between 0 and 1 and this is exactly what we want from the gates. We want these gates to specify how much should be used from the information whether no information the value of 0 or whether is the full information the value of 1. So here with the update gate for each element in the force in output vector the update gate decides the mixing ratio. How much of the information should come from the new candidate state and how much of the information should come from the other state. So it is weighted some of these two values and obviously you want to mix them so that it is weighted some of these two values. So that's why the sigmoid function is more appropriate for the gates. You want to specify how much of something goes there. You don't want to swap the directions as the 10H allows you with the minus one output value. if you want to go on. You want to use this again direct and niin Voilà donc mois Et parsingiya và