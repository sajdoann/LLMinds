 We need to somehow reduce that because practically we can fit vectors or the rest of the computation and so on can reasonably work if we have 30 to 80 thousand elements in this one-hot representation vectors. We need to somehow reduce that large representation. So the very simple idea is to use subord units. So instead of the original words and there would be all those two million Czech words, word forms. You can go to smaller units linguistically motivated such as syllables or morphemes that would reduce that the size of the vocabulary but with productive morphology that we know from Czech or compounding languages such as German that would still not be good because then the you still need to have like a sequence of you still need to break down these linguistically motivated units into small pairs. So then you can also do linguistically less motivated or unmotivated units and that could be character pairs or individual characters or you can go to individual bytes and some people would like to go to bits as I said. So what works best is some automatically trained setup which is which varies the size of the units based on their frequency and you will get a homework on this to implement the byte pair encoding by Rico Sandrich yourselves. That is one way to break long words into smaller units but there is other approaches that will slightly differ. The general idea is that you you will start with the alphabet as the set of allowed units and your vocabulary size is still pretty much like unused and you will combine the most frequent pairs of characters to produce large units. And then in the next step you will again combine some two units in your current vocabulary to produce yet a large unit. And you do this until the vocabulary size is is fully occupied. With this approach you will arrive at a vocabulary which has all the letters so it can produce all the words but it also has individual units for parts of words that are frequent and for very frequent words the words themselves will land in the dictionary.