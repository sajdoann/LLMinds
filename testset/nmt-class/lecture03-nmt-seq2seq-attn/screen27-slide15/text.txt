 So these embeddings are continuous space representations of words. So the embedding is the mapping from the one-hot representation to this dense space. You specify the size of the embedding layer as one of the parameters or hyperparameters of your network. generalized photos is up to the network. So the embedding mapping is the first hidden layer of the neural network. The embedding elements in the embedding matrix are first initialized randomly and then through the training of for the particular given task the embeddings will get trained so that they are most useful. So theamientos of the embeddings of words are like, they don't have clear interpretation. Some of the dimensions can be used for verbs, to indicate whether this is more in the present tense or more in the past tense. Some of the dimensions or combinations of these dimensions can be used to indicate whether it is like more blue or more red. Some can be used to talk about the politeness. All these features of the input are recognized empirically by the network from the training examples and somehow compactly represented in the elements of the embeddings. Some of the embeddings are famous such as the Wartowak embeddings and there the embeddings are designed to allow an easy prediction of the neighbors of a word in the running text or the other way around to predict the. So that is the two different styles of embeddings that Wartowak can produce. If you use your text processing or sentence processing for downstream tasks such as sentence classification, whether the sentence is polite or impolite, whether someone is complaining or someone is happy about something, then again the embeddings will get trained differently and then the and the dimensions of the embeddings will reflect whether this is a four-letter word or a nasty word or whether it is some good valuation of of that and the content words can be can be like totally put to the same area of the embedding space because the actual content what you are talking about is not that important for for the sentiment analysis. So each of the different tasks will lead to different set of embeddings or the trained values of that.