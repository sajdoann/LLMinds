 So that was the input side and then another part is the output side. There again we are expected to produce one or two million units if we didn't use subord units or 30,000 subord. So a sequence of units from a vocabulary size of 30,000. So this is again something computationally quite demanding and the way we do this is that we indeed and it asks the network to give us the probability of each of these possible output words, output units. So the last state of the network is projected scaled up to the size of the vocabulary and then it is normalized to make it a probability distribution so that it will sum to one and then you choose the highest scoring element and that's the actual word that you are going to emit. So the terminology talks about logits or any energies for word at a time t. So the network is going again we will have a recurrent neural network there. So it will be producing one output at a time and at each of these time steps it will first calculate the non-scaled vector of the size of the output vocabulary. and that's the logits or energies for words and then it will normalize them with the softmax normalization. The weight matrix is very big the one that converts the hidden state to vocabulary size. It is hidden state times vocabulary size and that is a lot of parameters that need to be trained. Yeah and there were some tricks in the past how to handle this big matrix such as the matrix was not trained in full but it was trained in parts. So it was like hierarchical softmax you were first deciding whether you were to produce a noun or another part of speech and then within that smaller set of candidate words you were choosing which words to choose. This is now not frequently used because with subord units and the 30k or 80k output vocabularies we can. fit in there. So this is this is the softmax you take the exponent of every energy and you normalize it with the sum of the exponent shaded energies and that's the definition. So this is the scaling from the hidden state to the word energies and this is the softmax which normalizes all that. Yeah. So now.