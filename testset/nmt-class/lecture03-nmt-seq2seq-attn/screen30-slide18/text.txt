 Neural Network Translation Systems. As I told you already in the overview, NMT systems are only clever language models. They are producing the most likely sequence, most likely sentence and they are producing it conditioned on the aggregated input of the aggregated input sentence. So the idea, this is actually older idea than NMT obviously, the idea is to train recurrent neural network to classify what the next word in a sentence should be. So it is like a language model but not Ngram language model, it has unlimited history. You feed in from the beginning word by word all the beginning of the sentence that you have and it will every step if you ask it or at the end if you ask it, it will give you the probability distribution over all possible subsequent words and the most likely one is hopefully the one that will actually appear in the sentence. So when you are training this, you are giving it the beginnings of the sentence and you are always asking and what is the next word now? And the network uses the transformation many times. It updates then in the training this transformation many times and it adjusts the weights so that it is most likely to produce the word which was seen in the training data. Once this is trained, you can use this network to estimate the probability of the sentence. So you are perplexity like how strange the given sentence is given the knowledge encoded in the transformation. So for this estimation, you are contrasting the probabilities estimated by the model with the current word and you can also use this trained network to sample from that distribution of sentences. So you start with the star symbol, ask the network for the probability of the next word, so the first word in the sentence. It will give you the probability distribution and you will sample from this distribution according to the weights. So if you pick the most likely element, then you are going to produce the most likely sentence in the language. So whatever word you choose, you use this word and embed it again and give it as the input to the network, to the next state of the network and ask the network what is the next expected word. And the network is trained to be able to digest any variable length input and given these words that it has just seen, it will produce a distribution of the subsequent of the forthcoming word. And again, this is a distribution over all the words in the language and you can again choose the most likely element, the highest scoring element or you can choose some other one. So this is the sampling. If you take a random choice given the distribution, you will produce a random sentence. in the language. If you take the highest scoring element, you are going to get to the most likely sentence of the given language. So again, you pick the word and you need to tell the network what the word was so that it can give you the third one. So here notice that there is word embeddings used. They are used on the target language and they are used as the input of the word again. So the embeddings, we have talked about them as a mapping from the one-hot representation from words to the continuous space. That's exactly the same setup here. You also use the embeddings to convert the word to the continuous space of word embeddings. It's not the other way around. You are not like reversing the embedding mapping. The embedding mapping is one direction, you use it always in the direction to give you the the representation of a given word. So here you are like one step later, you are using the softmax to choose the word and then you embed this word and tell the word to the network. So the embeddings are used in the forward direction also at the output side. This is something which you need to remember. Okay.