 is that at each of the state you have the information about the full sentence. You have the information which is if you take the word h2 then the representation of in this encoder the representation contains this upper part which was able to remember whatever was interesting in the beginning and it also has the slower part which has the capacity to remember everything what is important in the tail. So this bidirectional representation is good because at every state you have access or potentially the network can decide to remember it can decide to have that access to the complete sentence. So bidirectional recollect neural networks are very effective for most of natural language processing tasks because the network of course given enough training data will simply spot whether it should focus on some keywords or on some like sequences of words it can it has the capacity to do various countings how often it has seen these things or whether a particular thing has some state for information whether whether I think whether the verb has followed the noun or vice versa all that the network can spot given enough training data. indulged in the use of the Teles strategically Tailor hornъe��이. Oh Millionen.