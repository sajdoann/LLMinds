 Source language input target language, conditional language model. If you want to implement it in some Python-like code, you initialize the encoder state with zeros. It is of a fixed size, which is the size of the sentence representation vector. And then you take the words one at a time, you process them with the source embeddings, and you use the encoder to digest its current state and the input embedding to give you the new state. So you digest the whole input sentence and then you start, you ignore the outputs of this RNN, these are not important. And then you start the decoder, you initialize the last word with the beginning of the sentence symbol and while as long as the decoder is producing some symbols in the language and not the end of sentence symbol, you keep asking for the next one. and then you have initialized the state is actually like just inherit the value of the state is inherited from the encoder. So you run the decoder cell to predict what will be the next state and the current output given the previous state and the last word in the input. Then you use the current the current output to create the probability over all the possible output words. So these are the logits first. So that is the output projection which scales up the decoder representation size to the full vocabulary of the target language. And then you use the maximization to find the element that you would like to print. You print it or you if this is a generator you give it as the output and also you keep looping. So this last word is then again embedded and then the decoder cell will give you the new state and the new output and so on. Yeah.