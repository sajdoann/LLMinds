 The encoder is initialized with a vector of zeros. And the encoder is an RNN, a record neural network, which digests the state and the current word. So it is, so this is the simplest possible implementation. It will be one non-milarity over the scaled, somehow scaled previous state. So there is first weight matrix for the the consumption of the embedded word. So you have the xj which is the card input symbol. You embed it with an embedding matrix. So that is the simple multiplication of a vector of a one-hot vector with a matrix. So if this vector is really one-hot, it will only select a line in this embedding matrix. And that is it. so you look up the embedding, the embedded representation of the input word and you use the weight matrix to digest that word to put that word into the continuous space. You add the bias and you apply the non-linearity and you get the state of the recurrent neural network of the encoder. And after consuming all the words in the input sequence, after tx steps, you arrive at a state which represents the whole sentence. Yeah.