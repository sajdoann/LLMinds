 And the decoder is initialized with this output of the encoder. And it is again a recurrent neural network. These states are denoted S in this case. And again as a recurrent neural network it consumes its previous state. It also consumes the last word. So this is initialized with the start of sentence symbol at the beginning. Here is the embedding matrix. so this is the one-hot representation in the target language. It selects a line in the embedding matrix of the target language. And it is scaled accordingly to the weights in the weight matrix for the decoder. The bias is applied and the non-linearity. And then this output state of the decoder is used both in the next computation step and it is also used in and this is the output word. So you so this is this is where it is used. You use again the non-linearity. So the output projection is one fully connected layer. It uses the state and the last word again embedded. Here is slightly different matrices because they serve different purposes. And they also the bias is different. And the TI is the so here is the vector. Okay. That's the word score. Well here the implementations obviously differ. Like where exactly they apply what. But the output projection is ... this is one extra layer. Not necessarily to the size of the output vocabulary. The scaling to the output vocabulary happens here by multiplying the vector with the output vocabulary matrix. So this is the matrix which is as big as the TI size times the vocabulary size. And you choose the highest scoring element there. OK.