 to compute. But the ugly consequence of this training is that we expect, we are training the model to produce the exact word at the exact position. And that goes very much against the variability of language output. There are many ways how to translate a sentence and even with a fixed set of words there are many ways to reorder them. There are easy ways to insert some and suddenly everything in the output shifts by one and with this shift your model is heavily pasted. It will get very low score. So the training criterion of neural networks for machine translation task is inadequate in the sense that they are forced to memorize the parallel corpus. So the generalization that happens to be there, the flexibility or the variability of the output that that we actually see in the end from the systems is to me kind of a miracle because they are trained to memorize the corpus. If they have a large enough number of parameters and if you train them for a long enough time they will simply memorize all the sentences and you could then start just with the beginning of the sentence from the training data and the model would already know exactly what what full output it should produce. Because it is trained to produce the exact words at their exact positions. Yeah.