 Okay, so anyway here is the summary of that. We have the encoder. It's used the same time at runtime and also in training. The decoder is used in two different ways. During training you use the calculation to produce the output words. But the output words that the network actually produced are not really selected. challenged. And you are not producing the output instead you are using the probability distribution. The Softmax gave you to estimate the loss, so you know what what was expected. You know that, why one was expected here, you check out the probability that why one received here. If it was if it was low probability and then the loss will be alright. If it was the highest possible probability in the Softmax then the loss will be low from this first word. and then you feed the network the best word, the word that was in the reference sentence and you ask it to produce the next word and again you contrast the distribution over all possible output words with the single word that was expected in the training data and that adds to the loss. So you process the whole expected sentence always asking the network to produce the next word and checking whether it has it has offered and giving it the loss according to the high scoring element there. In the run time you feed the network with the state of the encoder and you ask it to produce the distribution you select the highest scoring word and you tell the network well this was the word tell me the next one and so on and you run until it emits the end of sentence symbols.