 and you repeat until the end of sentences decoded. This is fast and memory efficient. You don't need to store any candidates. You remember only the previous state and the previous output work. And interestingly, it already gives reasonable results for neural networks because the record language model is strange to produce these sequences. So the greedy decoding is like in line with the training of that