 So that is what the attention ensures. So the idea is we are not going to force the network to represent long distance dependencies within one single vector. We will use the decoder to represent the target sentence dependencies which is the language model properties like if I started with singular with a noun in singular then also the verb has to be in singular and we will use it as a query for the source word sentence. So the decoder will at every time step it will consider which source words it should look at at the moment. So that is the attention.