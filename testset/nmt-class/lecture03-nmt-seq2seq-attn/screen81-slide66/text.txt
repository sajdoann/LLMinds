 So here is the illustration. We have the input sequence. We consume it with for example the bidirectional recurrent neural network. So we have these states. We know that each of these states represents essentially the whole sentence. But always the focus is on the current word. It has just digested the word x2 either from the left or from the right. so all the rest of the sentence is somehow encoded there. But it primarily represents that particular position. And in the decoder we're going to produce the output and the trick, the new thing is that we do not necessarily initialize the decoder with some aggregate representation of the sentence. More importantly this information is queried for. So the decoder asks the encoder which words should I like consider now when producing the first word in the sentence or the second word in the sentence and so on. And this query that's the attention mechanism. So the attention mechanism that's a separate part of the network trained for the task of choosing or weighting these states. So these A's are the energies. We will see them in the formulas in a second. These are attention energies for every position in the source sentence. There is an associated weight. This weight is automatically estimated by the network based on the previous state and the previous output word. So the decoder like tells the attention mechanism this is my current state. Which includes also like the distribution with which I should combine all these states. So another way to put this is that the attention mechanism creates a weighted sum of the states of the encoder and this weight these weights change at every decoder step. So put yet another different in yet another different way. You have a variable length input but you need some way to condense this variable length input to some fixed size representation. And the baseline approach was to take the last element in this sequence. So that is fixed size. Another option would be some max pool. you could aggregate the steps by max pooling. You could aggregate the steps by max pooling. And these max pooling approaches are good if you are searching for presence of a keyword. But if you are trying to like copy or replicate the structure of the sentence. It's best to use the attention mechanism. And to at various time steps of your output. Be able to consider different parts of the sentences as different. As differently important. the attention mechanism is the weighted sum. Over all these encoder states. And the weights change and are determined automatically as you proceed with the output. So.