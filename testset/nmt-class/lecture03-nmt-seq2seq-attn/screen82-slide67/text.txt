 Here is the formulas. The inputs to the attention mechanism is the decoder state and all the encoder states. Here the encoder states are concatenations of the left RNN and right RNN in the bidirectional recurrent neural network. The attention energies are the weights. These weights specify which of the input states is now how important. So that's one layer of neural network. the previous decoder state and all the different inputs or encoder states and then some scaling for that. These attention energies need to be normalized so that they sum to one and then when they sum to one you can see and simply use them as weights to combine all the encoder states. So this is the softmax for the normalization so that the weights sum to one and this is the context vector. So this is the fixed size representation that is now aggregating everything from the input sentence and it is aggregating according to the weights which were determined which were determined here using the attention weights. proved those are Nabalian masterpiece.