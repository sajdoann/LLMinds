 So if you plot the attention as it changes over the output words, so I think that the inputs here are the columns. So the input sentence was encoded bidirectionally by the by the encoder and when producing the very first word of the sentence the attention mechanism decided to consider the first word the most, the first element and in the sequence of encoder states then it's con so it translated this English the into the differential and then with the next word the attention mechanism it had access to the previous state of the decoder so it knew that it is the word the first word was kind of covered. So let's focus on the second word. So the weights were automatically identified to be most useful by primarily looking at the second position representation of the encoder and so on. So the attention mechanism automatically learned to follow the diagonal more or less and do the reorderings only for the expressions that deserve reordering in the given pair of languages. So here we are getting something which is kind of an alignment between the source and target but it is aligned it is rather alignment between the states of the encoder and the decoder. So the decoder is most influenced by this particular state of the encoder at this position within time of decoding. So it is alignment of the encoder and decoder.