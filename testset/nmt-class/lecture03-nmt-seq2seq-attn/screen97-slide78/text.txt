 The benefits of attention is that they are very flexible and they are learned in a very clever way. So when a colleague of mine Jan Juhers tried to use two inputs, so he wanted to benefit from the standard systems in Juhl-MT, so what he did was he pre-translated the given input sentence with some baseline system, the classical phrase-based system one, and he concatenated the original source and this candidate translation into double the input and then he used this double input to train a neural network to translate to the target language. So the neural network had access to the pre-translation from the classical phrase-based MT and it was an attentive approach and the network immediately automatically learned to follow not one diagonal but two diagonals. It was very easy for the network to identify that this pre-translation part is very useful and sometimes it actually attended more to the pre-translation than it attended to the source sentence because it like trusted the pre-translation. But as you see it is like there is no rule in the system which would say attention should be diagonal because most of languages have similar word or there is maybe like some swaps of words. Mostly but it should be diagonal. There is no such rule and still the system automatically learned this the correspondence from the data and it quickly learned it also when there was two diagonals and not just one. So this attention mechanism is