 of the different subsets of the training data on the translation quality. And for this type of evaluation, I have two test sets. So I have a test set of news-like sentences, that's the in-domain test set, which will be denoted in green with green whiskers. And then I will also have like out-of-domain test set, other texts that I want to be translated, but they do not match any of the training data very well. And that's the out-of-domain test set. and I'll be plotting always this pie chart indicating which of the training data did I use for that. So one of these subsets will be the green part, the in-domain, which is professional translation that matches the test set, the in-domain test set. Then the black part is professional translations, but not related to the domain that much. So it's the proprietary data. and then the subtitles, there's the red part, which is not very much for the domain of interest. And then the community supplied and correctly labeled contribution. That's the very small fraction, the yellow thin triangle. So the two axes on which I'm going to observe the translation quality are bleh, which is automatically estimated translation quality of the whole sentences. And another thing is the out-of-ocabulary rate. So the percentage of words of the source that are not covered in any way. So that the system cannot translate. is the highest estimated translation quality. And then the out-of-ocabulary rate is the lowest. So here. So here is. So here is. So here is. So here is.