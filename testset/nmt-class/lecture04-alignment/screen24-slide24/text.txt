 So we first need to get the text from somewhere and the web is an ideal source so people have been trying to to mine the web for many years already. So the very simple idea that was tested in one of the theses in Zarbrikan was to start with just the two language names. So you would tell the program I need a Czech and English parallel corpus and the program would do everything automatically. It would first in Wikipedia what these languages are. It would download some sample pages in that language because Wikipedia is split according to language version. It would train language identification for these two languages. It would also know the the name of the languages and then it would search for web pages in English containing the word Czech and vice versa. So search engines normally allow you to search for the languages, search for languages in a particular language. So this is the trick. And then you just need to somehow process and clean up the links that you obtained. A complete and still working pipeline. So the tool which is still in heavy use today is this bitexter. So that's something that I can highly recommend. There were also European projects on that and we had also a student software project on gathering the data. So that's that's the goal of finding parallel texts. It turned out in our experiments that actually getting the seed URLs is the problem. So if you use the search engines as a way to get these URLs, they will stop returning any results after 600 responses or so. So that's why people then moved to the common crawl or other other resources like scanning the whole because there you can find the text better. I would also like to highlight another stream of getting the sources. So either you can be searching for really parallel texts or you can be trying to exploit quasi comparable or comparable corpora. So these are texts on the same topic and but they are written independently of each other. So all the Wikipedia pages are of this nature. So you know that if you search for the Wikipedia page, the content in Prague, the content in Czech will be quite different. It will be there will be some overlap in content but no overlap in wording between the Czech version and the English version. And that's also because the target audience for these two languages is different. And also because the authorship is different. So the authors who know the domain will write in their language better. There will be more details. So these are quasi comparable or comparable corpora and we cannot expect to find complete texts there or complete paragraphs. But we could find their parallel sentences and we definitely should be able to find their translations of phrases. But the question is how to extract these phrases. So there has been a series of workshops on that. This is a workshop on building and using comparable corpora. And the method that I would like to highlight here is called lightly supervised training. It goes back to 2008. And that's the basis of unsupervised training. of machine translation. So essentially you start with some baseline machine translation system. And you translate the monolingual text in one of the languages to the other languages. And then you find similar sentences in the monolingual text on the target side. So that's one of the specific ways how to deal with that. But the basis is you rely on a machine translation system of some baseline run and then improve it with data that is synthetically translated or locate it by first synthetically translating it.