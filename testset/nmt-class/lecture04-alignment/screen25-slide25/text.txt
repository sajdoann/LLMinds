 So the document alignment is the next step. You already have obtained the two heaps of texts in the two languages and you need to find pairs among that. So this is again something that many people have tried including students of mine at the seminar and here exactly we got the observation that finding the source URLs is the tricky part. yeah. The list can be verified by nets, which shows you the right data. You can use a proper minimum pairing algorithm because you have you have you have two sets of elements and you want to find pairs and you can define criteria of a good pairing of a document. The largest exercise is obviously done by parkroll, that's the European project that gathers this data. The parkroll people try to align documents that the whole document has a translation of its whole in the other language. We relaxed this assumption and three years ago with a student of mine we were trying to extract paragraphs. So the setting is different in that if you have a page and the single page contains text and its translations, Paracrawl would not be able to find that. Our setup would break it into paragraphs and we would still be able to match these two parts of text and include them in our Parallel Corpus. So the precision and recall were pretty good when testing it on the ShuffleChang corpus and the main test was whether we can use the Common Crawl data which amounts to 150 terabytes of text in total, whether we can extract English Czech parallel sentences from that. And it turns out that the number of extracted texts was quite small in the end. After all this huge processing we ended up with only 100,000 Czech English sentence pairs. So that's much less than what we gathered automatically. So the reason for this low total data acquired was that Common Crawl only samples from the webs. So Common Crawl does not contain the full version of the full site copy of each of the web. It only enters the web and collects some random pages and then it does this in multiple languages independently. So the chance that you will have the source and target versions of the text in Common Crawl is pretty low. So the way to proceed is actually first use this huge effort to identify the URLs and then use the URLs to do full re-crawl of these web domains and that way you can obtain larger data sets. So that's something which we did in the past for specific domains like medical translation, but not for the whole.