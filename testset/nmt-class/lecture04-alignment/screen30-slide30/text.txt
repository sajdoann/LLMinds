 which is solved essentially. And now we can move to word alignment. So in word alignment you are already given a sentence in two languages and you need to align tokens, I'll keep saying words but I actually mean tokens in these two languages. And the state of the art is Giza++ which is already 20 year old software. It's quite complicated to compile and that's why people tend to use fast align these days. The fast align uses a reduced set of alignment modules and it can be pretty bad especially if the training data is small. So if you really strive for quality I would still recommend try compiling this Giza++ tool. And the nice thing about this word alignment is that it is totally unsupervised. It only requires sentence parallel text. And it will find the links between the words. So we will see the basis of the algorithm in a second. The word alignments formally are restricted to a function. So for every source token you are indicating which of the target token corresponds to it or optionally you can make the alignment to point to no word so to a null. So this obviously limits it to that it is in one to many setting and you need to do some trick and that will be also discussed in the following like how to get the normal many to many alignments. And one Giza++ implements is a cascade of models. and the word level models, the IBM 1 till IBM 4 remained in use only for the word alignment. So we'll discuss in close detail the IBM model 1, which relies on the so-called lexical probabilities, the probabilities of translations of individual words, and will only illustrate what the other models do. So I've already said it only creates many to one links, so it's actually used in both directions.