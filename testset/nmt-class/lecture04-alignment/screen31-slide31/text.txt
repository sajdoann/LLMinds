 So let's now look at the details of IBM Model 1. The lexical probabilities disregard the positions of words in sentences and they can be estimated fully automatically in the expectation maximization loop. If there are two copies of the same word then these lexical alignments are highly inadequate because they do not distinguish whether the first copy of the word corresponds to the first copy or the second copy. So this is only the basis of... So the IBM Model 1 is good for finding a dictionary of word level translations but it is not good for translation as such because it doesn't consider the position of the words in the sentence at all. So we'll now move to slides that were done by Filipken several years ago and there we will see the formulas for both the expectation and the maximization and there is a trick how to swap a sum and product in that which makes it tractable and there is also a pseudocode. So I would highly recommend that you reimplement this IBM Model 1 in your favorite language and look at the outputs. So for my students this is actually homework. So I'll send you the details how to do it, what data set you should use. But this is a very good exercise for also understanding the expectation maximization loop. And again if you want this illustrated in some live animation then the empty talk number 8 covers that. Okay so now let's... Yep now let's...