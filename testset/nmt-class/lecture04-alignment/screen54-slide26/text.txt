 Here is a summary of the quantities that we are working with. We have the lexical probabilities. So that's the probability of the individual words. This is already some iteration where we already know that the and la are quite linked together and then the and mezon have some probability and la. So we have only two words in each of the languages to consider. We have only the definite article the or la and we have only the word house or mezon. And the lexical probabilities are for all these pairs. So either the definite articles can correspond to each other like the and la 0.7 and the house and mezon can correspond to each other 0.8 is the conditional probability or the model also permits the crossed alignments that the but that's not so likely and then house corresponds to line that's also very unlikely. So this is this is the lexical translation table. All the words corresponding to all the words with some probabilities and we have a corpus and the corpus consists only of one sentence pair here in this single simple calculation and the sentence pair is la mezon the house. And what is what is here is displayed are all the possible alignments. So all the possible alignment functions that we can define for this single sentence pair. So either the words are aligned like monotonically in parallel or the word la is corresponds to both the and house and a mezon is unaligned or the other way around or the alignments are swapped. And also this is simplified in that we do not consider the null alignment. So we have four different possible alignments for this given sentence pair. And the IBM model one is this probability. So that's we have you remember the definition of that does the probability of the conditional probability of the target sentence and the alignment given the source sentence. And it is defined as the product of the word level the lexical probabilities and the normalization constant. So we ignore the normalization constant here or maybe it comes out to one I'm sure. So the in this first possible alignment the la and the are aligned together 0.7 and house and meson are aligned together. So if we multiply these two probabilities we get 0.6 as the probability of the whole and the same. So the corpus is given up front but we don't know the missing part of the of the data is the alignment. So based on our model we estimate that this interpretation of the data is the most likely. Another option would be this la producing both the and house and meson being unaligned. And there we have 0.7 times 0.05. on the. So the target is the limit that are much less probable here is another option and a third option. So all these options can be easily calculated and we know that this is the most most probably option now. So this whole up until here will be the expectation step. We have our model. We have the lexicon. and their probability. So we have now calculated the probability of each of these different alignments for the given sentence pair and we know and that's the trick we know that one of the alignments was the true one. So we can easily change that we can simply normalize these probabilities and find out what is the probability of of the alignment given the source and target sentences. So this probability quantity the IBM model one tells you what is the probability of the target sentence and alignment given the source and it doesn't sum to one. But if we then later after considering all the possible alignments normalize it so that it sums to one then we know the probability of the alignment given the source and target sentences. So the trick that happens here is simply the normalization. We normalize having observed all the possible alignments calculated their probabilities in IBM model one. We normalize them and we know that this will sum to one and like if you have as input to the source and the target sentence this alignment is the most likely and it has the probability of 0.82 and that one is the least likely and so on. So now we can move so now we know how so now we can move to the second step the the maximization step we can create we can observe the the partial counts so the partial counts are like how often the word la was mapped to the word the and how often the word la was mapped to the house. So these counts are the basis for the re-estimation of the lexical probability. and originally all the alignment points here were equally likely but now we have adjusted our assumptions our estimates of that because we already know that there was some previous knowledge there was the model and we we have considered all the alignments and we have calculated the probability of each of these alignments. So now we trust these links more than we trust these links. So when the was linked with la we can consider this with more weight and this was less weight less weight and this would not contribute to that at all. So the idea here is that you first don't know which words to align to which you allow that all of them align to all of them equally likely but you have some lexical knowledge you apply the model to the data so that is the expectation step and that refines the knowledge you now know that this alignment this interpretation of the sentence is more likely and then you use this new knowledge the refinement that we trust this more for the better estimate how often this word was connected with which. So the fractional counts in the maximization step are now the additions of the points that were like when the alignment was seen and that with the weight that corresponds to the probability of the alignment as we have normalized it. So the and la was seen to be together twice of the four cases but it's weighted so it was 0.824 for this from this picture and 0.052 from this picture. So that is this is the fractional count of the and la. and then based on these counts you can again do the the normalization based on the words and you will see that the has two translations and so the probability of the being translated into one of those is as should sum to one and you normalize it and you obtain the updated lexical probabilities. exatamente automatically. organizations and BOC and those and season Howo