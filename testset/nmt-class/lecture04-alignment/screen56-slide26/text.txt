 Let's look at the numerator, at the denominator. Let's look at how do we calculate the probability of the target sentence given the source, considering all the possible alignments. Well, that's exactly what these formulas say. It will be sum over all possible alignments and for each of the possible alignments we will rely on the IBM Model 1. So the IBM Model 1 is the definition which says that you should like the lexical probabilities of the words and normalize it in some way. So what is it, what does it mean to sum over all alignments? It means to sum over all the possible dot filled matrices of which words corresponded to which. So in the first column you are indicating which target word is connected to which source word. In the second column you do it for the second word and so on until the end of the and so on. So this is just an explanation of the sum. It sums over all the columns in the matrix and for each such alignment when you know the matrix of alignment points you are using the definition of the IBM Model 1 of the probability of the sentence which says normalize depending on the length of the sentences and multiply all the lexical probabilities and these are the and those were the matrix matrices have the ones indicating the.