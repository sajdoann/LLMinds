 So what is here? Yeah. And if you do it, then you can plot for yourself a similar gradual improvement of the of the model and the alignment of the data. So this is another way of illustrating what the EM does. The model is the lexical probabilities which words are or can be translated as which words and regardless their position. So this is We initialize the dictionary, we initialize the model with uniform probabilities. So at the beginning we do not know whether black in Czech corresponds to Bili or black in English corresponds to the Czech word Bili or Cerny or whether it corresponds to DŌM which is house or to PES which is dog. So at the beginning we do not know anything. All we have is the corpus. The corpus has in this case three sentence pairs. White house is paired with Bili DŌM, white dog is paired with Bili PES and black dog is paired with Cerny PES. So this is our corpus and these are the alignment probabilities in that corpus. So if we do not know anything about which words correspond to which, we also do not know anything about which words correspond to which in a particle sentence pair. But we take this we take these fractional counts as given and we use these new observations to estimate a better dictionary. So here we observe that Bili and White and Bili and House were like equally likely. We know that Bili summed to one. So Bili was surely linked to something. It was either linked to White or to House but we don't know which one. So these are the uncertain are certain alignments. Fractional counts of 0.5 and 0.5 and here we will collect with which words Bili was linked and here we normalize the other way around. So we know that White was linked with Bili and that happened in two sentences. So one half and one half and White was also linked to that's another half. So in total White was seen with four fractional counts of half and of these four fractional counts two belong to the word Bili and one each of the two remaining ones belongs to Doom and PES respectively. So if you look at the fractional counts where White was linked to some words then you see it was more often linked to Bili than to the other words. And another interesting observation from this corpus is that White was never linked to Cerni. So there was not a single sentence where White would co-occur with Cerni and Cerni in Czech means black. So after the very first observation of the corpus where we didn't know anything about the non-occurrence of white and Cerni already ruled out the chance of White having the meaning of black. So this is the first observation and we already know like a flat zero the probability of zero that White translates to Cerni. So that helps already a lot. For the other words we are not yet quite certain. We know that White is most likely Bili and it could be also so with some probability of doom or pass. And only later as we proceed with the calculation these uncertainties will tear apart and also the probability of White and Bili will grow. So now we have an improved model and we use again the expectation step to draw the new alignments here. And now for the first sentence we already know that White and Bili are more likely to belong to each other than White and House. So that's why this dot is lighter. And the House and Bili that is still not decided. And only after one more iteration it will turn out that House is more likely linked to doom than it is linked to White. So hopefully with this illustration you see how the whether it makes sense or not. So we are gradually improving the model and gradually improving the data.