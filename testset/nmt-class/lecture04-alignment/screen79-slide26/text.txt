 in functional words. And then I compared this observation with the performance of automatic alignment as run by Giza of two models. One was the baseline model and then some improved model which simplified the token. So somehow like improved the setting so that Giza had a better chance to align the words. And the observation here to make is that for tokens where humans have had no troubles in aligning that. So this is these two lines. There the improved model indeed increased the number of tokens that were well aligned compared to the tokens where the Giza prediction did not match humans. So for the parts of the tokens where humans had a clear idea how to align words, Giza was able to discover this and we saw the improvement. When we improved the processing of the data we got better alignments from Giza. If we looked at the tokens where humans did not have the clear idea of which words correspond to which, then there the improved processing had no effect. It did not improve. It did not guess better what the humans want because the humans were not able to tell what they want. So the main message of this is that if your task is ill-defined then there is no point in improving, in trying to improve your algorithm. You first need to have a well-defined task where humans agree and when humans agree it makes sense to work on improving the algorithm. So always whenever you are doing any type of annotation always estimate the inter-annotator agreement and focus on the rules for annotations so that the task is well defined. And in a well-defined task you can use the machine and it makes sense to work on automatic algorithms. But without a clear definition you don't get any measurable improvement. Cinex.