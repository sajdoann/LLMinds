 Make things really correct. So now the features. So what features, what feature functions are used in phrase-based MT? So obviously the most important one is the phrase translation probability that scores phrases independently of each other. So when the feature function is launched over the whole sentence given a fixed segmentation already, the feature actually just multiplies the probabilities of. So the phrase translation probability feature is decomposed along the segmentation and this probability of the now target because of the base law motivation behind the probability of the of the source phrase given the target phrase is estimated from the from parallel corpus and we will see that as a reminder in a second. So this phrase translation probability tells us how good the phrases each of them alone regardless the context seem to be good translations of the source phrases. Then we have already briefly mentioned that we have various counts. So we have word count which only considers the number of the words in the target sentence and that is to give the model some control centuries. We have også of what the desired expressions 아이고úblic 언 BB 언 filety probably coばと思わ less of the object above the total length of the output wether it is In general tempted to shorten the output or make it longer. We also have the phrase count or phrase penalty which considers only in segmentation and within the segmentation it considers only the number of fairly of formal phrases or number of you werto into which the sentence is broken. And again, this phrase count or phrase penalty controls whether the system will prefer to use longer phrases or or shorter phrases. And this is an important parameter which allows the model to react to the match of the training data and the test data. If the training data matches very well the test data, then in the test data we can expect to use very long phrases and with fewer phrases there is lower risk to make errors in translation. Unfortunately, if we are in a bad setting where we do not have an and the model has the power to do that, we would need to use shorter phrases because if the test set is different then we cannot expect to have long sequences in the in the phrase table in the dictionary of phrases and the system through this parameter through adjustment of this parameter silently resorts to using shorter phrases. There is a higher risk of introducing errors the phrase boundary but the robustness or the recall the coverage of that system improves. Then one feature which we are not going to discuss in any detail is the reordering model. There are various strategies how to estimate whether the particular reordering of phrases, particular modifications of the order of the segments in the segmentation is good or bad for a given language pair. We can skip that for a moment. and there is obviously the feature which we have discussed in the past and that is the language model probability. And the language model probability considers only the target sentence not the source and if it is an Ngram language model it simply checks the probabilities of each word given N-1 predecessors of the word.