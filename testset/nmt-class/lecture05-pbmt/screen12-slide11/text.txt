 So the observations of the phrase counts can be pretty low numbers because this can be very long phrases and long phrases appear infrequently and if you are doing the maximum likelihood estimate with low numbers then there is a high risk that some of these numbers will be very imprecise so we will be dividing like two by one and you have a probability of 0.5 and that's like highly exaggerated number. so in addition to these phrase probabilities which can be too rough, too sparse the lexically weighted ones consider the individual words and there the number of observations of word pairs is much higher so the lexically weighted scores are reliable even for long and infrequent phrases. and then so this is the four numbers because this lexical weighting is again used in two in two directions and then we have the fifth score which has been removed only in very late versions of the standard decoder for that the Moses decoder and the phrase penalty that was always set to e to the power of one so 2.71 and that is just the phrase phrase count. So whenever you are using any of these phrases you are adding these scores to their appropriate to the appropriate elements of the final score vector and the last element here that is the counter that is something which is counting how many phrases have I used across the hypothesis the candidate translation.