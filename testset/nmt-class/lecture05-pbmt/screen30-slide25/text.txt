 So how does that work? The problem is that if we were putting all the partial hypothesis into one stack, then the longer ones would be always scored lower. And that's because longer hypotheses contain more elements. For example, the language model score that always grows like the number of n-grams in considered in the language model score always grows. Each of these elements is a probability, a number between 0 and 1. So the total score of the language model always decreases as we extend the hypothesis. So if all the hypotheses were competing in one queue, then essentially the search would be breadth first. It would first explore all the short hypotheses and only when it has run out of other short hypotheses, it would move to the 1s and 3 word 1s. And this breadth first is something which would kill us. We would spend endless time in the search because we know that the search space is exponentially large. So the way that this is reduced in phrase-based MT is called stack-based beam search. And here you organize partial hypothesis in a sequence of stacks. And these stacks are designed so that it is guaranteed that we will proceed along the sentence linearly and not exponentially. So we will not explore at every bit, at every step, we will not explore too many options. We will explore only a limited number of options. And still within the stack, the hypothesis that compete to each other compete in kind of a fair comparison. So the best setup was to label the stacks by the number of source words that were already so in the first stack. So in the first stack, there is the only the empty hypothesis, which is the only one which covers no input word. Then we have the first stack and the first stack contains hypotheses that have covered exactly one word. And then further stacks are for hypotheses that covered two words or three words and so on. And when you are doing the incremental step, the expansion of the partial hypothesis, you are taking a hypothesis from the current stack. And you are adding some new words to that. So you are kind of moving for sure you're moving to the higher stacks. And depending on how many words you cover with this one translation option, you either put the continuation, the subsequent hypothesis to the immediately following stack or you put it two stacks apart if you have added two words and so on. So every stack contains hypotheses that cover the same number of source words and during expansion every hypothesis moves to the higher stacks. And the limitation now comes from the stack limit. That's the constant that you give to the system. And the constant says never store more than 20, 200 or 1000 candidate translations or partial hypotheses within a stack. So at the beginning we know that the first stack contains only the empty hypothesis. We expanded some of these expansions maybe have covered the whole sentence. So they move directly to stack number 10. Some covered only one word. So we have all one word translations. If there are more translations, more possible expansions of the first hypothesis than the stack limit, then all the lower scoring ones are discarded. then when we move to the first stack and we start expanding the hypothesis from there, we know that we will do this at most stack limit number of times. And that is the linear bound for the translation. So you have dropped all the hypotheses which are unlikely given the scores so far.