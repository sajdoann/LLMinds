 Now how does that work? So the baseline was 18 point something BLEU score. We have seen the improvement with the part of speech model in the previous setup. And then we've moved to the linguistically adequate morphological generation model. And the BLEU score dropped by 5. So how is that possible? The problem is that we are introducing independence assumptions where they don't have to be introduced. Sometimes you needangests because you don't see kneecaps in all the cases. But there are many words, which you do see in the correct cases and you can directly copy the the correct word form and you don't have to do the decomposition into lemmas and tags. So these additional independence assumptions bring an information loss and therefore the translation quality drops.