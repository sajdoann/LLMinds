 Translation quality. So here is a summary of what the results were in 2009. And before looking at the outputs, I'll actually summarize how it went across the years. So in 2007 the translate and check was the best setup because we had relatively small data but already big enough so that the linguistically adequate path would not be the best choice. από from China and then a year after that, the translate and check setup was still worth the effort. But in 2009, our computers were already like fast enough and we had large enough disk space and large enough data so that we could handle seven grams of words forms and not seven grams of part of speech text only as in the previous years. So, in 2009, The best result was actually achieved with the vanilla phrase-based system and the translate and check system was already a little worse and that was because the added complexity of the search space was not then compensated with sufficient gains from the language model. The language model on word forms was good enough to make the decisions. In the previous years the language model on the word forms was unreliably estimated due to smaller data and slower machines and like smaller language models. So in previous years it was worth the effort with larger data the baseline was better. And the setup of translation, translating separated dilemma and the morphological tag and then generating the form that was too big. It simply exploded. With the larger data there were too many possible words and too many possible like all the 2000 observed morphological tags and that Cartesian product of words and the text then exploded. So it was not possible to train the setup with the large data that we had, the 2.2 million sentence pairs. I used only much smaller data set and there on the smaller data set at the runtime the search was too complicated. So it actually got lost in the search space and the choices, this of the linguistically adequate decomposition made were worse than what the phrase based system did on average. Yeah.