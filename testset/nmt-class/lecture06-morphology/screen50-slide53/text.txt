 So that was the unfortunate linguistically adequate setup which didn't work with synchronous factors and what worked very well in many years was this translate and check setup. But I mentioned that in 2009 there was like the gain from the translate and check was limited and that was limited with if I had only one language model. So the setup that I had in the last years of phrase based MT and that did help with this translate and check was to use several language models of different type and that is something that I would like to share with you because that's useful. So we have these two output factors. We have this output factor of word forms and we have this output factor of morphological tags and we have parallel data and monolingual data. All the language models can be estimated on the monolingual data and depending on what factor we look at we can effort various lengths of the n-gram language models. So in my best system in the last years of phrase based MT, I actually used four language models at the same time and each of them was reliable for other parts of the sentence. So one language model was called long that was based on the word forms and that was the seven grams of full word forms and it was trained on like six, seven hundred million tokens. And I could not afford to use more data in this language model because then there would be too many seven grams and I couldn't fit the the language models on the disk. But I had more data. I had actually almost four giga words of Czech texts. So with those I trained a model which I call big and that was only four grams. So this language model had like smaller span. It didn't didn't look so much around but it had these four grams estimated very well because it was trained on a large amount of data.. So this is the two. So this is what I used to do. The two. So I used to do the math in the last year. So I had morphological tags which were like even longer. I could afford 10 grams of morphological tags and even 15 grams of morphological tags. The 15 grams are sparser, sparser so I edit them only like later and here you see what is the BLEU score if you use the various subsets of these language models so the the basic setup is to use translate and check so the basic setup actually is just to translate and use only the more for the the model based on word forms the seven gram of word forms then using four grams is actually better if the four grams are estimated on then the long and morph together give you a little bit of improvement and the big long and morph is the best setup and then you can still improve a little if you use the 15 grams of morphological text so each of these language models has a different view on the output sentence and each of them is estimated with a different confidence or reliability so using all of them and and tuning their out set is the best thing that you can do