 unmodivation. But then the pruning happens without linear context. So there is very risk, very high risk of search errors. So we'll now cover two techniques which try to handle the sparseness but still avoid the explosion. One of them is called two-step translation and the other is called reverse self-training. So the two-step translation