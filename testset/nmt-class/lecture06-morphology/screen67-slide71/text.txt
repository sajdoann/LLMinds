 So we have already hinted this when we were introducing the neural machine translation. There is the word embedding stables and you cannot make these stables too big. We know that the classical, the phrase-based MT already struggled with productive morphology when we have around one million or more word forms such as these compound words or where large number of morphological variations of words. The neural MT with its embedding tables can fit at most 30 to 80 thousand different word forms. So for this reason immediately words had to be broken into some things and we can go for syllables and or morphemes as two examples of linguistically motivated separations or segmentations of words. And then we and then something that we will illustrate on the next slide is that's called bipair encoding. That is a combination that is based on compression algorithm. It simply creates the dictionary of known word forms so that the most frequent words are there and less frequent words.