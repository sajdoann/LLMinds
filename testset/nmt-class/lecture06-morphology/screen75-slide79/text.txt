 So you end up with this sequence of merges and then when you apply this to the new input, you simply apply it in the order in which it was created. So when you are then compressing or processing with this byte pair encoding the word newest, you first merge this V and then you merge this ST and then you are at the end of your merge operations list. So you end up with this sequence of of translation units. The first one is just the letter N. The second one is just the letter E. Then we have this unit WE and then we have the ST at the end. So the vocabulary size will be as big as your alphabet and the number of merge operations. And this is something which you control yourself. So you can stop whenever you like. And whatever and whatever is not merged any further that will simply remain represented as the subword with the subword units. So for our machine translation system, the the input is now not a single token newest, but it is a sequence of four tokens. But it is designed so that frequent words will have their own unit and less frequent words will be broken even maybe like if their stems, if their basic meaning components are frequent enough, then there could be like a stem part and the ending part. And there the neural empty will get the capacity to to learn that stems should be translations of something. And then the endings come from our other indications in the sentence. So this this separation of subword units is actually sufficient. So all the complex things that I've discussed for phrase based empty throughout the lecture have been totally superseded by the neural empty and the subword units. Neural empty simply learns by itself what is the correct combination of the subword units and it gracefully handles all the morphological richness of objects. So when people who were never did any phrase based empty came to to machine translation and came from the neural, neural empty area, they said that neural empty is complex is conceptually simpler than phrase based empty. And I have to agree with that at this point. Indeed, you don't have to care about morphological richness. The model will learn it for you just by observing sequences of subword units.