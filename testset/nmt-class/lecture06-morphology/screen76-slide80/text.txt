 So there are a number of flavors of these sub-bord units. So we have discussed this byte-per-encoding by Ricos Henrich. There is also Google Word pieces which were introduced for speech processing around the same year. And then in Tensor2Tensor, which was a popular implementation of the transformer model, there was another implementation which was called sub-bord text encoder. And the current best or currently most-used implementation is called sentence piece. And again, each of them has slightly different processing. So we tried once to compare them as well empirically. So we've applied STE, the sub-bord text encoder and BPE. And we have observed an interesting thing. STE has always added an underscore at the end of the word. And that had the positive effect that for Czech, where you have and the ending can be separated from the word even if it's like an empty ending. So we have two sentences. Bliží se kto be tramvaj and stramvaj nevy stoupili. And these two sentences, when processed with STE, which appends these underscores, will share the unit tramvaj. And this sharing of this unit, the fact that you observe the word tramvaj, the token tramvaj, in both of these in these sentences, gives the model the necessary generalization power. So it will know that tramvaj now is something which approaches people and tramvaj is something that you can get off or get onto. And this knowledge allows the model, the identity of the token allows the model to realize all the similarities, the behavior of the word. If you use the BPE in its default variation, bliží se kto be tramvaj, stramvaj nevy stoupil, then unfortunately there is no way to separate an empty suffix. BPE indicates unfinished words with these double ampersent, the double ad signs. And the nominative was observed frequently enough. So that becomes a single word in the dictionary. But this other case was not is not observed enough and it happens to be broken into two components. And here the model does not see the same unit. So it has no way of finding out that tramvaj is in any relation with tramvaj. So here the identity of the word is broken. If we add the underscores before processing the corpus with the BPE, and we can be lucky now. This tramvaj will get the ampersand as an unfinished word. And it will be the same token, regardless whether the ending was empty or whether the ending included a single word. So this is something that we've figured out only empirically, because it's like tedious to read the source code. It's easier to look at the data and... ...and um...