 Try to figure out why one of the methods is better than the other. And in our German-to-Czech experiments, we tried this underscore trick and we found one more interesting thing. It helped. So applying, adding this underscore to every word did indeed increase the BLEU score with BPE a little. But if we added the underscore after every word form, every token except for the final full stop, then it helped much better. So the best technique that we had for this particular language pair was to, well, the best one was actually the sub-vortex encoder from tensor to tensor. But BPE could get almost to that performance if you added the underscore at the end of all tokens except for the last one. And what we haven't, like, searched for the underscore was observed so that the BPE learned different words or whether this indication of the last word token was actually helpful for the subsequent translation model. So this is something that that could be still investigated. But the increase in the in the BLEU score was was amazing. So one message to the first thing that I would take from this is that the very tiny details such as adding an underscore at the end of every word can suddenly very dramatically change the performance of your method. And it's always worthwhile to do these simple variations if you get the idea that you should do them in the first place.