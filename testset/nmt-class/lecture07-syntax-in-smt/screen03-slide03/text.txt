 Ghm. But let's first motivate for some use of syntax. So we know that before Neural MT the state of the art was phrase based machine translation and the phrases have had a serious problem. They didn't check for any grammatical coherence. The only grammatical device of the standard phrase based approach was the language model and that was often trigrams only if you had small data. with the factored language models and the part of speech language models, which included up to 15 grams of text. But still, sentences can easily go beyond 15 words. And also, you do not see all these sentence patterns to make sure that the model will produce grammatically correct sentences with proper syntactic structure. So, it was very common that the output of a phrase-based system was a perfectly with the end-user. But as a whole, the sentence did not have any word at all and it was word-salad impossible to understand. So, that is for the end-user. And the practical problem from the data point of view was that the phrases did not contain or did not allow for any gaps. And there are language constructions which do need gaps. So, you need like circumfixation. One of these examples is French. if you are negating a French verb for the translation of I don't know, you need to say je ne sais pas. This ne and pas, these two parts, need to be put around the verb. And you would need, ideally, one lexical entry with a gap for the verb. But if your system does not allow any gaps in phrases, then you have to memorize the negation for all the verbs. So, in other words, you have to see all verbs in their positive and negative variant so that you would be able to learn them, how to learn how to translate them. And also, the reordering modules were very weak. So, there was no explicit linguistic information available to decide whether we are in a subordinate clause in German and therefore the verb has to go to the end and things like that. So, there was no notion of grammar. All was A Yes! WYismus Hmm, lebih