 So the problem is that in your normal phrase-based extraction you already have a large number of phrases. You have the short ones, long ones, up to 10 words. And now you are also adding many more by subtracting varying shorter ones from the bigger ones. So the number of extractable lures is too high. And you have to limit it somehow. So these constants that were used to limit it were very arbitrary. So the initial phrases were extracted as normal up to 10 words in a row, but the rules that were created were always limited to have at most six symbols. Some could be non-terminals. Some could be non-terminals. At least one of the terminals had to be aligned, so that when applying that rule the system would know when to fire it. There had to be some evidence in the input. Also it was not allowed to extract more than two phrases from a longer That somewhat limited the number of extracted rules. So the files were manageable in size and would fit on the disk, but the search space was similarly cluttered, because the same output could be constructed without any gap annihilor images, or with the full hierarchical structure. So there was even a much bigger need for the hypothesis recombination that we have discussed in the phrase-based lecture. And we'll talk about this later. The language model is a non-local feature, so it was more difficult to integrate it into this new type of search. But we'll talk about it only in a second. So one reservation that people always had is that this is not a proper syntactic model, so why don't we move to proper syntax where the non-terminals have some linguistic meaning and where there is some restriction on what can be plugged