 this part where we're translating this input. Well, I obviously cannot read it. This thing which corresponds to China, this corresponds to the yin, and this corresponds to Beijing. And we had already the example of the rule. So the rule that will be used here is the rule which says that these two phrases, China, something, Beijing, in this order, with this symbol in between, that can be digested and that will create a valid component in the sentence. And the corresponding translation in English is first the city name, so the element which was previously the second one, then in, and then the country. So this is the step in which a contiguous in the source sentence is digested and a contiguous span in the target language is constructed. And we know that we want to score all contiguous sequences, we want to score finally the whole target sentence with an N-gram language model. So what we have to do is to provide the language model with sufficient information so that it can score these partial hypotheses. So when we are producing the, when we are first digesting only this China word, then we create only one word, the China in the target language. So there is nothing that a trigon language model can score here. And similarly for the Beijing. So for these partial translations, no language model score was yet applied. They also do not compete with each other, because each of them translates a different part of the source sentence. But if there were multiple translations for China or multiple translations for Beijing, these would compete against each other based on the translation model scores only and not on the language model scores. It is only later when we put these phrases together that trigrams emerge on the target side. And it is important to realize that we cannot do the scoring based on the language model gardiens has the power to swap them. So without that rule if we are not on the list yet, we would not know whether the English anagram would be China – Beijing or Beijing – China or Beijing in China in Beijing within China. It is only after the rule has been applied and after the rule has put these two items together to form the bigger icon. And this bigger item then, already includes the information that it will be produced on target side as Beijing in China. So that is the new trigram that gets created at this point and only then this can be scored. So only at this point the hypothesis will be scored more reliably including the language model information. So the notion that you have to also get at the the table of constituency parsing the CKY parsing and there we have the search space with the states. So the states were labeled with the non-terminal and each of the states indicated that from this position to this position there is a valid derivation that covers the whole span and that valid derivation is of type NP. In other words this span in the sentence can be interpreted as a noun phrase. So that is a state. Here this is the hierarchical model example. So here the non-terminal is very boring. The non-terminal is always x. So when we have one state in the hierarchical model it will only say yes there are hierarchical phrases that allow us to cover this span in the sentence. So that is indicated by this x here. That x indicates that from position 3 up to 6 or just before 6 in the sentence you can process this with your rules. So it is valid according to the grammar, according to the rules extracted from the corpus. This is a valid sequence of Chinese words. The modal understands this sequence of Chinese words from 3 to 6. And because it's a hierarchical model, it just says yes it is a valid thing. It is an x. We are not saying whether it is a prepositional phrase or noun phrase or whatever. It is just an x. So this state label, the x going from 3 to 6 is good for checking whether the whole sentence can be processed. So if we at the end end up with an x that goes from 0 to the last position in the sentence, we know that this sentence is understood by the modal. But it is not sufficient for the language modal scoring. if we illustrate it here for example with the Beijing thing. Here we know that from position 5 to 6 we have digested something which can serve in any phrase here. But when putting together this Beijing with that in and China, we need to know that it is the word Beijing. So this state of x goes to x covering 5 and 6 has to have more details. So the idea of CKY parsing is like putting all the words on the desk and then wrapping the words into boxes. So these boxes correspond to the grammar rules. The grammar says which of these smaller bits that are on the table should you now take and wrap them in a box. And the left hand side of the rule specifies what you should put on as the label on the box. And you never open the box again. So that's the idea of bottom up parsing. You start with smaller box and you wrap them into bigger and bigger boxes. And you always label them so that you can use them in the combination. Now for the purposes of the language model scoring on your box, you must put not only the label of the non-terminal, but you also need to put there the exact words that the hypothesis ended with and started with. So that if you use these two boxes, so this rule says consume this box for x0 and this box for x1 and then use these boxes on the target side. But on the target side the language model will ask, okay, what is the word between in and what is the word after in and if your smaller boxes had only the x as the label, you would not know. You would not know what the engram was. So the state splitting is the idea that your search space, all the labels of the boxes that you have on your table are split to contain more information. Normally you would have only one label for x that of the box. So the state splitting is the idea that your search space is larger, your state labels are more fine-grained as is needed by the further processing in the system. This is actually a the problem with the history and the hypothesis recombination that we discussed in the phrase-based MT. Except in phrase-based MT, it was all left to right. Now it's all bottom up, so we have like two ends of history that needs to be taken care of.