 Why did it fail so? There was cumulation of errors along the pipeline. So the pipeline is a set of sequence of processing tools and whenever any of the tools makes an error, these errors will propagate. Then there is a huge data loss that happens due to the incompatibility of structures. And that is something that we have discussed with David Cheng's slides. When you are extracting three let's translation rules that are compatible with both word alignment and three let's. And that reduces the number of three let's that you can extract. Then there is the combinatorial explosion of defected output that struck me in the tectogrammatical approach. So the search is over too big space and the NBEST list varies in unimportant attributes and so on. So another problem behind this is that there were of the data are two strong independence assumptions. So you should never analyze and factorize things that you can copy verbatim that you have in the in the data seen enough so that you can copy them without thinking. And also if you have a very complex system there is a bigger risk of technical errors that you make and it's harder to optimize the weights of the various independent components of the system. So complex modules are harder to and harder to debug and harder to tune. So all this together.