 So the idea is to use something which is very similar to HMM tagging. In HMM tagging you have a sequence of words and you assume that there was some hidden sequence of states. There were multiple possible states for each word and you are choosing the best choice, the best state for each position so that the overall sequence is the most likely. This is the very same thing I sort of show that no longer a linear thing it is a branching structure it is a tree and you proceed through the tree. theoretical fashion bottom up and now the translation goes from Czech into English so you are choosing between different possible translations of the word. So Překlad can be translated as translation or Arcade. That alone has some probability. Překlad is maybe a little bit less likely depending on what we have seen in our training data. Translation is a little less likely than arcade for the word překlad. Arcade is like from the construction of buildings. And strojovi can be translated as machine or engine. And again in our data engine happened to be like separately the more probable translation. But now if you these words together engine arcade or engine translation or machine arcade all these combinations are jointly less probable than the choice of machine translation. So this is where the choice of labels of individual nodes is heavily influenced by their context. So in the hidden Markov 3 model you assume that the observed sequence is the sequence but branching sequence is the tree of the source sentence and then the hidden states are the labels the possible labels of each of these nodes in the target language. And you are making choices in the target language so that these hidden states together form the best combination. And there are two components to this. One is the of the translation. This is the translation model because these are the probabilities that link together the source words and target words and how often they co-occurred in a parallel tree bank. And they correspond to the emission probabilities in the hidden Markov model. So from which state which output word was emitted. That's the emission state. And then you have also the of the probabilities of co-occurrences within the target language only. So that is the kind of language model view of that sentence. And that corresponds to the transition states, the transition probabilities in the hidden Markov model. So you consider all possible combinations in a bottom-up fashion and you will end up with the best choice so that overall the sentence, the optimal tree, you will uses a particle choice of each translation candidate within each node. So it's interesting generalization of the hidden Markov model and I recommend to go over this in the detail.