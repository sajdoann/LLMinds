 We'll start with the transformer architecture. So today, first, I'll briefly remind you what the sequence-to-sequence model was, when it was equipped with attention. Then we'll go in detail over the transformer architecture, and there we will focus on one of its most important components, and that's the self-attention. And then we'll move over to explicit syntax in neural machine translation. And we'll cover multiple ways how that information can be added through new networks, so we'll be using syntactic information for the network structure, or, We'll be adding it to each individual token. And we'll also be adding it somehow to the attention mechanism. It's... there is, because of the diversity of these approaches, there is currently no single way in which syntax could be useful. So, there are multiple ways, and depending on the training data sizes, the languages in question, the quality of the explicit linguistic annotation, all that affects whether the information will be useful or not. And the results at the end of this talk are fairly interesting because they suggest that transformer network can learn many of these things by its own without any explicit information.