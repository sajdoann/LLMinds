 Okay, so here is something that we have discussed in quite close detail in the third lecture in the series. If you have a sequence-to-sequence system with attention, what you have at the top is the input sentence. You process the input sentence with two recurrent neural networks encoders, so two recurrent neural networks, one reading the sentence left to right, the other reading the right to left, and then these states for the decoder, then the decoder would have access only to the particle word. But as you are translating the whole sentence, as you are producing the output for the whole sentence, you need to cover all the positions. And that's where the attention mechanism comes into play. And the attention mechanism is something which is trained and on the fly. Then it decides what weight should be given to each of the input positions. at the particular decoding step. So the decoder produces the words as we are used to that, one word at a time. And before emitting the word, it will always consider all the source positions, but each of them weighted according to the attention mechanism. So the attention mechanism is now controlling the word order, but also the general flow of the translation.