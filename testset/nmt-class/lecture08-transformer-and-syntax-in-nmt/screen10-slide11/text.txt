 performs the transformation. So if we now zoom into one of the encoders and one of the decoders, then in transformer there is this sublayers. The encoder has two sublayers. The first of them is the self-attention, and we'll cover that in big detail. And then a standard feedforward network, which just transforms the coordinates. So if you vaguely remember when we introduced neural networks in one of the early lectures, we were saying that a feedforward network corresponds to a transformation of coordinates, and it can do like the skew and the transposition, and then the non-linearity. So this very simple operation, or short sequence of operations, is the only kind of processing capacity. except for the self-attention. So the self-attention is a really critical component. And the computation of the transformation is limited to this simple feedforward network. Remember, it's six times, so it's six layers, so it's quite a deep network, but you cannot possibly expect to obtain sensible translations if you used only six layers of feedforward network. So the self-attention is something which is critical. The decoder is similar. It also has this self-attention, so it's confused in the previous layers itself. And it also has the standard style of attention that we know from sequence to sequence.