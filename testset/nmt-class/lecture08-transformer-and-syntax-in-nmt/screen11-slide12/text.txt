 Yeah. Now we can add the word vectors. So if we have the input sequence, there is a standard thing. You do some subword processing. So you create some subword units. Here the words were frequent enough so they are not subword units. They are full words. You have the embedding layer. So you convert each of these sparse one-hot representations of the words to the dense vectors. Then you feed them to self-attention. and it will be clear from the next slides that self-attention allows every word to consider every other word. So there is like mutual dependencies between words are studied here within self-attention. That is the place where the network can flow or the information can flow from any position to any position. You get some intermediate representations. In some sense, they correspond to the positions of the input words. And the residual connections would like highlight that, that the position one remains to reflect information from the word number one. But this is not needed. The network is free to move the information anywhere. So if it's important for later processing, the words can be like totally shuffled. So the correspondence to positions comes kind of as a side effect. and it's not obligatory. And then there is the feedforward network, which actually... ...