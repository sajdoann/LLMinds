 So let's first motivate for self-attention in some way. So we know that we are processing arbitrarily long sequences and we need to process them with like fixed-size parameters, fixed-size structure of the network. And the first method that we've introduced with the sequence-to-sequence architecture was the recurrent neural networks. And that is a simple single matrix that converts the input vector to some state representation and it combines the state representation from the previous time step with the now new token producing some output and also producing the state for the next calculation. And if you like the way you implement this is then you time unroll it. You put all these matrices after one another in the calculation so the network is actually as deep as is the maximum length of the sentences that you are willing to process. And the same parameters are reused many times along this path. The problem here and we've already discussed this problem is that the calculation is like deep over many steps and you are reusing the same number many times. And reusing the same number in multiplication can very easily lead to either exploding gradients or vanishing gradients so that's why the clever cells were introduced to avoid this problem with training. But even if you succeed in training the problem is that you have to carry out and you can handle the computation one at a time. And so it's like it will take as many steps as is the length of the sequence. If you do not want to pay this linear cost if you want to access different parts of the sequence quicker then the traditional device to do that with neural networks were convolutional neural networks and there you define at how many surrounding positions, am I looking at each step? So there's the kernel size. There could be also some stride specifying like that you skip some of the positions so you can miss some of the intermediate information and look broader. And if you apply this style of processing, if you apply the convolution network in multiple layers, the receptive field, the span of input elements, which contribute to the information in one of the positions, can be pretty broad. So with a convolution neural network, you are specifying two constants, the depth of the network, how many layers, and the kernel size, so the breadth in which it explores the network. And these two numbers specify how long sequences or subsequences are processed at a time. And if this receptive field covers the whole sentence, then perfect, you can process the whole sentence in constant time. Constant in the sense not dependent on the number of positions in the sentence.KE. So the parameters, the kernel size and the depth are then arbitrary and they would depend on the type of text that you are processing. It is possible to train a convolutional network and at the end of the slides, there is a citation. There was actually paper on using convolutional networks for translation, aiming at the same thing as the transformer paper did. But the transformer was so interesting that people then totally abandoned the convolutional approach. So these days, only very few people continue working with convolutional networks, and everybody moved to this self-advention.