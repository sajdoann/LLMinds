 And that is what the self-attention networks achieve. They allow to access any position in constant time. And so here is the summary of the complexities. That obviously comes at a cost. It comes at the cost of more intermediate numbers needed. So the memory needed in computing the self-attentive network is dependent on the square of the length of the sequences. threatened. But the number of operations, the actual calculations that you have to do is also N-squared. But the design of the network structure is so that all these computations can be run in parallel. So assuming that your GPU has enough arithmetic logic unit so it can do many computations in one step, then you can really process the arbitrary long sequence in one time step. So that's the benefit.