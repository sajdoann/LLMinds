 at which point. And what we have as the input are only the sequence of input words or input vectors. So the self-attention introduces this concept of keys, queries, and values. And these are just three different views of the input units. And it's up to the network to decide what should serve as key. keys, what should serve as queries, and what should serve as values. So the way you obtain these keys, queries, and values is that you simply project the input word embeddings with some projection matrices to the queries or keys or values. So there's one matrix which specifies how the queries are generated from the input tokens. Another matrix which specifies is how the keys are generated and how the values are then used or are then computed. If these matrices would be identical, yes you could you could take the whole vector of the word. So it's up to the network to decide what is important. And now