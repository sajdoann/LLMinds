 Then you have some weights which indicate how important for the given position one all the positions in the sentence were, including the position one itself. So maybe sometimes the network will decide that when doing the transformation at this layer, I will like preserve most of the information for this word. I will not consider many of the words around. So the self-attention is like heavily focused on the current position itself on position one in this example. And only a little bit of information will come from the position two. So this way you aggregate the information from the whole sentence at every token position to a new representation. So after the self-attention, you have the same number of positions.