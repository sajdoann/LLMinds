 So that's the main benefit. The self-attention can be done in matrices. You have the two input words, the two lines of the X matrix. You first produce the corresponding two query representations for each of the positions. You also create the keys for every position and you create the values of every position. And then you match the keys and the queries, normalize it, and these are the weights. after the soft mach normalization, with which you combine the values. And that is how you obtain the intermediate representation. So this all can be done in constant time.