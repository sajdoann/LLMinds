 So here is the summary of the self-attention. You have the input sequence of representations. At the beginning these are the embeddings of individual words combined with the positional encodings. At the subsequent layers this is the previous layer representation and in the decoder you do this twice and once you are considering the the self-attention the previous decoder information and in the second step you are considering also the information from the encoder. So we have the sequence of input vectors. You have a predefined number of heads. Each of the heads is defined by the number by the weight matrices that specify what are keys queries and values. You compare all the keys with all the values. queries and that will give you the weights that you apply to the values and then you obtain the resulting representation. You have eight of those. You concatenate them. You have one train again projection matrix which squashes it to the original dimension of the representation. And then the Z, this final Z can be used as the input for the for the next layer calculation.