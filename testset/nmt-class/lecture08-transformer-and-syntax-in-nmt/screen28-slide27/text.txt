 The self-attention is used three times in Transformer. In the standard thing that we would expect based on the sequence to sequence with attention is the encoder-decoder attention. So there the queries are previous decoder layers and the keys are identical to the values and they are outputs of the encoder. So the decoder positions here the attention mechanism here only uses the similarity of the representation. So the decoder word finds the most similar word in the encoder. So that's the encoder-decoder attention. Then we have the encoder self-attention. that is what we have discussed so far. Their keys values and queries are actually said to be identical. So there is only one matrix used for all of that. And then the encoder positions attend to all the encoder positions of the previous layer. And in decoder self-attention you have the very same thing. But obviously there has to be one more trick done. The decoder produces the output one step at a time. And it would be very wrong to look at the words which I have not produced so far. So the decoder self-attention uses masking. And it's all the following positions from the current positions are like forcefully set to blank, set to zeros. And that's how this cheating is preventing this looking into the future is prevented. Okay.