 So this is what the heads can do. And there have been multiple papers studying what the heads look at, what they consider. You can use the attention weights in a similar way that we've used the attention in the sequence-to-sequence algorithm to see where the network is looking. So if you consider the layer 5 of the encoder and this and you look at one of the heads and you look at what the head is looking at, what weights it is assigning to the positions when it is producing the representation, the next layer representation for the word it. In this example sentence you can see this. The sentence was the animal didn't cross the street because it was too tired. And arguably when interpreting this sentence we as humans have to know that this it and this is the head number 1 on the layer 5 was actually able to figure that out. So when producing the next layer representation of it, it really looked most, it consulted most the information which is available in the antecedent of this pronoun. So the weights, the trained weights, managed to identify the antecedent of the pronoun. So the network learned how to find