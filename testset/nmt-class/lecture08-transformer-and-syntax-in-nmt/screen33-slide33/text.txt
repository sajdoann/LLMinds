 So how do we add linguistic information in a neural machine translation system? The idea is that you use tools outside of the of the NMT system as such. You pre-process the input sentences or the training sentences, the target site training sentences with whatever parsers, taggers that you have and then you need to somehow put this information to use in the NMT system. So one way is that you really follow the linguistic structure of the sentence, the syntactic structure, and you construct the network to mimic that structure. And that is that is very rigid and it is motivated especially by the fact that the recurrent neural networks process the sequence only left to right and right to left so the information which was far away in this linear order was too far away in the calculation and the network might not notice. So that's why three LSTMs were introduced. Then we'll briefly mention something which is called graph conversion networks. Essentially this structuring of the network based on the source syntax can be done for the source side only. If you, another option how to add this explicit information syntactic or morphological or any other is to put this information to every token. So we'll discuss how that can be done and this is kind of similar to what we have done in morphology in statistical machine translation in the in one of the previous lectures. So it will be factors like factors of and that is the way to put them on the source side of the NMT system and on the target side. In some way you have to rely on the multitasking mechanism. So the network is then trained to produce sequences of words and in addition to that also some sequences of this additional information. And this multitasking strategy is supposed to help the network in the main task that you are focusing on. So this is like for the encoder and decoder parts and there is also the attention mechanism which was made central by the transformer paper. And you can also somehow equip the attention calculation with explicit linguistic information. So we'll mention two So we'll mention two