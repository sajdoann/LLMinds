 Here is another approach some years later. The idea here is that you have the sequence of states from the normal bidirectional encoder and what the structure adds on top is that you have additional states which are then traversed bottom up and allow the encoder and decoder to reconsider and now in this second step deliberation it is following the dependencies that humans found important because this upper part is made to mimic the syntactic structure of the sentence. So this is less rigid than the three LSTMs because it has the linear backbone the standard thing that worked well in the sequence to sequence approach and in addition to that it exploits the explicit knowledge.