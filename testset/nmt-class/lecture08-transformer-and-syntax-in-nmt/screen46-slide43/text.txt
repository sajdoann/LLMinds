 See from the results that sometimes you can replace linguistic annotation with the dummy symbol and you will get the very same score. So this is the sequence to sequence, so the recurrent neural network processing. The interesting thing is that if you use the same tag, if the vocabulary size is set to one for the interleaved tag, then the network does not learn anything useful. So this is like a badly designed training setting and the network does not learn anything. And then for the CCG tags or random tags or the baseline which is no... this is the interleaf setup and this is the multi decoder setup. So we are testing both and they both come out similarly. Regardless how you produce this secondary sequence it's you get to about the similar results these differences are sometimes reported in papers as significant. Yes they are. So if you use the interleaf setup the random tags come out slightly better than the correct CCG tags and both of them are better than the baseline. In the multi decoder setup where the output length of the sequences can differ explanation is that the CCG tags are too easy for the network to produce. So it produces them as a side effect. But more importantly, it gets one more time step to think about the next word. So with the interleaving setup, the network has an additional b Ferreira OUT paydás o d mirá readers toיתf, so it can deliberate one more step about the next word and produce some CZ username manages on the go. And this increased depth of the network is what gives you the gain. So that is our speculation.