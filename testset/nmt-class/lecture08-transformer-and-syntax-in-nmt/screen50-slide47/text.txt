 I'm going to skip the details of three coverage in attention. I just wanted to say that there is a paper, at least one, that restricts the way the attention is calculated. So it's like constraining or changing or specifying the network structure in the attention part based on the source syntax. And that helps because it avoids repeating words in the output. So considering this in the form of the formulas was beneficial for this English Chinese translation.