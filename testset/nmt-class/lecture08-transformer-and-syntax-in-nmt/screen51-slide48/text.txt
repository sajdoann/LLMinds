 I would like to report more in more detail on experiment that we did. So we realized that the attention heads, the self-attention heads in the transformer model are square matrices and these square matrices can be seen also as the dependency representation of the sentence. So if one of the heads of the source sentence, then it would always attend to the governor and no other words. So if the if the attention matrices were very peaked, if they would be like one hot, then the the this particle head would strictly follow the syntactic structure of the sentence. And since we can parse the sentence at the upfront, we can very easily like add it to the training objective and penalize the network for not producing the parse with this particle head. So we have source tokens. We have the multi layer, multi hatch transformer encoder and we constrain the training of the encoder. So that not only it's trying to be most useful for the decoder and match the output sentence, but also its head number one has to produce the dependency parts of the source sentence. So this is like a soft way of constraining what the network does. Instead of forcing the calculation to follow the syntactic structure of the sentence, we say, do you find whatever way of combining the numbers you like, but make sure to realize what was the structure of the sentence. And we can obviously add this requirement on the representation of the dependency tree at any of the layers. So we tried each of them separately. So the overall picture is that the objective has two parts. One is the normal match, the cross-entropy with the target sequence. And another is that the attention head, one of the eight, is representing the dependency. And similarly, that is the decision. to redeem these responsibilities. Number 20, equals to theetta is this parameter