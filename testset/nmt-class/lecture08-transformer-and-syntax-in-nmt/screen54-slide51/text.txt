 The results on the devset and also on the test set in terms of translation quality are actually better than when we use true parse. So if we use this dummy parse, if we force the transformer to learn to produce low cross-entropy outputs and at the same time to produce a diagonal matrix, obviously it succeeded greatly in. The precision of that is close to 100%. But it also improves the translation and here the real explanation is not so clear. Somehow it helps for the attention matrices to be peaked. That's it. And it's not important whether we we make these self-attention matrices peaked towards the true parse or towards a diagonal parse. The effect on the translation quality is the same or even better with the diagonal parse. So it helps the transformer to know explicitly what is the preceding word. Again, it helps most when this information is learned at the first layer or available at the first layer. and you are getting the same gain, then you have to think back and you are somehow improving just the calculation properties, the stability of the training, or the distribution of the weights in some way. So some peak distribution could be beneficial. So you're just improving the numerical properties of that setup. And the additional linguistic information is actually not