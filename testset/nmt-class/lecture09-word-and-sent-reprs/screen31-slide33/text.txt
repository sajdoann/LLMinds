 Then we created our own model which included when creating the word embeddings also the subword units or some substrings of characters. We call that subgram and on the original test set the performance was 42, a little bit lower than the word2vec model performance. The difference between these two word2vecs is the size of the training data. So we didn't have access to the huge I will tell you that this is the US RFbe is designed by Tomas Mikolov and our training data these two systems seemed very similar on the original test set. On our test set there was a huge difference between these systems.