 So I'm coming towards the end of word representations, but I would just like to highlight that some of these evaluations are based on the human assessment. So the whole idea of evaluating against some human annotation of words can be seen as risky, and I think it is risky. So it depends on which humans you have asked and what or what they think about the words. And there is also, if you ask humans how similar are these words, then people will do different things. They will score different things. So we can talk about relatedness, which is that the words appear in similar contexts. So teacher is related to student and coffee is related to the cup or break as well. But in similarity, that is like, that's a different notion and teacher is similar to professor and car is similar to train. So these two types of similarities are apparent in the human annotation. And if you are not specific enough, if you are not precise enough when collecting the judgments, you can accidentally conflate these two things. So Hill and colleagues observed something that we also observed in our evaluation independently. These monolingual models, when they are used to create word embeddings, they reflect rather the non-specific relatedness and the models that are trained on parallel data, such as neural MT, they will reflect the conceptual similarity. So that is something that you have to consider. And even if we distinguish the relatedness and similarity, which do we want to be reflected in the word embeddings? So there is a survey of evaluation methods from 2018. There was a word vectors evaluation web page where you could upload your word vectors and it will tell you how well they perform in a number of metrics. So there is a lot of things to do.