 I'm not going into much detail, but there's an interesting paper by Carverne and Canavra who illustrate how you can represent structure in vectors of real numbers. So in order to represent structure, you need to have some operations and you need to be able to reverse some of these operations. So they illustrate that it is possible to represent hierarchical structure or whatever you like that you have put together. So this is like one hierarchy and you can represent variable assignments with the help of multiplication. So you could have like variable vectors and to put some variables together you would use these variable vectors to multiply the values, add it all together and then you can recover that value from the vector. So you can like query a vector by multiplying it with the and it will give you the value of that variable and disregard the other variables. And you can also do one more operation and that is permutation of vector elements and that also allows you to create many different like variations or new like subspace or new directions to explore in that continuous space. And this is very useful for representing structure. competitors. So if you want to represent list in programming languages such as Lisp, you would introduce operators carre and coded for the head of the list and the tail of the list and these can be created or constructed in the continuous space by particular permutation of the elements. So if you have a vector which is set to represent a list, then you will apply the carre permutation. arrive at the vector which corresponds to the head and you reply and you can do the same with the other permutation to represent the tail. So this is the way to put together the representation of the list as a whole. So I don't have any details on the very recent experiments of how much compositionality is reflected in the learned Mm-Hmm. But yes, there is a paper which highlights or documents that recurrent neural networks are capable of learning these compositional structures but they don't do that very often. And there is no way for us to make sure that the network will learn the representation in this compositional way. Sometimes the network decides to to understand the compositionality and reflect it in its activations. And in some training runs, the networks simply decide to memorize the input data. So there is the capacity, but there is not the urge to learn in the meaningful way.