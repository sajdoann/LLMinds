 called the inner attention. So if you want to somehow give the system the flexibility to look at various inputs, various parts of the input sentence, you can use these multiple heads. But the trick is that you will use this attention in a static way. You will use it only once before the decoder starts. So you will have the matrices that we have discussed in the transformer architecture that the matrices that like create different views of the sentence. But you will have a fixed number of these views. And you specify upfront that you want to have four views of the sentence. And then you calculate these four views each with a separate, so each is.