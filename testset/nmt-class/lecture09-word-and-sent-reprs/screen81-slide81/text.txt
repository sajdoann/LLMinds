 It corresponds to a separate head. And then you can use this attentional representation as a static vector and you feed this vector to every position in the decoding process. So the network has the capacity to attend to various parts of the sentence but it has a fixed and limited number of these views and then the decoder from the 받아 bit is free to access to any on this head . So either the decoder will get this inner attention of the encoder as one concatenated vector . . . that's the attention context. So the decoder operates on the entire embedding of the sentence . . . or the decoder can even attend to this summary of attention . . . . . so it's attention-attention or compound attention as we call it. So the decoder has now the attention over the heads. And this is something which gives the network the flexibility to attend to various parts of the sentence. And still we have one single vector to represent the sentence meaning. So we have the sentence representation. And so this is something that you can do with the sequence to sequence.