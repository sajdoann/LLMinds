 Variation now of the evaluation. We also had the transformer model in the setup and if we remove the transformer results, which we had the transformer setup only in the English to German evaluation, if we remove transformer the picture is even clearer, the BLE is more negatively correlated and the other metrics are more positively correlated with each other. model possibly behaves like deeper. It is also deeper, it has more layers. So this should be re-evaluated with the recurrent neural networks but deep and to see whether the depth of the network would indeed lead to representations which perform better in the semantic scoring. But the standard Bahdanau shallow recurrent neural network is simply processing rather