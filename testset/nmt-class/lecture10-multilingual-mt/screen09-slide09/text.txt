 called the child and on the child data set normally you would have this orange curve the performance on the orange curve and that already shows some little bit of overfitting so the training data for the child is maybe very small so the model learns memorizes that data too much and it does not then generalize well and perform well on the held out set so that would be the baseline the orange one orange one but if you initialize this child model not with random weights as this as is the normal case but if you simply continue training with the model as it was prepared as it was trained on the parent language pair then you can get this transfer learning effect you can get rid of the degradation due to the overfitting and you get the better performance