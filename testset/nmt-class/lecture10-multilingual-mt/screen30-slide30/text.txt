 So why did it help? Well, we don't really know what is behind this success. There is the intuition that languages have similar structure and yeah. So we were trying some of these, but this analysis should be deepened a little. We were checking the vocabulary size and we were checking the scores. So one thing that we noticed is that a child produced longer outputs. So the BLEU score, which we used as our evaluation measure, striked a longer<|en|> penalty. So that is already a little benefit, but it was not the only benefit. It was not just that the child would produce long enough sentences to score well. The child also had a good words like better words. And then when we looked at these words, we saw that the setup where the, the setup where the child learned some new words that the parent previously didn't have and there was about 10% of such tokens in the output and these 10% of tokens when we looked at them we were expecting to see words which would come from the parent language. So maybe some names that do not change the spelling or some numbers that the system would really copy over from the parent. And that was not the case. So the better translated tokens, the better produced words were Estonian words, no named entities. So the model got some improvement in overall sentence structure. We don't speak Estonian so we couldn't really assess that but it was regular Estonian words.