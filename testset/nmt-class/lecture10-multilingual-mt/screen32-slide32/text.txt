 Could there be the sentence structure gain? So well, we didn't go as far to study the sentence structure, we just studied sentence length. So this is a slide that also confirms what I was telling you before about the neural machine translation system memorizing the sentence length. Here we have the parent model. So this is again English to Czech translation. And what we try with the parent modelを say a sentence sentence in a certain length and we train the parent with the sentences or only within some range of number of words and we evaluate on the normal like fix size and full length full range of links test corpus. So when we look at the performance of the parent alone on the parent test set, then if we train on short sentences only then the BLEU精 will be very low and the actual outputs of the system for the test set will be very short. At most, on average, 10 words. So actually the outputs are somewhat longer than what the training data was. But they don't go as far as needed. If you trained on everything and the performance was the best possible on this corpus, then the average number of output words would be 15 and we are only at 10. on longer sentences than the average. If you train on sentences with 20 to 40 words, then the BLEU score is actually also low, much lower than the baseline. Because the system will produce two long sentences, two long outputs, actually much longer, five words or six, seven words longer on average than what the best performance would do. And with these long sentences, long outputs, many of these words are obviously not confirmed by the reference. So the BLEU score decreases. So with two short output, the system is penalized on brevity penalty. With two long outputs, the system is penalized on the precision of n-grams. So now we have these different parents. These parents know only how to produce sentences of certain length and they tend to be too short or too long depending on which parent it was.