 at hand. Okay, so what were the results? This is the multiway and multi-source setup. It is trained on independent pairwise corpora across many languages. And you are translating from Spanish into English or from French into English. And you have either a model which is trained only on the Spanish-English data, that is the single language model. Or you have a model which is trained, which has also seen the encoders and decoders from the other languages in the collection. And these models perform better when translating into English. So the Spanish-English system got some gain buy seeing the other language data... Apart to the baseline, which didn't see the other language data. When we reverse the direction, we see, again, the sad picture. The other language data is not helping performance with Spanish, so the baseline using just the English-Spanish data is better for for the other direction.