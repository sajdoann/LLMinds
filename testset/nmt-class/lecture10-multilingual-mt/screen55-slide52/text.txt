 The Zero-shot translation. So imagine that you have two parallel corpora, Spanish-English and English-French, and you would like to translate between Spanish and French. So one way to do it would be to train independent systems and explicitly go via English, but that is like two steps, so it takes twice the time, and there can be very easily some information because you need to like pick the wording, you have to resolve the ambiguities when translating into English, and then you have to resolve the ambiguities again. So the pivot translation is kind of suboptimal, and also we have this idea of the multi-way encoder-decoder system that can know both pairs within one model. So can we use this model for a Zero-shot training? So we know that the encoders were trained on Spanish, and the decoders were trained on French, but they were never paired together. So what happens if we run this system for translation from Spanish into French directly, and we do not run it twice? We can run it twice. We can run the same system twice. feed it with Spanish, obtain English, and then take the same system and feed it with this intermediate English, and obtain the fun French. So that is pivoting with a multilingual system. But the Zero-shot is different. The Zero-shot is that you directly ask the system to produce the French output. feed it with a multilingual system. So that is the output.