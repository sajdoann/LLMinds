 So here are the scores. Unfortunately, if you do not do the pivoting, then the performance on the DEF and test sets are just horrible. So if you imagine the structure of the network, you have this shared component of attention, you have the multiple encoders and you have the multiple decoders. And they were trained so that the span was always trained with the English decoder and the English encoder was always trained with the French decoder. And then you try to cross, you try to use the attention in a way in which it was never used. It doesn't work at all. Like what we obtained only is a neural network which can process can do two tasks, but there is no sharing of information. These two tasks are mutually independent and you cannot take one part of the network and combine it with the other end, the never used end, and expect the performance. This didn't help. If we use the two paths of the network, the Spanish to English and then English to French, in the way they were and then we are trained for it. So with the explicit pivoting, it works. You get reasonable performance. So the zero shot...