 If we use a true parallel corpus, then we get towards the performance of the explicit pivoting setup. Or maybe slightly below that, but not much. So we can reach the performance, but we need true parallel data. And unfortunately, if we have these single pair baseline models and we use one million we will get a better performance even with this with the sampling. So this is a negative result in a way. Having separate encoders and separate decoders and trying to use new language pairs, test these encoders and decoders across the setup, we will not get any better even with this fine tuning where we could with a reasonably big true parallel corpus. If the true parallel corpus was rather small, like 100,000 sentences, then the performance of the baseline would be worse than this. But this is something that we have already seen in a way. And I think that the trivial transfer learning that we discussed previously would give us equally big improvement.