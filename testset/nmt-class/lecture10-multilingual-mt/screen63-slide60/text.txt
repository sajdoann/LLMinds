 So let's do something simpler and the simple thing is We will use just the same Encoder and just the same decoder and we will train this system to do many translations at once for the encoder part We can simply feed all the many languages and the encoder will learn how to like it will learn automatically what is the the current input sentence what language it is in and it would learn to produce the Target sentence regardless what the source was so so making a system multilingual on the source side is very easy You just mix in all the source languages Into your training data and that's it But for the target side you need to specify what you would like the system to produce and this is done with so-called language token so The source sentence will be now Extended the source sentence will now add Will include one more token it can appear at the beginning at the end or maybe both sides and this token will be a special request I'm giving you now the German sentence But I would like you to translate it into English so this token to en means that Now you are getting some input you will know you recognize the language by yourself But I want you to produce the translation in English and then the expected target in your apparel corpus Would be in the English and then the next item in the batch Or the next item in the in the training corpus would be another sentence pair From another language pair and it could be An English source and now the request would be to translate into Dutch So the token would indicate well whatever the language is of this input string I want you to produce Dutch and then the expected output that the model is trained on with backpropagation as we have discussed it in the past The the expected sentence will be will be in Dutch So with this way we have a model of a fixed number of parameters we have not changed the structure at all We have only used one more artificial token to indicate which language we want as as the output and Hopefully the system will like for free benefit from the similarities between the languages if there are some So we have to be clever about like keeping all the languages in the mix in each of the batch if we Organize the training corpus by language pairs so that would first train on one language pair and then go to another language pair That would be transfer learning and not this this multitask training This multitask training needs to refresh the knowledge of all the languages with within every batch So then you have to somehow like make sure that You handle the disproportion of language sizes of training corpus sizes well if one of the languages language pairs was was like too big compared to the others then The other languages would be already overfit and the This big language pair was would be still not fully employed in the training so there is some Things that you have to consider when when training this setup, but the main Result is that you have a fixed set of parameters Into the model of the same size you are now fitting more languages at once, so you are getting the hardware So you are getting the hardware savings right away without any any loss hopefully so