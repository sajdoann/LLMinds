 This is the transformer setup. We have discussed that two weeks ago in quite some detail. And as a reminder, the transformer setup is an encoder-decoder sequence-to-sequence system where the encoder consists of N layers. And normally there are six of these encoder layers. And each of these encoder layers has two sub-layers. dal dal Trez nuashempÃ«r stops. First is the multihead attention in which there are multiple scale dotsAdd products that provide multiple views on the previous layer outputs. So there is another constant here is one constant which says the number of layers, the depth of the network. Here is the number of heads in each of these layers. And there is also the feet forward network which like expands and then releases range to trace and reduces the representation again. So here is some processing of the intermediate representation happening. So that is there is another constant the size of the feedforward network. And that's the encoder. So after a few layers of self-attention with the multiple heads, you arrive at the representation of the sentence at that position. And then you run the decoder and the has essentially the same structure, multiple layers, and you make the decoder consider itself. So it has the self-attention component and it also has the encoder-decoder attention. So it again uses the multi-head attention to consider all the source sentence positions as they were prepared by the encoders. and then there is again the feedforward network which allows the decoder to improve the representation at that layer. And after, for example, six such layers, you will just scale it to the size of the vocabulary and then you softmax to pick the current output word. So this is the the model that performed pretty well in the in the single language pair setups. And now the question is, how can we make use of this model in the multi-language setups? And the previous experiments that we have discussed so far were based on the recurrent neural networks, mainly on the recurrent neural networks, and they didn't really perform well for the for the high resource. Bref. What is the right thing? Now the key is that there are wcze, and that's the most important platforms. I think people face it to the size that had to be more energy, they were least80 or a larger wall off transmission, and they have to be exactly the same thing! Some are relatively different in all, right, been the same then.