 languages. So what the Google guys did is they will increase the size of this transformer setup. So the default is to use just one GPU on which you can fit six layers of encoder and six layers of decoders or 12 layers as such. each of these layers is actually the multiple layers, the two sub layers of the encoder and three sub layers of the decoder. Then by default, there is 8000 vector elements in the feedforward network and the default is to use 16 heads. So in total, when you look at all the matrices that need to be trained, there is 400 million parameters to train. This is the setup that performs very well across many languages and this is the state of the art today. So what if we increase the depth? What if we use 12 layers in encoder, 12 in decoder, so 24 in total and we also double the size of the dimension of the feedforward network and we double the size of the heads? That will get to 1.3 billion parameters and that would be a wide network, so to say, a wide transformer. And you will need definitely two GPUs to train it at the same time. And then you can use the same number of parameters, 1.3 billion, but organize them differently in the network structure. So the network can have smaller or like the default breadth of the feedforward network and the default number of heads, 16 heads. But you can use double the number of layers. So that is a deep network. And to train this, you actually need to use four GPUs because of the different layout of the network structure. And there is other models. So the largest one discussed in this paper and based on the G-pipe setup, which allows to train these big networks by better parallelization across the multiple GPUs. The largest setup that they used was to have 64 layers in encoder and 64 layers in decoder. So 128 layers in total with 32 heads and the bigger intermediate dimension. And that is 6 billion parameters. And as you will see, the deep setup, as you will see on the following slide, the deep setup will be better than the wide setup, especially on low resource languages. And that's probably because the network has more time to generalize and to construct some generic representation and then to produce the output based on that. But with these big networks, that many parameters, and despite having a huge corpus, which consists of all the hundred languages paired with English, there are non-trivial things that you have to do in order to keep the training stable. Transformer model as such is already pretty unstable. That's why the learning rate is only gradually decreased and then slowly decreased, and here you have to add further clippings of the weights explosion.