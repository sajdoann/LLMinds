 So here is the performances. This is on the x-axis. You will see the 100 languages or 103 languages and the baseline is all in the relative performance of the BLUE score. So we always train or Google always trained or always trained the pairwise system for that particle language paired with English. That gave him some BLUE score and then he trained the single model on the many languages at once. And it was a model of a particle size, transformer of a particle size. And again he used the standard thing, the XPR token, to identify the desired target language. and he compared the BLUE score of the multilingual system given the baseline pairwise system BLUE score. And the languages are sorted according to the data sizes available for them. So this is the high resource languages and this is the low resource languages. And this picture which was trained for many months, I will and this is the low resource languages. And this picture clearly confirms that it's the low resource languages that benefit from the multilingual setting. And the high resource languages, if you use the normal transformer size, will actually suffer. and the high resource languages. And the high resource languages are the low resource languages. And the high resource languages are the low resource languages compared to the baseline. And now one would have to very carefully think like up until when is it better to use the separate pairwise models. And from when it is better to use the multilingual models. Because at some point the languages become low resource compared to the average.