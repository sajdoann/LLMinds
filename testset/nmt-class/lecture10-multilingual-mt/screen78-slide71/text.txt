 And here is last year's continuation of that experiment. So here we see again the bilingual baselines and we see the massively multilingual model with the standard number of 400 million parameters. That is the standard transformer setup and that hearts on high resource languages. If you go to the six hM parameters, then you can avoid the loss on the high resource languages. And then if you do further tricks and go up to 50 billion parameters, then you can get up to five black points even on the on the high resource language pairs. So what