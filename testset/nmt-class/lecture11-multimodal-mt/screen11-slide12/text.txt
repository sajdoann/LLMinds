 Then one particle aspect of machine translation which we have not discussed and there are insufficient details on it in this lecture but it's also getting fashionable or popular among the researchers is something which is called incremental machine translation. So there are multiple reasons why you may want to process the input to machine translation incrementally. One of the clear motivations is that the input is still being uttered. So if you are talking about incremental machine translation that means that it is text based so there is some text source but the sentence is not fully complete as you are translating. You are receiving the words one by one and you have to decide whether you want to wait for more words or whether you want to emit the current words. And it obviously depends on the language combination on the source language and the target language. What is the the tactic to take and depending on these languages sometimes you have to wait for a word long for example until the German verb comes at the end of the sentence. What interpreters do is a whole set of strategies. They maybe keep some of the output vague and they leave all the back doors open so that they can recover from the new information that will still come. The motivation for incremental machine translation as a research is to keep the machine translation output stable because machine translation, the standard one that operates from sentences to sentences is free to reorder the words in any way. And if you imagine a partial sentence and then a new word coming to that partial sentence then suddenly this new word may hint the empty system and it actually does the empty system to reshuffle the sentence in a very crazy way. So if you have shown your partial output to the user then suddenly the whole output may change and that that will be very annoying or distracting so the user would not be able to follow this changing text too much. So the main aim of incremental machine translation is to provide the stability of the output.