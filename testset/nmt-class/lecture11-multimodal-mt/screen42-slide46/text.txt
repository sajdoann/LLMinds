 So let's look at the integration of these two components and the core of the problems of the integration is the link between the sequence of words without any punctuation and lowercase and the individual correct sentences that are expected by the machine translation output. And there are a number of things that you can do here. The thing that we are doing now is we're introducing the segmentation step, a special component which and then we feed the sentences to the machine translation system. You could also change the ASR to predict the correct punctuation directly and that would be a better setup than we have now because the ASR still has access to the full sound. So it could make use of pauses and of intonation and other like extra non-verbal aspects of the input. And the fully end-to-end spoken language translation obviously somehow implicitly handles this segmentation. But the truth is that the current SLT system end-to-end are always trained and tested on datasets which already come in isolated utterances. So once you will see that yes, end-to-end spoken language translation is solved, you should bear in mind that it will be solved at the level of utterances. So if someone would be then listening and clicking the mouse button after every completed sentence, then that particular send or utterance based SLT system would work. If you still have to identify where the utterances are, the errors will be much greater, much larger than the SLT system alone would suggest. So if you can take a reminder, what's your framework for the Anti-Fì¢… %10 certification