 The benefits of this direct spoken language translation is that all the uncertainty or the ambiguity of speech recognition is left unresolved until later, until also the target language language model can affect that, can have its say in the decisions what was probably uttered. So the main drawback, and I've already hinted that, is the insufficient training data. We already have quite sizable corpora for speech recognition, that is speech and the correct transcript, and we have huge corpora for translations or parallel texts. Obviously we have even larger texts that are monolingual, but we have reasonably sized parallel texts. But what we don't have is speech in the source language and the text in the target language in sufficient amounts. So what people sometimes do is they synthesize some side of that. So either they take a speech corpus and machine translate it to a target language, or they take a parallel corpus and they speech-synthetize the source side, and then they train on the synthesized voice. That's one drawback, that you don't have appropriate data for spoken language translation. the drawback is the input sequences are much longer. So you are now working with timeframes of sound instead of subword units. And that is computationally challenging, so you need to fit all that input into the GPU memory card and that's difficult. The alignment problem is obviously much more complicated than when you have text input. and if, yeah, as I've already said, people are not really considering that the input comes non-segmented. The input is often assumed to come segmented manually and correctly into utterances.