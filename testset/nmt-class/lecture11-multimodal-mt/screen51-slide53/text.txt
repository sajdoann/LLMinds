 across the whole corpus. The first truly end-to-end approach was in 2018 by the same team. And they have a speech encoder which uses some feedforward layers at the beginning and then the convolution because we have to first reduce the input length to some smaller number of positions. What people use here also is paramiddle recurrent neural networks which is like a standard deep recurrent network except that the deeper layers are not run at every time step but they are run only every three steps or every five steps. And that this deep recurrent networks then condense the time dimension of the input. So then the bidirectional LSTM is digesting this condensed input of frequency timeframes. Then there is the standard attention that we have discussed in the past and they used character level decoding in this 2018 experiments. So they are predicting either the English transcription or the French translation so they are using both sides of the data.