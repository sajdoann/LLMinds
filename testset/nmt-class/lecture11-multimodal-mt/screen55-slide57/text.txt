 can be also used in this approach. So this is a 2019 paper from the Italian FBK team. And they... So in the standard transformer that we have discussed in the past, you have the encoder which contains the self-attention layers and the forward network. And the input to this deep, like six-layered encoder is and the positional encoding because the self-attention is position agnostic. And they have modified that architecture in two ways. In one way, they have added these similar to the Berard experiments. They have added the linear transformations, two layers of the transformation on the same combined input. Now, the of the frequencies of the particular timeframe and the positional encoding. And then they used two levels of convolutional network to reduce the size of the input. And then on top of that, they put the standard encoder layers of the transformer. So this is like the baseline transformer approach. And they have a better setup, the S-transformer, which uses similar compression of the input. The convolutional networks and some attention over that. And that is applied to the timeframes still without the positional encoding. And the positional encoding is added to that output only right before entering the self-attention phase. So this is just an illustration that there are many possible components, how to put the network structure together. And all these different components will either help or damage your training capabilities. And this is a very recent research. So a number of experiments will have to be done so that we will find the best.