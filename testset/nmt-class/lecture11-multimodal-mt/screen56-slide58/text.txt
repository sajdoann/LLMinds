 And I wanted also to mention another interesting example of now directly speech-to-speech translation. This is a paper by Google and they use... They directly train a system to convert the spectrograms, the time frames of frequencies into spectrograms in the target language. And they then use which is called vocoder that's actually synthesizing the waveform in the target language. The resulting system is called Translator-Tran. And well, as I said, I don't know anything about the target side, the speech synthesis. And I know somewhat about the ASR system. But the interesting setup here is that they also separate the speaker characteristics. So after processing the spoken language on the source side and after synthesizing or creating the sequence of phonemes in the target language, they concatenate this phoneme representation with the speaker information. And then they create the spectrograms in like a a second decoder in the pipeline. And that allows them to preserve speaker characteristics in the speech translation. So you will hear the same voice, but it will be saying words in the other language. And the words do not appear as words in the representation. So the intermediate transcripts in the source and target language are necessary in the training phase. So they use them as also in the orientation. So they do not use them as a in the runtime and inference anymore. So it operates with full ambiguity at the level of phonemes. And from the system you can hear words which are not words but the system. They only are close to some words. So it is truly end-to-end. And it has the capacity of preserving the voice characteristics and also all the as the speech. And it's still somewhat worse than the combination of the spoken language translation and text to speech.