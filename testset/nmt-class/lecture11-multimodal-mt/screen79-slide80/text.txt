 The plaintext translation was performing better and Desmond Elliott has made an interesting experiment. He has...so in our experiments when we were using this English to Hindi translation, the challenge words that I was talking about, the penalty box, these words were translated perfectly even with a system that did not look at the images at all. And Desmond Elliott's experiment is even more directed at this exploration. So what he used was several...were several multimodal systems. They were trained to consider the image. And along with the input sentence, such as two dogs play with an orange toy in a tall grass, he fed the modal either with the correct image, which is two dogs in the grass, or with a random wrong image. And this non-congruent image was pretended to help...to be useful for the input sentence. And interestingly, only the hierarchical attention approach was sensitive to that image. So the other setups in which people use the images work equally well if you provided them with the correct image as if you correctly and provided them with the wrong image. And that means that the image information was never used by those models. So the hierarchical model was the only one which...which really learned when to observe the image. And it got a worse performance when the image was non-matching with the input text. So there are other papers where the images have not helped much. So that's...