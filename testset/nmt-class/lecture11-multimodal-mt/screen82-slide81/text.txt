 So what they were doing, or actually that was Desmond's, Eliot's paper, sorry. So what they were doing is that they were degrading the text input and they were removing all the words and replacing them with just a void placeholder in four words, the meaning of which could be recovered from the image. So we have this tennis player and the original sentence was a young girl in white dress holding a tennis racket. That was the original sentence. So the original sentence was already clearly stating that this was a female. So in my motivating example, I have replaced that with the gender ambiguous term tennis player, the data set actually mentioned the gender. And therefore, in the original setting, if you were given this sentence with the gender mentioned, the translation would be perfect. And it was perfect and you would not need to look at the picture at all. In the Desmond's Elliot experiment, they removed the gender specific word and they left there only a young someone in something holding a tennis something.. The colors and they removed the other words that could be guessed from the image. And then with this degraded input, there was indeed a difference between the systems trained on text only information and systems trained on text and visual information. So the NMT, the non-multimodal setup produced a buoy. So guessed that on average, as boys, is in the pictures and the color was also blue instead of white. And the image informed system actually picked up this information from the image and it correctly produced a femme, the French word for girl and the French word for the correct white color. And here similarly the color of the towel was in the source text and the multimodal system was able to pick it up from the image. So the final message is that the architectures really are capable of finding the relevant information at least concerning colors and some key other aspects such as the gender. They are able to pick it up from the image but mostly this information is not